--- book.adoc.orig	2019-09-04 14:19:11.371722047 +1000
+++ new-book.adoc	2019-09-04 14:15:54.539404969 +1000
@@ -1,19 +1,20 @@
-= Think Stats
+= Think Stats: Exploratory Data Analysis in Python
 Allen B. Downey
+v2.0.38
+:doctype: book
 :stem: latexmath
+:toc: left
+:sectnums:
+:sectlinks:
+:sectanchors:
+:xrefstyle: short
+:source-highlighter: highlightjs
+:source-language: python
 
-3 Think Stats +
-Exploratory Data Analysis in Python
-
-Version 2.0.38
-
-3 Think Stats +
-Exploratory Data Analysis in Python
-
-Version 2.0.38
-
-Allen B. Downey +
+:sectnums!:
 
+[colophon]
+= Colphon
 Green Tea Press
 
 Needham, Massachusetts
@@ -35,6 +36,7 @@
 
 The LaTeX source for this book is available from http://thinkstats2.com.
 
+[preface]
 == Preface
 
 This book is an introduction to the practical tools of exploratory data
@@ -274,6 +276,8 @@
 Jeff Pickhardt, Rohit Deshpande, Joanne Pratt, Lucian Ursu, Paul Glezen,
 Ting-kuang Lin, Scott Miller, Luigi Patruno.
 
+:sectnums:
+
 [[intro]]
 == Exploratory data analysis
 
@@ -292,15 +296,15 @@
 claims. I found many examples like these:
 
 ____
-"`My two friends that have given birth recently to their first babies,
+My two friends that have given birth recently to their first babies,
 BOTH went almost 2 weeks overdue before going into labour or being
-induced.`"
+induced.
 
-"`My first one came 2 weeks late and now I think the second one is going
-to come out two weeks early!!`"
+My first one came 2 weeks late and now I think the second one is going
+to come out two weeks early!!
 
-"`I don’t think that can be true because my sister was my mother’s first
-and she was early, as with many of my cousins.`"
+I don’t think that can be true because my sister was my mother’s first
+and she was early, as with many of my cousins.
 ____
 
 Reports like these are called *anecdotal evidence* because they are
@@ -441,6 +445,7 @@
 including functions that read the Stata dictionary and the NSFG data
 file. Here’s how they are used in `+nsfg.py+`:
 
+[source]
 ....
 def ReadFemPreg(dct_file='2002FemPreg.dct',
                 dat_file='2002FemPreg.dat.gz'):
@@ -605,6 +610,7 @@
 `+nsfg.py+` includes `+CleanFemPreg+`, a function that cleans the
 variables I am planning to use.
 
+[source]
 ....
 def CleanFemPreg(df):
     df.agepreg /= 100.0
@@ -657,6 +663,7 @@
 One important note: when you add a new column to a DataFrame, you must
 use dictionary syntax, like this
 
+[source]
 ....
     # CORRECT
     df['totalwgt_lb'] = df.birthwgt_lb + df.birthwgt_oz / 16.0 
@@ -664,6 +671,7 @@
 
 Not dot notation, like this:
 
+[source]
 ....
     # WRONG!
     df.totalwgt_lb = df.birthwgt_lb + df.birthwgt_oz / 16.0 
@@ -757,6 +765,7 @@
 
 To deal with this error, I added a line to `+CleanFemPreg+`:
 
+[source]
 ....
 df.loc[df.birthwgt_lb > 20, 'birthwgt_lb'] = np.nan
 ....
@@ -781,6 +790,7 @@
 do some processing to collect the pregnancy data for each respondent.
 Here’s a function that does that:
 
+[source]
 ....
 def MakePregMap(df):
     d = defaultdict(list)
@@ -955,6 +965,7 @@
 In Python, an efficient way to compute frequencies is with a dictionary.
 Given a sequence of values, `+t+`:
 
+[source]
 ....
 hist = {}
 for x in t:
@@ -965,6 +976,7 @@
 Alternatively, you could use the `+Counter+` class defined in the
 `+collections+` module:
 
+[source]
 ....
 from collections import Counter
 counter = Counter(t)
@@ -1021,6 +1033,7 @@
 To loop through the values in order, you can use the built-in function
 `+sorted+`:
 
+[source]
 ....
 for val in sorted(hist.Values()):
     print(val, hist.Freq(val))
@@ -1028,6 +1041,7 @@
 
 Or you can use `+Items+` to iterate through value-frequency pairs:
 
+[source]
 ....
 for val, freq in hist.Items():
      print(val, freq)
@@ -1035,10 +1049,9 @@
 
 === Plotting histograms
 
-image::figs/first_wgt_lb_hist.png[Histogram of the pound part of birth
-weight.,height=240]
-
-[#first_wgt_lb_hist]#[first_wgt_lb_hist]#
+[[first_wgt_lb_hist]]
+.Histogram of the pound part of birth weight.
+image::figs/first_wgt_lb_hist.png[height=240]
 
 For this book I wrote a module called `+thinkplot.py+` that provides
 functions for plotting Hists and other objects defined in
@@ -1057,10 +1070,9 @@
 You can read the documentation for `+thinkplot+` at
 http://greenteapress.com/thinkstats2/thinkplot.html.
 
-image::figs/first_wgt_oz_hist.png[Histogram of the ounce part of birth
-weight.,height=240]
-
-[#first_wgt_oz_hist]#[first_wgt_oz_hist]#
+[[first_wgt_oz_hist]]
+.Histogram of the ounce part of birth weight.
+image::figs/first_wgt_oz_hist.png[height=240]
 
 === NSFG variables
 
@@ -1077,13 +1089,13 @@
 into a single quantity, `+totalwgt_lb+`. In this section I use these
 variables to demonstrate some features of histograms.
 
-image::figs/first_agepreg_hist.png[Histogram of mother’s age at end of
-pregnancy.,height=240]
-
-[#first_agepreg_hist]#[first_agepreg_hist]#
+[[first_agepreg_hist]]
+.Histogram of mother’s age at end of pregnancy.
+image::figs/first_agepreg_hist.png[height=240]
 
 I’ll start by reading the data and selecting records for live births:
 
+[source]
 ....
     preg = nsfg.ReadFemPreg()
     live = preg[preg.outcome == 1]
@@ -1093,6 +1105,7 @@
 the DataFrame and returns a new DataFrame. Next I generate and plot the
 histogram of `+birthwgt_lb+` for live births.
 
+[source]
 ....
     hist = thinkstats2.Hist(live.birthwgt_lb, label='birthwgt_lb')
     thinkplot.Hist(hist)
@@ -1103,10 +1116,9 @@
 are dropped. `+label+` is a string that appears in the legend when the
 Hist is plotted.
 
-image::figs/first_prglngth_hist.png[Histogram of pregnancy length in
-weeks.,height=240]
-
-[#first_prglngth_hist]#[first_prglngth_hist]#
+[[first_prglngth_hist]]
+.Histogram of pregnancy length in weeks.
+image::figs/first_prglngth_hist.png[height=240]
 
 <<first_wgt_lb_hist>> shows the result.
 The most common value, called the *mode*, is 7 pounds. The distribution
@@ -1147,6 +1159,7 @@
 integer `+n+` and return the `+n+` largest or smallest values from the
 histogram:
 
+[source]
 ....
     for weeks, freq in hist.Smallest(10):
         print(weeks, freq)
@@ -1191,6 +1204,7 @@
 babies and others. I divided the DataFrame of live births using
 `+birthord+`, and computed their histograms:
 
+[source]
 ....
     firsts = live[live.birthord == 1]
     others = live[live.birthord != 1]
@@ -1201,6 +1215,7 @@
 
 Then I plotted their histograms on the same axis:
 
+[source]
 ....
     width = 0.45
     thinkplot.PrePlot(2)
@@ -1214,10 +1229,9 @@
 plot; it uses this information to choose an appropriate collection of
 colors.
 
-image::figs/first_nsfg_hist.png[Histogram of pregnancy
-lengths.,height=240]
-
-[#first_nsfg_hist]#[first_nsfg_hist]#
+[[first_nsfg_hist]]
+.Histogram of pregnancy lengths.
+image::figs/first_nsfg_hist.png[height=240]
 
 `+thinkplot.Hist+` normally uses `+align='center'+` so that each bar is
 centered over its value. For this figure, I use `+align='right'+` and
@@ -1316,6 +1330,7 @@
 Pandas data structures provides methods to compute mean, variance and
 standard deviation:
 
+[source]
 ....
     mean = live.prglngth.mean()
     var = live.prglngth.var()
@@ -1327,7 +1342,7 @@
 of 2-3 weeks to be common.
 
 Variance of pregnancy length is 7.3, which is hard to interpret,
-especially since the units are weekslatexmath:[^2], or "`square weeks.`"
+especially since the units are weeks{}latexmath:[^2], or "`square weeks.`"
 Variance is useful in some calculations, but it is not a good summary
 statistic.
 
@@ -1358,6 +1373,7 @@
 the groups and latexmath:[s] is the "`pooled standard deviation`".
 Here’s the Python code that computes Cohen’s latexmath:[d]:
 
+[source]
 ....
 def CohenEffectSize(group1, group2):
     diff = group1.mean() - group2.mean()
@@ -1485,6 +1501,7 @@
 Given a Hist, we can make a dictionary that maps from each value to its
 probability:
 
+[source]
 ....
 n = hist.Total()
 d = {}
@@ -1581,10 +1598,9 @@
 sequence of values, computes a histogram, and plots it. Since I use Hist
 objects, I usually don’t use `+pyplot.hist+`.
 
-image::figs/probability_nsfg_pmf.png[PMF of pregnancy lengths for first
-babies and others, using bar graphs and step functions.,height=288]
-
-[#probability_nsfg_pmf]#[probability_nsfg_pmf]#
+[[probability_nsfg_pmf]]
+.PMF of pregnancy lengths for first babies and others, using bar graphs and step functions.
+image::figs/probability_nsfg_pmf.png[height=288]
 
 <<probability_nsfg_pmf>> shows PMFs of
 pregnancy length for first babies and others using bar graphs (left) and
@@ -1598,6 +1614,7 @@
 Here’s the code that generates
 <<probability_nsfg_pmf>>:
 
+[source]
 ....
     thinkplot.PrePlot(2, cols=2)
     thinkplot.Hist(first_pmf, align='right', width=width)
@@ -1636,6 +1653,7 @@
 the mode. So it makes sense to zoom in on that part of the graph, and to
 transform the data to emphasize differences:
 
+[source]
 ....
     weeks = range(35, 46)
     diffs = []
@@ -1655,10 +1673,9 @@
 babies are less likely to be born in week 39, and somewhat more likely
 to be born in weeks 41 and 42.
 
-image::figs/probability_nsfg_diffs.png[Difference, in percentage points,
-by week.,height=240]
-
-[#probability_nsfg_diffs]#[probability_nsfg_diffs]#
+[[probability_nsfg_diffs]]
+.Difference, in percentage points, by week.
+image::figs/probability_nsfg_diffs.png[height=240]
 
 For now we should hold this conclusion only tentatively. We used the
 same dataset to identify an apparent difference and then chose a
@@ -1703,6 +1720,7 @@
 PMF, compute the mean, and report that the average class size is 23.7.
 Here’s the code:
 
+[source]
 ....
     d = { 7: 8, 12: 8, 17: 14, 22: 4, 
           27: 6, 32: 12, 37: 8, 42: 3, 47: 2 }
@@ -1719,6 +1737,7 @@
 probability associated with each class size is "`biased`" by the number
 of students in the class.
 
+[source]
 ....
 def BiasPmf(pmf, label):
     new_pmf = pmf.Copy(label=label)
@@ -1736,6 +1755,7 @@
 
 Now we can plot the actual and observed distributions:
 
+[source]
 ....
     biased_pmf = BiasPmf(pmf, label='observed')
     thinkplot.PrePlot(2)
@@ -1743,10 +1763,9 @@
     thinkplot.Show(xlabel='class size', ylabel='PMF')
 ....
 
-image::figs/class_size1.png[Distribution of class sizes, actual and as
-observed by students.,height=288]
-
-[#class_size1]#[class_size1]#
+[[class_size1]]
+.Distribution of class sizes, actual and as observed by students.
+image::figs/class_size1.png[height=288]
 
 <<class_size1>> shows the result. In the biased
 distribution there are fewer small classes and more large ones. The mean
@@ -1762,6 +1781,7 @@
 use it to estimate the actual distribution. Here’s the function that
 unbiases a Pmf:
 
+[source]
 ....
 def UnbiasPmf(pmf, label):
     new_pmf = pmf.Copy(label=label)
@@ -1943,7 +1963,7 @@
 
 Hint: use `+nsfg.MakePregMap+`.
 
-[#relay]#[relay]#
+[[relay]]
 
 In most foot races, everyone starts at the same time. If you are a fast
 runner, you usually pass a lot of people at the beginning of the race,
@@ -2014,11 +2034,10 @@
 at birth in pounds. <<nsfg_birthwgt_pmf>>
 shows the PMF of these values for first babies and others.
 
+[[nsfg_birthwgt_pmf]]
 image::figs/nsfg_birthwgt_pmf.png[PMF of birth weights. This figure shows
 a limitation of PMFs: they are hard to compare visually.,height=240]
 
-[#nsfg_birthwgt_pmf]#[nsfg_birthwgt_pmf]#
-
 Overall, these distributions resemble the bell shape of a normal
 distribution, with many values near the mean and a few values much
 higher and lower.
@@ -2070,6 +2089,7 @@
 you want to find the corresponding value, one option is to sort the
 values and search for the one you want:
 
+[source]
 ....
 def Percentile(scores, percentile_rank):
     scores.sort()
@@ -2086,6 +2106,7 @@
 approach is to use the percentile rank to compute the index of the
 corresponding percentile:
 
+[source]
 ....
 def Percentile2(scores, percentile_rank):
     scores.sort()
@@ -2114,6 +2135,7 @@
 Here’s what that looks like as a function that takes a sequence,
 `+sample+`, and a value, `+x+`:
 
+[source]
 ....
 def EvalCdf(sample, x):
     count = 0.0
@@ -2166,9 +2188,9 @@
 value in the sample, latexmath:[\mathrm{CDF}(x)] is 0. If latexmath:[x]
 is greater than the largest value, latexmath:[\mathrm{CDF}(x)] is 1.
 
+[[example_cdf]]
 image::figs/cumulative_example_cdf.png[Example of a CDF.,height=240]
 
-[#example_cdf]#[example_cdf]#
 
 <<example_cdf>> is a graphical representation of
 this CDF. The CDF of a sample is a step function.
@@ -2184,15 +2206,15 @@
 * `+Value(p)+`: Given a probability `+p+`, computes the corresponding
 value, `+x+`; that is, the *inverse CDF* of `+p+`.
 
-image::figs/cumulative_prglngth_cdf.png[CDF of pregnancy
-length.,height=240]
+[[cumulative_prglngth_cdf]]
+image::figs/cumulative_prglngth_cdf.png[CDF of pregnancy length.,height=240]
 
-[#cumulative_prglngth_cdf]#[cumulative_prglngth_cdf]#
 
 The Cdf constructor can take as an argument a list of values, a pandas
 Series, a Hist, Pmf, or another Cdf. The following code makes a Cdf for
 the distribution of pregnancy lengths in the NSFG:
 
+[source]
 ....
     live, firsts, others = first.MakeFrames()
     cdf = thinkstats2.Cdf(live.prglngth, label='prglngth')
@@ -2201,6 +2223,7 @@
 `+thinkplot+` provides a function named `+Cdf+` that plots Cdfs as
 lines:
 
+[source]
 ....
     thinkplot.Cdf(cdf)
     thinkplot.Show(xlabel='weeks', ylabel='CDF')
@@ -2225,6 +2248,7 @@
 here is the code that plots the CDF of birth weight for first babies and
 others.
 
+[source]
 ....
     first_cdf = thinkstats2.Cdf(firsts.totalwgt_lb, label='first')
     other_cdf = thinkstats2.Cdf(others.totalwgt_lb, label='other')
@@ -2234,11 +2258,10 @@
     thinkplot.Show(xlabel='weight (pounds)', ylabel='CDF')
 ....
 
+[[cumulative_birthwgt_cdf]]
 image::figs/cumulative_birthwgt_cdf.png[CDF of birth weights for first
 babies and others.,height=240]
 
-[#cumulative_birthwgt_cdf]#[cumulative_birthwgt_cdf]#
-
 <<cumulative_birthwgt_cdf>> shows
 the result. Compared to
 <<nsfg_birthwgt_pmf>>, this figure makes
@@ -2287,6 +2310,7 @@
 
 Here’s how we can compute it. First, we make the Cdf of birth weights:
 
+[source]
 ....
     weights = live.totalwgt_lb
     cdf = thinkstats2.Cdf(weights, label='totalwgt_lb')
@@ -2295,6 +2319,7 @@
 Then we generate a sample and compute the percentile rank of each value
 in the sample.
 
+[source]
 ....
     sample = np.random.choice(weights, 100, replace=True)
     ranks = [cdf.PercentileRank(x) for x in sample]
@@ -2306,17 +2331,17 @@
 
 Finally we make and plot the Cdf of the percentile ranks.
 
+[source]
 ....
     rank_cdf = thinkstats2.Cdf(ranks)
     thinkplot.Cdf(rank_cdf)
     thinkplot.Show(xlabel='percentile rank', ylabel='CDF')
 ....
 
+[[cumulative_random]]
 image::figs/cumulative_random.png[CDF of percentile ranks for a random
 sample of birth weights.,height=240]
 
-[#cumulative_random]#[cumulative_random]#
-
 <<cumulative_random>> shows the result.
 The CDF is approximately a straight line, which means that the
 distribution is uniform.
@@ -2337,6 +2362,7 @@
 
 Cdf provides an implementation of this algorithm, called `+Random+`:
 
+[source]
 ....
 # class Cdf:
     def Random(self):
@@ -2360,6 +2386,7 @@
 More generally, given position and field size, we can compute percentile
 rank:
 
+[source]
 ....
 def PositionToPercentile(position, field_size):
     beat = field_size - position + 1
@@ -2378,6 +2405,7 @@
 I can answer that question by converting my percentile rank in M4049 to
 a position in M5059. Here’s the code:
 
+[source]
 ....
 def PercentileToPosition(percentile, field_size):
     beat = percentile * field_size / 100.0
@@ -2452,11 +2480,10 @@
 [[exponential]]
 === The exponential distribution
 
+[[analytic_expo_cdf]]
 image::figs/analytic_expo_cdf.png[CDFs of exponential distributions with
 various parameters.,height=240]
 
-[#analytic_expo_cdf]#[analytic_expo_cdf]#
-
 I’ll start with the *exponential distribution* because it is relatively
 simple. The CDF of the exponential distribution is
 
@@ -2482,6 +2509,7 @@
 all 44 babies was reported in the local paper; the complete dataset is
 in a file called `+babyboom.dat+`, in the `+ThinkStats2+` repository.
 
+[source]
 ....
     df = ReadBabyBoom()
     diffs = df.minutes.diff()
@@ -2495,11 +2523,10 @@
 columns `+time+`, `+sex+`, `+weight_g+`, and `+minutes+`, where
 `+minutes+` is time of birth converted to minutes since midnight.
 
+[[analytic_interarrival_cdf]]
 image::figs/analytic_interarrivals.png[CDF of interarrival times (left)
 and CCDF on a log-y scale (right).,height=240]
 
-[#analytic_interarrival_cdf]#[analytic_interarrival_cdf]#
-
 `+diffs+` is the difference between consecutive birth times, and `+cdf+`
 is the distribution of these interarrival times.
 <<analytic_interarrival_cdf>>
@@ -2527,6 +2554,7 @@
 So on a log-y scale the CCDF is a straight line with slope
 latexmath:[-\lambda]. Here’s how we can generate a plot like that:
 
+[source]
 ....
     thinkplot.Cdf(cdf, complement=True)
     thinkplot.Show(xlabel='minutes',
@@ -2562,10 +2590,10 @@
 out that there is a good reason for its ubiquity, which we will get to
 in <<CLT>>.
 
+[[analytic_gaussian_cdf]]
 image::figs/analytic_gaussian_cdf.png[CDF of normal distributions with a
 range of parameters.,height=240]
 
-[#analytic_gaussian_cdf]#[analytic_gaussian_cdf]#
 
 The normal distribution is characterized by two parameters: the mean,
 latexmath:[\mu], and standard deviation latexmath:[\sigma]. The normal
@@ -2593,6 +2621,7 @@
 `+EvalNormalCdf+`, which takes parameters `+mu+` and `+sigma+` and
 evaluates the CDF at `+x+`:
 
+[source]
 ....
 def EvalNormalCdf(x, mu=0, sigma=1):
     return scipy.stats.norm.cdf(x, loc=mu, scale=sigma)
@@ -2609,11 +2638,10 @@
 the empirical CDF of weights for all live births and the CDF of a normal
 distribution with the same mean and variance.
 
+[[analytic_birthwgt_model]]
 image::figs/analytic_birthwgt_model.png[CDF of birth weights with a
 normal model.,height=240]
 
-[#analytic_birthwgt_model]#[analytic_birthwgt_model]#
-
 The normal distribution is a good model for this dataset, so if we
 summarize the distribution with the parameters latexmath:[\mu = 7.28]
 and latexmath:[\sigma = 1.24], the resulting error (difference between
@@ -2649,14 +2677,15 @@
 `+thinkstats2+` provides `+NormalProbability+`, which takes a sample and
 returns two NumPy arrays:
 
+[source]
 ....
 xs, ys = thinkstats2.NormalProbability(sample)
 ....
 
+[[analytic_normal_prob_example]]
 image::figs/analytic_normal_prob_example.png[Normal probability plot for
 random samples from normal distributions.,height=240]
 
-[#analytic_normal_prob_example]#[analytic_normal_prob_example]#
 
 `+ys+` contains the sorted values from `+sample+`; `+xs+` contains the
 random values from the standard normal distribution.
@@ -2672,6 +2701,7 @@
 plots a gray line that represents the model and a blue line that
 represents the data.
 
+[source]
 ....
 def MakeNormalPlot(weights):
     mean = weights.mean()
@@ -2696,10 +2726,10 @@
 the standard normal distribution and values from `+weights+`. If the
 distribution of weights is normal, the data should match the model.
 
-image::figs/analytic_birthwgt_normal.png[Normal probability plot of birth
-weights.,height=240]
+[[analytic_birthwgt_normal]]
+.Normal probability plot of birth weights.
+image::figs/analytic_birthwgt_normal.png[height=240]
 
-[#analytic_birthwgt_normal]#[analytic_birthwgt_normal]#
 
 <<analytic_birthwgt_normal>> shows
 the results for all live births, and also for full term births
@@ -2716,11 +2746,9 @@
 Whether it is good enough for practical purposes depends on the
 purposes.
 
-[[brfss]]
+[[lognormal]]
 === The lognormal distribution
 
-[#lognormal]#[lognormal]#
-
 If the logarithms of a set of values have a normal distribution, the
 values have a *lognormal distribution*. The CDF of the lognormal
 distribution is the same as the CDF of the normal distribution, with
@@ -2737,10 +2765,10 @@
 standard deviation is ugly (see
 http://wikipedia.org/wiki/Log-normal_distribution).
 
+[[brfss_weight]]
 image::figs/brfss_weight.png[CDF of adult weights on a linear scale
 (left) and log scale (right).,height=240]
 
-[#brfss_weight]#[brfss_weight]#
 
 If a sample is approximately lognormal and you plot its CDF on a log-x
 scale, it will have the characteristic shape of a normal distribution.
@@ -2771,10 +2799,10 @@
 ASCII file that contains data from the BRFSS, and `+brfss.py+`, which
 reads the file and analyzes the data.
 
-image::figs/brfss_weight_normal.png[Normal probability plots for adult
-weight on a linear scale (left) and log scale (right).,height=240]
+[[brfss_weight_normal]]
+.Normal probability plots for adult weight on a linear scale (left) and log scale (right).
+image::figs/brfss_weight_normal.png[height=240]
 
-[#brfss_weight_normal]#[brfss_weight_normal]#
 
 <<brfss_weight>> (left) shows the distribution
 of adult weights on a linear scale with a normal model.
@@ -2810,10 +2838,10 @@
 shows CDFs of Pareto distributions with latexmath:[x_{m} = 0.5] and
 different values of latexmath:[\alpha].
 
-image::figs/analytic_pareto_cdf.png[CDFs of Pareto distributions with
-different parameters.,height=240]
+[[analytic_pareto_cdf]]
+.CDFs of Pareto distributions with different parameters.
+image::figs/analytic_pareto_cdf.png[height=240]
 
-[#analytic_pareto_cdf]#[analytic_pareto_cdf]#
 
 There is a simple visual test that indicates whether an empirical
 distribution fits a Pareto distribution: on a log-log scale, the CCDF
@@ -2840,10 +2868,10 @@
 U.S. Census Bureau publishes the population of every incorporated city
 and town in the United States.
 
-image::figs/populations_pareto.png[CCDFs of city and town populations, on
-a log-log scale.,height=240]
+[[populations_pareto]]
+.CCDFs of city and town populations, on a log-log scale.
+image::figs/populations_pareto.png[height=240]
 
-[#populations_pareto]#[populations_pareto]#
 
 I downloaded their data from
 http://www.census.gov/popest/data/cities/totals/2012/SUB-EST2012-3.html;
@@ -2868,11 +2896,9 @@
 The lognormal model is a better fit for the other 99%. Which model is
 appropriate depends on which part of the distribution is relevant.
 
-image::figs/populations_normal.png[CDF of city and town populations on a
-log-x scale (left), and normal probability plot of log-transformed
-populations (right).,height=240]
-
-[#populations_normal]#[populations_normal]#
+[[populations_normal]]
+.CDF of city and town populations on a log-x scale (left), and normal probability plot of log-transformed populations (right).
+image::figs/populations_normal.png[height=240]
 
 === Generating random numbers
 
@@ -2897,6 +2923,7 @@
 ++++
 So in Python we can write
 
+[source]
 ....
 def expovariate(lam):
     p = random.random()
@@ -2975,7 +3002,7 @@
 billion people in Pareto world, how many do we expect to be taller than
 1 km? How tall do we expect the tallest person to be?
 
-[#weibull]#[weibull]#
+[[weibull]]
 
 The Weibull distribution is a generalization of the exponential
 distribution that comes up in failure analysis (see
@@ -3023,7 +3050,7 @@
 distribution generated each file. If you are stumped, you can look in
 `+mystery.py+`, which contains the code that generated the files.
 
-[#income]#[income]#
+[[income]]
 
 The distributions of wealth and income are sometimes modeled using
 lognormal and Pareto distributions. To see which is better, let’s look
@@ -3115,6 +3142,7 @@
 For example, `+thinkstats2+` provides a class named `+NormalPdf+` that
 evaluates the normal density function.
 
+[source]
 ....
 class NormalPdf(Pdf):
 
@@ -3138,7 +3166,7 @@
 
 The following example creates a NormalPdf with the mean and variance of
 adult female heights, in cm, from the BRFSS (see
-<<brfss>>). Then it computes the density of the
+<<lognormal>>). Then it computes the density of the
 distribution at a location one standard deviation from the mean.
 
 ....
@@ -3173,12 +3201,11 @@
 `+mu - 3*sigma+` to `+mu + 3*sigma+`. Optionally, `+MakePmf+` and
 `+Render+` can take keyword arguments `+low+`, `+high+`, and `+n+`.
 
+[[pdf_example]]
 image::figs/pdf_example.png[A normal PDF that models adult female height
 in the U.S., and the kernel density estimate of a sample with
 latexmath:[n=500].,height=211]
 
-[#pdf_example]#[pdf_example]#
-
 === Kernel density estimation
 
 *Kernel density estimation* (KDE) is an algorithm that takes a sample
@@ -3188,6 +3215,7 @@
 `+scipy+` provides an implementation of KDE and `+thinkstats2+` provides
 a class called `+EstimatedPdf+` that uses it:
 
+[source]
 ....
 class EstimatedPdf(Pdf):
 
@@ -3242,10 +3270,10 @@
 
 === The distribution framework
 
-image::figs/distribution_functions.png[A framework that relates
-representations of distribution functions.,height=211]
-
-[#dist_framework]#[dist_framework]#
+[[dist_framework]]
+.A framework that relates
+representations of distribution functions.
+image::figs/distribution_functions.png[height=211]
 
 At this point we have seen PMFs, CDFs and PDFs; let’s take a minute to
 review. <<dist_framework>> shows how these
@@ -3298,6 +3326,7 @@
 These methods are all implemented with dictionary operations. For
 example:
 
+[source]
 ....
 # class _DictWrapper
 
@@ -3327,6 +3356,7 @@
 Pmf provides `+Normalize+`, which computes the sum of the probabilities
 and divides through by a factor:
 
+[source]
 ....
 # class Pmf
 
@@ -3373,6 +3403,7 @@
 Given a sequence, pandas Series, or dictionary, the constructor makes a
 Hist. Then it uses the Hist to initialize the attributes:
 
+[source]
 ....
         self.xs, freqs = zip(*sorted(dw.Items()))
         self.ps = np.cumsum(freqs, dtype=np.float)
@@ -3388,6 +3419,7 @@
 Here is the implementation of `+Prob+`, which takes a value and returns
 its cumulative probability:
 
+[source]
 ....
 # class Cdf
     def Prob(self, x):
@@ -3402,6 +3434,7 @@
 here is the implementation of `+Value+`, which takes a cumulative
 probability and returns the corresponding value:
 
+[source]
 ....
 # class Cdf
     def Value(self, p):
@@ -3416,6 +3449,7 @@
 consecutive cumulative probabilities. If you call the Cdf constructor
 and pass a Pmf, it computes differences by calling `+Cdf.Items+`:
 
+[source]
 ....
 # class Cdf
     def Items(self):
@@ -3448,6 +3482,7 @@
 ++++
 Or if you prefer Python notation:
 
+[source]
 ....
 def RawMoment(xs, k):
     return sum(x**k for x in xs) / len(xs)
@@ -3466,6 +3501,7 @@
 ++++
 Or in Python:
 
+[source]
 ....
 def CentralMoment(xs, k):
     mean = RawMoment(xs, 1)
@@ -3483,7 +3519,7 @@
 When you report moment-based statistics, it is important to think about
 the units. For example, if the values latexmath:[x_i] are in cm, the
 first raw moment is also in cm. But the second moment is in
-cmlatexmath:[^2], the third moment is in cmlatexmath:[^3], and so on.
+cm{}latexmath:[^2], the third moment is in cm{}latexmath:[^3], and so on.
 
 Because of these units, moments are hard to interpret by themselves.
 That’s why, for the second moment, it is common to report standard
@@ -3505,6 +3541,7 @@
 distribution. Given a sequence of values, latexmath:[x_i], the *sample
 skewness*, latexmath:[g_1], can be computed like this:
 
+[source]
 ....
 def StandardizedMoment(xs, k):
     var = CentralMoment(xs, 2)
@@ -3543,6 +3580,7 @@
 Where latexmath:[\bar{x}] is the sample mean, latexmath:[m] is the
 median, and latexmath:[S] is the standard deviation. Or in Python:
 
+[source]
 ....
 def Median(xs):
     cdf = thinkstats2.Cdf(xs)
@@ -3560,14 +3598,14 @@
 This statistic is *robust*, which means that it is less vulnerable to
 the effect of outliers.
 
-image::figs/density_totalwgt_kde.png[Estimated PDF of birthweight data
-from the NSFG.,height=211]
-
-[#density_totalwgt_kde]#[density_totalwgt_kde]#
+[[density_totalwgt_kde]]
+.Estimated PDF of birthweight data from the NSFG.
+image::figs/density_totalwgt_kde.png[height=211]
 
 As an example, let’s look at the skewness of birth weights in the NSFG
 pregnancy data. Here’s the code to estimate and plot the PDF:
 
+[source]
 ....
     live, firsts, others = first.MakeFrames()
     data = live.totalwgt_lb.dropna()
@@ -3582,14 +3620,14 @@
 skewness coefficients are negative: sample skewness is -0.59; Pearson’s
 median skewness is -0.23.
 
-image::figs/density_wtkg2_kde.png[Estimated PDF of adult weight data from
-the BRFSS.,height=211]
-
-[#density_wtkg2_kde]#[density_wtkg2_kde]#
+[[density_wtkg2_kde]]
+.Estimated PDF of adult weight data from the BRFSS.
+image::figs/density_wtkg2_kde.png[height=211]
 
 Now let’s compare this distribution to the distribution of adult weight
 in the BRFSS. Again, here’s the code:
 
+[source]
 ....
     df = brfss.ReadBrfss(nrows=None)
     data = df.wtkg2.dropna()
@@ -3701,6 +3739,7 @@
 
 Here’s the code that reads the data file and extracts height and weight:
 
+[source]
 ....
     df = brfss.ReadBrfss(nrows=None)
     sample = thinkstats2.SampleRows(df, 5000)
@@ -3708,7 +3747,7 @@
 ....
 
 `+SampleRows+` chooses a random subset of the data:
-
+[source]
 ....
 def SampleRows(df, nrows, replace=False):
     indices = np.random.choice(df.index, nrows, replace=replace)
@@ -3723,6 +3762,7 @@
 
 `+thinkplot+` provides `+Scatter+`, which makes scatter plots:
 
+[source]
 ....
     thinkplot.Scatter(heights, weights)
     thinkplot.Show(xlabel='Height (cm)',
@@ -3733,9 +3773,8 @@
 The result, in <<scatter1>> (left), shows the shape of the
 relationship. As we expected, taller people tend to be heavier.
 
-image::figs/scatter1.png[Scatter plots of weight versus height for the
-respondents in the BRFSS, unjittered (left), jittered
-(right).,height=288]
+.Scatter plots of weight versus height for the respondents in the BRFSS, unjittered (left), jittered (right).
+image::figs/scatter1.png[height=288]
 
 But this is not the best representation of the data, because the data
 are packed into columns. The problem is that the heights are rounded to
@@ -3748,6 +3787,7 @@
 were rounded to the nearest inch, they might be off by up to 0.5 inches
 or 1.3 cm. Similarly, the weights might be off by 0.5 kg.
 
+[source]
 ....
     heights = thinkstats2.Jitter(heights, 1.3)
     weights = thinkstats2.Jitter(weights, 0.5)
@@ -3755,6 +3795,7 @@
 
 Here’s the implementation of `+Jitter+`:
 
+[source]
 ....
 def Jitter(values, jitter=0.5):
     n = len(values)
@@ -3773,12 +3814,13 @@
 of the figure and gives disproportionate emphasis to outliers. This
 effect is called *saturation*.
 
-image::figs/scatter2.png[Scatter plot with jittering and transparency
-(left), hexbin plot (right).,height=288]
+.Scatter plot with jittering and transparency (left), hexbin plot (right).
+image::figs/scatter2.png[height=288]
 
 We can solve this problem with the `+alpha+` parameter, which makes the
 points partly transparent:
 
+[source]
 ....
     thinkplot.Scatter(heights, weights, alpha=0.2)
 ....
@@ -3799,6 +3841,7 @@
 divides the graph into hexagonal bins and colors each bin according to
 how many data points fall in it. `+thinkplot+` provides `+HexBin+`:
 
+[source]
 ....
     thinkplot.HexBin(heights, weights)
 ....
@@ -3822,6 +3865,7 @@
 
 NumPy and pandas provide functions for binning data:
 
+[source]
 ....
     df = df.dropna(subset=['htm3', 'wtkg2'])
     bins = np.arange(135, 210, 5)
@@ -3838,14 +3882,15 @@
 fall below the lowest bin are mapped to index 0. Values above the
 highest bin are mapped to `+len(bins)+`.
 
-image::figs/scatter3.png[Percentiles of weight for a range of height
-bins.,height=240]
+.Percentiles of weight for a range of height bins.
+image::figs/scatter3.png[height=240]
 
 `+groupby+` is a DataFrame method that returns a GroupBy object; used in
 a `+for+` loop, `+groups+` iterates the names of the groups and the
 DataFrames that represent them. So, for example, we can print the number
 of rows in each group like this:
 
+[source]
 ....
 for i, group in groups:
     print(i, len(group))
@@ -3853,6 +3898,7 @@
 
 Now for each group we can compute the mean height and the CDF of weight:
 
+[source]
 ....
     heights = [group.htm3.mean() for i, group in groups]
     cdfs = [thinkstats2.Cdf(group.wtkg2) for i, group in groups]
@@ -3860,6 +3906,7 @@
 
 Finally, we can plot percentiles of weight versus height:
 
+[source]
 ....
     for percent in [75, 50, 25]:
         weights = [cdf.Percentile(percent) for cdf in cdfs]
@@ -3946,6 +3993,7 @@
 orthogonal, and negative if they point in opposite directions.
 `+thinkstats2+` uses `+np.dot+` to implement `+Cov+` efficiently:
 
+[source]
 ....
 def Cov(xs, ys, meanx=None, meany=None):
     xs = np.asarray(xs)
@@ -4008,6 +4056,7 @@
 
 Here is the implementation in `+thinkstats2+`:
 
+[source]
 ....
 def Corr(xs, ys):
     xs = np.asarray(xs)
@@ -4046,10 +4095,9 @@
 If there’s a nonlinear relationship, latexmath:[\rho] understates its
 strength.
 
-image:figs/Correlation_examples.png[Examples of datasets with a range of
-correlations.,height=240]
-
-[#corr_examples]#[corr_examples]#
+[[corr_examples]]
+.Examples of datasets with a range of correlations.
+image::figs/Correlation_examples.png[height=240]
 
 <<corr_examples>> is from
 http://wikipedia.org/wiki/Correlation_and_dependence. It shows scatter
@@ -4082,6 +4130,7 @@
 `+thinkstats2+` provides a function that computes Spearman’s rank
 correlation:
 
+[source]
 ....
 def SpearmanCorr(xs, ys):
     xranks = pandas.Series(xs).rank()
@@ -4095,6 +4144,7 @@
 
 I could also use `+Series.corr+` directly and specify Spearman’s method:
 
+[source]
 ....
 def SpearmanCorr(xs, ys):
     xs = pandas.Series(xs)
@@ -4118,6 +4168,7 @@
 of skewness is to compute Pearson’s correlation with log-weight and
 height:
 
+[source]
 ....
     thinkstats2.Corr(df.htm3, np.log(df.wtkg2)))
 ....
@@ -4216,6 +4267,7 @@
 natural division of subjects into groups in ways that are at least
 approximately random.
 
+[[estimation]]
 == Estimation
 
 The code for this chapter is in `+estimation.py+`. For information about
@@ -4274,6 +4326,7 @@
 Here is a function that simulates the estimation game and computes the
 root mean squared error (RMSE), which is the square root of MSE:
 
+[source]
 ....
 def Estimate1(n=7, m=1000):
     mu = 0
@@ -4298,6 +4351,7 @@
 
 Here’s the function that computes RMSE:
 
+[source]
 ....
 def RMSE(estimates, actual):
     e2 = [(estimate-actual)**2 for estimate in estimates]
@@ -4373,6 +4427,7 @@
 Here is a function that simulates the estimation game and tests the
 performance of latexmath:[S^2] and latexmath:[S_{n-1}^2]:
 
+[source]
 ....
 def Estimate2(n=7, m=1000):
     mu = 0
@@ -4401,6 +4456,7 @@
 `+MeanError+` computes the mean difference between the estimates and the
 actual value:
 
+[source]
 ....
 def MeanError(estimates, actual):
     errors = [estimate-actual for estimate in estimates]
@@ -4464,6 +4520,7 @@
 
 The following function answers that question:
 
+[source]
 ....
 def SimulateSample(mu=90, sigma=7.5, n=9, m=1000):
     means = []
@@ -4481,8 +4538,8 @@
 `+n+` is the sample size, the number of gorillas we measured. `+m+` is
 the number of times we run the simulation.
 
-image::figs/estimation1.png[Sampling distribution of latexmath:[\bar{x}],
-with confidence interval.,height=240]
+.Sampling distribution of latexmath:[\bar{x}], with confidence interval.
+image::figs/estimation1.png[height=240]
 
 In each iteration, we choose `+n+` values from a normal distribution
 with the given parameters, and compute the sample mean, `+xbar+`. We run
@@ -4615,6 +4672,7 @@
 To test the performance of these estimators, we can simulate the
 sampling process:
 
+[source]
 ....
 def Estimate3(n=7, m=1000):
     lam = 2
@@ -4736,7 +4794,7 @@
 
 There are several ways we could formulate this question, including
 Fisher null hypothesis testing, Neyman-Pearson decision theory, and
-Bayesian inferencefootnote:[For more about Bayesian inference, see the
+Bayesian inference{}footnote:[For more about Bayesian inference, see the
 sequel to this book, _Think Bayes_.]. What I present here is a subset of
 all three that makes up most of what people use in practice, which I
 will call *classical hypothesis testing*.
@@ -4782,6 +4840,7 @@
 `+thinkstats2+` provides `+HypothesisTest+`, a class that represents the
 structure of a classical hypothesis test. Here is the definition:
 
+[source]
 ....
 class HypothesisTest(object):
 
@@ -4824,13 +4883,14 @@
 fraction of elements in `+test_stats+` that exceed or equal the observed
 test statistic, `+self.actual+`.
 
-As a simple examplefootnote:[Adapted from MacKay, _Information Theory,
+As a simple example{}footnote:[Adapted from MacKay, _Information Theory,
 Inference, and Learning Algorithms_, 2003.], suppose we toss a coin 250
 times and see 140 heads and 110 tails. Based on this result, we might
 suspect that the coin is biased; that is, more likely to land heads. To
 test this hypothesis, we compute the probability of seeing such a
 difference if the coin is actually fair:
 
+[source]
 ....
 class CoinTest(thinkstats2.HypothesisTest):
 
@@ -4858,6 +4918,7 @@
 
 Now all we have to do is instantiate `+CoinTest+` and call `+PValue+`:
 
+[source]
 ....
     ct = CoinTest((140, 110))
     pvalue = ct.PValue()
@@ -4895,6 +4956,7 @@
 *permutation*; that is, we can take values for first babies and others
 and shuffle them, treating the two groups as one big group:
 
+[source]
 ....
 class DiffMeansPermute(thinkstats2.HypothesisTest):
 
@@ -4927,6 +4989,7 @@
 
 To test the difference in pregnancy length, we run:
 
+[source]
 ....
     live, firsts, others = first.MakeFrames()
     data = firsts.prglngth.values, others.prglngth.values
@@ -4941,12 +5004,13 @@
 a difference as big as the observed effect about 17% of the time. So
 this effect is not statistically significant.
 
-image::figs/hypothesis1.png[CDF of difference in mean pregnancy length
-under the null hypothesis.,height=240]
+.CDF of difference in mean pregnancy length under the null hypothesis.
+image::figs/hypothesis1.png[height=240]
 
 `+HypothesisTest+` provides `+PlotCdf+`, which plots the distribution of
 the test statistic and a gray line indicating the observed effect size:
 
+[source]
 ....
     ht.PlotCdf()
     thinkplot.Show(xlabel='test statistic',
@@ -4974,6 +5038,7 @@
 then we would not take the absolute value of the difference; instead we
 would use this test statistic:
 
+[source]
 ....
 class DiffMeansOneSided(DiffMeansPermute):
 
@@ -5004,6 +5069,7 @@
 to be on time. So we might hypothesize that the standard deviation is
 higher. Here’s how we can test that:
 
+[source]
 ....
 class DiffStdPermute(DiffMeansPermute):
 
@@ -5035,6 +5101,7 @@
 world where the distributions of age and birth weight are the same, but
 where the variables are unrelated:
 
+[source]
 ....
 class CorrelationPermute(thinkstats2.HypothesisTest):
 
@@ -5055,6 +5122,7 @@
 
 Here’s the code that reads the data and runs the test:
 
+[source]
 ....
     live, firsts, others = first.MakeFrames()
     live = live.dropna(subset=['agepreg', 'totalwgt_lb'])
@@ -5083,7 +5151,7 @@
 confiscate the die, but now you have to prove that it is crooked. You
 roll the die 60 times and get the following results:
 
-[cols="<,^,^,^,^,^,^",options="header",]
+[cols="<,^,^,^,^,^,^",options="header,autowidth",]
 |===
 |Value |1 |2 |3 |4 |5 |6
 |Frequency |8 |9 |19 |5 |8 |11
@@ -5102,6 +5170,7 @@
 
 Here’s a version of `+HypothesisTest+` that answers that question:
 
+[source]
 ....
 class DiceTest(thinkstats2.HypothesisTest):
 
@@ -5147,6 +5216,7 @@
 Where latexmath:[O_i] are the observed frequencies and latexmath:[E_i]
 are the expected frequencies. Here’s the Python code:
 
+[source]
 ....
 class DiceChiTest(DiceTest):
 
@@ -5187,6 +5257,7 @@
 
 The code combines elements from previous examples:
 
+[source]
 ....
 class PregLengthTest(thinkstats2.HypothesisTest):
 
@@ -5217,6 +5288,7 @@
 
 Here’s the code that computes the test statistic:
 
+[source]
 ....
 # class PregLengthTest:
 
@@ -5293,6 +5365,7 @@
 accurate, we can use the observed samples as a model of the population
 and run hypothesis tests with simulated data:
 
+[source]
 ....
 def FalseNegRate(data, num_runs=100):
     group1, group2 = data
@@ -5318,6 +5391,7 @@
 `+Resample+` takes a sequence and draws a sample with the same length,
 with replacement:
 
+[source]
 ....
 def Resample(xs):
     return np.random.choice(xs, len(xs), replace=True)
@@ -5325,6 +5399,7 @@
 
 Here’s the code that tests pregnancy lengths:
 
+[source]
 ....
     live, firsts, others = first.MakeFrames()
     data = firsts.prglngth.values, others.prglngth.values
@@ -5482,6 +5557,7 @@
 But unless the correlation is perfect, this prediction is only
 approximate. The vertical deviation from the line, or *residual*, is
 
+[source]
 ....
 res = ys - (inter + slope * xs)
 ....
@@ -5528,6 +5604,7 @@
 `+thinkstats2+` provides simple functions that demonstrate linear least
 squares:
 
+[source]
 ....
 def LeastSquares(xs, ys):
     meanx, varx = MeanVar(xs)
@@ -5547,6 +5624,7 @@
 `+thinkstats2+` also provides `+FitLine+`, which takes `+inter+` and
 `+slope+` and returns the fitted line for a sequence of `+xs+`.
 
+[source]
 ....
 def FitLine(xs, inter, slope):
     fit_xs = np.sort(xs)
@@ -5557,6 +5635,7 @@
 We can use these functions to compute the least squares fit for birth
 weight as a function of mother’s age.
 
+[source]
 ....
     live, firsts, others = first.MakeFrames()
     live = live.dropna(subset=['agepreg', 'totalwgt_lb'])
@@ -5578,8 +5657,8 @@
 year old mother is 7.3 pounds. The slope is 0.27 ounces per year, or
 0.17 pounds per decade.
 
-image::figs/linear1.png[Scatter plot of birth weight and mother’s age
-with a linear fit.,height=240]
+.Scatter plot of birth weight and mother’s age with a linear fit.
+image::figs/linear1.png[height=240]
 
 <<linear1>> shows a scatter plot of birth weight and age
 along with the fitted line. It’s a good idea to look at a figure like
@@ -5591,6 +5670,7 @@
 Another useful test is to plot the residuals. `+thinkstats2+` provides a
 function that computes residuals:
 
+[source]
 ....
 def Residuals(xs, ys, inter, slope):
     xs = np.asarray(xs)
@@ -5603,7 +5683,8 @@
 `+inter+` and `+slope+`. It returns the differences between the actual
 values and the fitted line.
 
-image::figs/linear2.png[Residuals of the linear fit.,height=240]
+.Residuals of the linear fit.
+image::figs/linear2.png[height=240]
 
 To visualize the residuals, I group respondents by age and compute
 percentiles in each group, as we saw in
@@ -5640,6 +5721,7 @@
 observed pregnancies as if they were the entire population and draw
 samples, with replacement, from the observed sample.
 
+[source]
 ....
 def SamplingDistributions(live, iters=101):
     t = []
@@ -5661,6 +5743,7 @@
 `+thinkstats2+` also provides `+ResampleRows+`, which returns a sample
 the same size as the original:
 
+[source]
 ....
 def ResampleRows(df):
     return SampleRows(df, len(df), replace=True)
@@ -5673,6 +5756,7 @@
 I summarize the sampling distributions by printing the standard error
 and confidence interval:
 
+[source]
 ....
 def Summarize(estimates, actual=None):
     mean = thinkstats2.Mean(estimates)
@@ -5696,6 +5780,7 @@
 the fitted lines, or for a less cluttered representation, plot a 90%
 confidence interval for each age. Here’s the code:
 
+[source]
 ....
 def PlotConfidenceIntervals(xs, inters, slopes,
                             percent=90, **options):
@@ -5721,9 +5806,8 @@
 interval, it selects the 5th and 95th percentiles. `+FillBetween+` draws
 a polygon that fills the space between two lines.
 
-image::figs/linear3.png[50% and 90% confidence intervals showing
-variability in the fitted line due to sampling error of `+inter+` and
-`+slope+`.,height=240]
+.50% and 90% confidence intervals showing variability in the fitted line due to sampling error of `+inter+` and `+slope+`.
+image::figs/linear3.png[height=240]
 
 <<linear3>> shows the 50% and 90% confidence intervals
 for curves fitted to birth weight as a function of mother’s age. The
@@ -5751,6 +5835,7 @@
 determination*, usually denoted latexmath:[R^2] and called
 "`R-squared`":
 
+[source]
 ....
 def CoefDetermination(ys, res):
     return 1 - Var(res) / Var(ys)
@@ -5827,6 +5912,7 @@
 can model the birth weights as random variations around their mean.
 Here’s a HypothesisTest for this model:
 
+[source]
 ....
 class SlopeTest(thinkstats2.HypothesisTest):
 
@@ -5854,6 +5940,7 @@
 
 Here’s the code that runs the hypothesis test:
 
+[source]
 ....
     live, firsts, others = first.MakeFrames()
     live = live.dropna(subset=['agepreg', 'totalwgt_lb'])
@@ -5878,10 +5965,8 @@
 distributions are identical. The distributions are also symmetric, for
 reasons we will see in <<CLT>>.
 
-image::figs/linear4.png[The sampling distribution of the estimated slope
-and the distribution of slopes generated under the null hypothesis. The
-vertical lines are at 0 and the observed slope, 0.017
-lbs/year.,height=240]
+.The sampling distribution of the estimated slope and the distribution of slopes generated under the null hypothesis. The vertical lines are at 0 and the observed slope, 0.017 lbs/year.
+image::figs/linear4.png[height=240]
 
 So we could estimate the p-value two ways:
 
@@ -5900,6 +5985,7 @@
 Here’s the code that estimates the p-value of the slope using the
 sampling distribution:
 
+[source]
 ....
     inters, slopes = SamplingDistributions(live, iters=1001)
     slope_cdf = thinkstats2.Cdf(slopes)
@@ -5945,6 +6031,7 @@
 according to the weights in `+finalwgt+`, and returns a DataFrame
 containing the resampled rows:
 
+[source]
 ....
 def ResampleRowsWeighted(df, column='finalwgt'):
     weights = df[column]
@@ -5965,6 +6052,7 @@
 Now we can compare the effect of resampling with and without weights.
 Without weights, we generate the sampling distribution like this:
 
+[source]
 ....
     estimates = [ResampleRows(live).totalwgt_lb.mean()
                  for _ in range(iters)]
@@ -5972,6 +6060,7 @@
 
 With weights, it looks like this:
 
+[source]
 ....
     estimates = [ResampleRowsWeighted(live).totalwgt_lb.mean()
                  for _ in range(iters)]
@@ -5979,10 +6068,9 @@
 
 The following table summarizes the results:
 
-[cols="<,^,^,^",]
+[cols="<,^,^,^",options="header,autowidth"]
 |===
-| |mean birth |standard |90% CI
-| |weight (lbs) |error |
+| |mean birth weight (lbs) |standard error |90% CI
 |Unweighted |7.27 |0.014 |(7.24, 7.29)
 |Weighted |7.35 |0.014 |(7.32, 7.37)
 |===
@@ -6081,6 +6169,7 @@
 As an example, I’ll run the model from the previous chapter with
 StatsModels:
 
+[source]
 ....
     import statsmodels.formula.api as smf
 
@@ -6107,6 +6196,7 @@
 that maps from variable names to their parameters, so we can get the
 intercept and slope like this:
 
+[source]
 ....
     inter = results.params['Intercept']
     slope = results.params['agepreg']
@@ -6119,6 +6209,7 @@
 p-values, so we can check whether the estimated slope is statistically
 significant:
 
+[source]
 ....
     slope_pvalue = results.pvalues['agepreg']
 ....
@@ -6138,6 +6229,7 @@
 The results object provides `+summary()+`, which represents the results
 in a readable format.
 
+[source]
 ....
     print(results.summary())
 ....
@@ -6178,6 +6270,7 @@
 plausible. Then we’ll use multiple regression to investigate more
 carefully. First, let’s see how big the difference in weight is:
 
+[source]
 ....
 diff_weight = firsts.totalwgt_lb.mean() - others.totalwgt_lb.mean()
 ....
@@ -6185,6 +6278,7 @@
 First babies are 0.125 lbs lighter, or 2 ounces. And the difference in
 ages:
 
+[source]
 ....
 diff_age = firsts.agepreg.mean() - others.agepreg.mean()
 ....
@@ -6192,6 +6286,7 @@
 The mothers of first babies are 3.59 years younger. Running the linear
 model again, we get the change in birth weight as a function of age:
 
+[source]
 ....
 results = smf.ols('totalwgt_lb ~ agepreg', data=live).fit()
 slope = results.params['agepreg']
@@ -6201,6 +6296,7 @@
 difference in ages, we get the expected difference in birth weight for
 first babies and others, due to mother’s age:
 
+[source]
 ....
 slope * diff_age
 ....
@@ -6212,6 +6308,7 @@
 Using multiple regression, we can explore these relationships more
 systematically.
 
+[source]
 ....
     live['isfirst'] = live.birthord == 1
     formula = 'totalwgt_lb ~ isfirst'
@@ -6281,6 +6378,7 @@
 One option is to create a column, `+agepreg2+`, that contains the
 squares of the ages:
 
+[source]
 ....
     live['agepreg2'] = live.agepreg**2
     formula = 'totalwgt_lb ~ isfirst + agepreg + agepreg2'
@@ -6313,7 +6411,7 @@
 
 The following table summarizes the results of these regressions:
 
-[cols="<,^,^,^,^",options="header",]
+[cols="<,^,^,^,^",options="header,autowidth",]
 |===
 | |isfirst |agepreg |agepreg2 |latexmath:[R^2]
 |Model 1 |-0.125 * |– |– |0.002
@@ -6369,6 +6467,7 @@
 https://en.wikipedia.org/wiki/Join_(SQL)). Join is implemented as a
 DataFrame method, so we can perform the operation like this:
 
+[source]
 ....
     live = live[live.prglngth>30]
     resp = chap01soln.ReadFemResp()
@@ -6400,6 +6499,7 @@
 than a second on an ordinary desktop computer. Now we can start testing
 variables.
 
+[source]
 ....
     t = []
     for name in join.columns:
@@ -6438,6 +6538,7 @@
 The next step is to sort the results and select the variables that yield
 the highest values of latexmath:[R^2].
 
+[source]
 ....
     t.sort(reverse=True)
     for mse, name in t[:30]:
@@ -6492,6 +6593,7 @@
 Having identified potential explanatory variables, I tested a few models
 and settled on this one:
 
+[source]
 ....
     formula = ('totalwgt_lb ~ agepreg + C(race) + babysex==1 + '
                'nbrnaliv>1 + paydu==1 + totincr')
@@ -6598,12 +6700,14 @@
 Odds and probabilities are different representations of the same
 information. Given a probability, you can compute the odds like this:
 
+[source]
 ....
     o = p / (1-p)
 ....
 
 Given odds in favor, you can convert to probability like this:
 
+[source]
 ....
     p = o / (o+1)
 ....
@@ -6623,6 +6727,7 @@
 latexmath:[x_2]. We can compute the predicted value of
 latexmath:[\log o], and then convert to a probability:
 
+[source]
 ....
     o = np.exp(log_o)
     p = o / (o+1)
@@ -6707,6 +6812,7 @@
 
 Again, I load the NSFG data and select pregnancies longer than 30 weeks:
 
+[source]
 ....
     live, firsts, others = first.MakeFrames()
     df = live[live.prglngth>30]
@@ -6716,6 +6822,7 @@
 boolean), so I create a new column named `+boy+`, using `+astype(int)+`
 to convert to binary integers:
 
+[source]
 ....
     df['boy'] = (df.babysex==1).astype(int)
 ....
@@ -6725,6 +6832,7 @@
 see if these effects appear in the NSFG data. I’ll start with the
 mother’s age:
 
+[source]
 ....
     import statsmodels.formula.api as smf
 
@@ -6741,6 +6849,7 @@
 they are NumPy arrays, it is sometimes convenient to convert them to
 DataFrames:
 
+[source]
 ....
     endog = pandas.DataFrame(model.endog, columns=[model.endog_names])
     exog = pandas.DataFrame(model.exog, columns=model.exog_names)
@@ -6766,6 +6875,7 @@
 comparing models. For example, here’s a model that includes several
 factors believed to be associated with sex ratio:
 
+[source]
 ....
     formula = 'boy ~ agepreg + hpagelb + birthord + C(race)'
     model = smf.logit(formula, data=df)
@@ -6800,6 +6910,7 @@
 strategy is to guess "`boy`" every time. The accuracy of this strategy
 is just the fraction of boys:
 
+[source]
 ....
     actual = endog['boy']
     baseline = actual.mean()
@@ -6810,6 +6921,7 @@
 
 Here’s how we compute the accuracy of the model:
 
+[source]
 ....
     predict = (results.predict() >= 0.5)
     true_pos = predict * actual
@@ -6824,6 +6936,7 @@
 Similarly, `+true_neg+` indicates the cases where we guess "`girl`" and
 get it right. Accuracy is the fraction of correct guesses:
 
+[source]
 ....
     acc = (sum(true_pos) + sum(true_neg)) / len(actual)
 ....
@@ -6837,6 +6950,7 @@
 pool. Suppose your friend is 35 years old and white, her husband is 39,
 and they are expecting their third child:
 
+[source]
 ....
     columns = ['agepreg', 'hpagelb', 'birthord', 'race']
     new = pandas.DataFrame([[35, 39, 3, 2]], columns=columns)
@@ -6962,6 +7076,7 @@
 The data I downloaded from Mr. Jones’s site is in the repository for
 this book. The following code reads it into a pandas DataFrame:
 
+[source]
 ....
     transactions = pandas.read_csv('mj-clean.csv', parse_dates=[5])
 ....
@@ -6993,6 +7108,7 @@
 by reported quality, and then transform each group into an equally
 spaced series by computing the mean daily price per gram.
 
+[source]
 ....
 def GroupByQualityAndDay(transactions):
     groups = transactions.groupby('quality')
@@ -7011,6 +7127,7 @@
 The loop iterates through the groups and calls `+GroupByDay+`, which
 computes the daily average price and returns a new DataFrame:
 
+[source]
 ....
 def GroupByDay(transactions, func=np.mean):
     grouped = transactions[['date', 'ppg']].groupby('date')
@@ -7051,6 +7168,7 @@
 DataFrame of daily prices. Here’s the code I use to plot the three time
 series:
 
+[source]
 ....
     thinkplot.PrePlot(rows=3)
     for i, (name, daily) in enumerate(dailies.items()):
@@ -7074,8 +7192,8 @@
 Since the labels on the x-axis are dates, I use `+pyplot.xticks+` to
 rotate the "`ticks`" 30 degrees, making them more readable.
 
-image::figs/timeseries1.png[Time series of daily price per gram for high,
-medium, and low quality cannabis.,width=336]
+.Time series of daily price per gram for high, medium, and low quality cannabis.
+image::figs/timeseries1.png[width=336]
 
 <<timeseries1>> shows the result. One apparent feature in
 these plots is a gap around November 2013. It’s possible that data
@@ -7098,6 +7216,7 @@
 of daily prices and computes a least squares fit, returning the model
 and results objects from StatsModels:
 
+[source]
 ....
 def RunLinearModel(daily):
     model = smf.ols('ppg ~ years', data=daily)
@@ -7107,6 +7226,7 @@
 
 Then we can iterate through the qualities and fit a model to each:
 
+[source]
 ....
     for name, daily in dailies.items():
         model, results = RunLinearModel(daily)
@@ -7116,7 +7236,7 @@
 
 Here are the results:
 
-[cols="<,<,<,^",options="header",]
+[cols="<,<,<,^",options="header,autowidth",]
 |===
 |quality |intercept |slope |latexmath:[R^2]
 |high |13.450 |-0.708 |0.444
@@ -7138,6 +7258,7 @@
 
 The following code plots the observed prices and the fitted values:
 
+[source]
 ....
 def PlotFittedValues(model, results, label=''):
     years = model.exog[:,1]
@@ -7150,8 +7271,8 @@
 `+exog+` and `+endog+`, NumPy arrays with the exogenous (explanatory)
 and endogenous (dependent) variables.
 
-image::figs/timeseries2.png[Time series of daily price per gram for high
-quality cannabis, and a linear least squares fit.,height=240]
+.Time series of daily price per gram for high quality cannabis, and a linear least squares fit.
+image::figs/timeseries2.png[height=240]
 
 `+PlotFittedValues+` makes a scatter plot of the data points and a line
 plot of the fitted values. <<timeseries2>> shows the
@@ -7218,6 +7339,7 @@
 represent this missing data explicitly. We can do that by "`reindexing`"
 the DataFrame:
 
+[source]
 ....
     dates = pandas.date_range(daily.index.min(), daily.index.max())
     reindexed = daily.reindex(dates)
@@ -7230,6 +7352,7 @@
 
 Now we can plot the rolling mean like this:
 
+[source]
 ....
     roll_mean = pandas.rolling_mean(reindexed.ppg, 30)
     thinkplot.Plot(roll_mean.index, roll_mean)
@@ -7238,8 +7361,8 @@
 The window size is 30, so each value in `+roll_mean+` is the mean of 30
 values from `+reindexed.ppg+`.
 
-image::figs/timeseries10.png[Daily price and a rolling mean (left) and
-exponentially-weighted moving average (right).,height=240]
+.Daily price and a rolling mean (left) and exponentially-weighted moving average (right).
+image::figs/timeseries10.png[height=240]
 
 <<timeseries10>> (left) shows the result. The rolling
 mean seems to do a good job of smoothing out the noise and extracting
@@ -7253,6 +7376,7 @@
 the weights for previous values drop off exponentially. Second, the
 pandas implementation of EWMA handles missing values better.
 
+[source]
 ....
     ewma = pandas.ewma(reindexed.ppg, span=30)
     thinkplot.Plot(ewma.index, ewma)
@@ -7281,6 +7405,7 @@
 A simple and common way to fill missing data is to use a moving average.
 The Series method `+fillna+` does just what we want:
 
+[source]
 ....
     reindexed.ppg.fillna(ewma, inplace=True)
 ....
@@ -7292,6 +7417,7 @@
 A drawback of this method is that it understates the noise in the
 series. We can solve that problem by adding in resampled residuals:
 
+[source]
 ....
     resid = (reindexed.ppg - ewma).dropna()
     fake_data = ewma + thinkstats2.Resample(resid, len(reindexed))
@@ -7303,7 +7429,8 @@
 random sample of residuals. Finally, `+fillna+` replaces `+nan+` with
 values from `+fake_data+`.
 
-image::figs/timeseries8.png[Daily price with filled data.,height=240]
+.Daily price with filled data.
+image::figs/timeseries8.png[height=240]
 
 <<timeseries8>> shows the result. The filled data is
 visually similar to the actual values. Since the resampled residuals are
@@ -7322,6 +7449,7 @@
 interval called a *lag*, and then compute the correlation of the shifted
 series with the original:
 
+[source]
 ....
 def SerialCorr(series, lag=1):
     xs = series[lag:]
@@ -7344,6 +7472,7 @@
 subtract away the trend. For example, we can compute the residual of the
 EWMA and then compute its serial correlation:
 
+[source]
 ....
     ewma = pandas.ewma(reindexed.ppg, span=30)
     resid = reindexed.ppg - ewma
@@ -7358,7 +7487,7 @@
 To check for weekly, monthly, and yearly seasonality, I ran the analysis
 again with different lags. Here are the results:
 
-[cols="^,^,^,^",options="header",]
+[cols="^,^,^,^",options="header,autowidth",]
 |===
 |lag |high |medium |low
 |1 |-0.029 |-0.014 |0.034
@@ -7385,6 +7514,7 @@
 analysis, including `+acf+`, which computes the autocorrelation
 function:
 
+[source]
 ....
     import statsmodels.tsa.stattools as smtsa
     acf = smtsa.acf(filled.resid, nlags=365, unbiased=True)
@@ -7405,9 +7535,8 @@
 With `+lag=0+`, `+acf+` computes the correlation of the series with
 itself, which is always 1.
 
-image::figs/timeseries9.png[Autocorrelation function for daily prices
-(left), and daily prices with a simulated weekly seasonality
-(right).,height=240]
+.Autocorrelation function for daily prices (left), and daily prices with a simulated weekly seasonality (right).
+image::figs/timeseries9.png[height=240]
 
 <<timeseries9>> (left) shows autocorrelation functions
 for the three quality categories, with `+nlags=40+`. The gray region
@@ -7430,6 +7559,7 @@
 fall on Friday or Saturday and add a random amount to the price, chosen
 from a uniform distribution from $0 to $2.
 
+[source]
 ....
 def AddWeeklySeasonality(daily):
     frisat = (daily.index.dayofweek==4) | (daily.index.dayofweek==5)
@@ -7461,6 +7591,7 @@
 which takes a DataFrame containing the explanatory variables and returns
 a sequence of predictions. Here’s the code:
 
+[source]
 ....
 def GenerateSimplePrediction(results, years):
     n = len(years)
@@ -7509,6 +7640,7 @@
 parameters are correct, but the random residuals could have been
 different. Here is a function that runs the simulations:
 
+[source]
 ....
 def SimulateResults(daily, iters=101):
     model, results = RunLinearModel(daily)
@@ -7537,6 +7669,7 @@
 
 The next step is to use the simulated results to generate predictions:
 
+[source]
 ....
 def GeneratePredictions(result_seq, years, add_resid=False):
     n = len(years)
@@ -7560,12 +7693,13 @@
 prediction. `+GeneratePredictions+` iterates through the sequence of
 RegressionResults and generates a sequence of predictions.
 
-image::figs/timeseries4.png[Predictions based on linear fits, showing
-variation due to sampling error and prediction error.,height=240]
+.Predictions based on linear fits, showing variation due to sampling error and prediction error.
+image::figs/timeseries4.png[height=240]
 
 Finally, here’s the code that plots a 90% confidence interval for the
 predictions:
 
+[source]
 ....
 def PlotPredictions(daily, years, iters=101, percent=90):
     result_seq = SimulateResults(daily, iters=iters)
@@ -7613,8 +7747,8 @@
 `+SimulateResults+` to use intervals of observation with different start
 and end dates. My implementation is in `+timeseries.py+`.
 
-image::figs/timeseries5.png[Predictions based on linear fits, showing
-variation due to the interval of observation.,height=240]
+.Predictions based on linear fits, showing variation due to the interval of observation.
+image::figs/timeseries5.png[height=240]
 
 <<timeseries5>> shows the result for the medium quality
 category. The lightest gray area shows a confidence interval that
@@ -7750,6 +7884,7 @@
 For example, in the NSFG dataset, we know the duration of 11189 complete
 pregnancies. We can read this data and compute the CDF:
 
+[source]
 ....
     preg = nsfg.ReadFemPreg()
     complete = preg.query('outcome in [1, 3, 4]').prglngth
@@ -7764,13 +7899,14 @@
 The DataFrame method `+query+` takes a boolean expression and evaluates
 it for each row, selecting the rows that yield True.
 
-image::figs/survival1.png[Cdf and survival curve for pregnancy length
-(top), hazard curve (bottom).,height=288]
+.Cdf and survival curve for pregnancy length (top), hazard curve (bottom).
+image::figs/survival1.png[height=288]
 
 <<survival1>> (top) shows the CDF of pregnancy length and
 its complement, the survival curve. To represent the survival curve, I
 define an object that wraps a Cdf and adapts the interface:
 
+[source]
 ....
 class SurvivalFunction(object):
     def __init__(self, cdf, label=''):
@@ -7794,6 +7930,7 @@
 We can instantiate a `+SurvivalFunction+` by passing the CDF of
 lifetimes:
 
+[source]
 ....
     sf = SurvivalFunction(cdf)
 ....
@@ -7801,6 +7938,7 @@
 `+SurvivalFunction+` also provides `+__getitem__+` and `+Prob+`, which
 evaluates the survival curve:
 
+[source]
 ....
 # class SurvivalFunction
 
@@ -7827,6 +7965,7 @@
 `+SurvivalFunction+` provides `+Render+`, so we can plot `+sf+` using
 the functions in `+thinkplot+`:
 
+[source]
 ....
     thinkplot.Plot(sf)
 ....
@@ -7854,6 +7993,7 @@
 `+SurvivalFunction+` provides `+MakeHazard+`, which calculates the
 hazard function:
 
+[source]
 ....
 # class SurvivalFunction
 
@@ -7869,6 +8009,7 @@
 
 The `+HazardFunction+` object is a wrapper for a pandas Series:
 
+[source]
 ....
 class HazardFunction(object):
 
@@ -7955,6 +8096,7 @@
 
 Here’s the code:
 
+[source]
 ....
 def EstimateHazardFunction(complete, ongoing, label=''):
 
@@ -8033,6 +8175,7 @@
 First, we read the respondent file and replace invalid values of
 `+cmmarrhx+`:
 
+[source]
 ....
     resp = chap01soln.ReadFemResp()
     resp.cmmarrhx.replace([9997, 9998, 9999], np.nan, inplace=True)
@@ -8041,6 +8184,7 @@
 Then we compute each respondent’s age when married and age when
 interviewed:
 
+[source]
 ....
     resp['agemarry'] = (resp.cmmarrhx - resp.cmbirth) / 12.0
     resp['age'] = (resp.cmintvw - resp.cmbirth) / 12.0
@@ -8050,6 +8194,7 @@
 have been married, and `+ongoing+`, which is the age at interview for
 women who have not:
 
+[source]
 ....
     complete = resp[resp.evrmarry==1].agemarry
     ongoing = resp[resp.evrmarry==0].age
@@ -8057,6 +8202,7 @@
 
 Finally we compute the hazard function.
 
+[source]
 ....
     hf = EstimateHazardFunction(complete, ongoing)
 ....
@@ -8082,6 +8228,7 @@
 The `+HazardFunction+` class provides `+MakeSurvival+`, which computes
 this product:
 
+[source]
 ....
 # class HazardFunction:
 
@@ -8101,8 +8248,8 @@
 compute the complement of `+ss+`, make a Cdf, and then instantiate a
 SurvivalFunction object.
 
-image::figs/survival2.png[Hazard function for age at first marriage (top)
-and survival curve (bottom).,height=240]
+.Hazard function for age at first marriage (top) and survival curve (bottom).
+image::figs/survival2.png[height=240]
 
 <<survival2>> (bottom) shows the result. The survival
 curve is steepest between 25 and 35, when most women get married.
@@ -8137,6 +8284,7 @@
 
 We can quantify sampling error by resampling. Here’s the code:
 
+[source]
 ....
 def ResampleSurvival(resp, iters=101):
     low, high = resp.agemarry.min(), resp.agemarry.max()
@@ -8168,9 +8316,8 @@
 `+PercentileRows+` takes this sequence and computes the 5th and 95th
 percentiles, returning a 90% confidence interval for the survival curve.
 
-image::figs/survival3.png[Survival curve for age at first marriage (dark
-line) and a 90% confidence interval based on weighted resampling (gray
-line).,height=240]
+.Survival curve for age at first marriage (dark line) and a 90% confidence interval based on weighted resampling (gray line).
+image::figs/survival3.png[height=240]
 
 <<survival3>> shows the result along with the survival
 curve we estimated in the previous section. The confidence interval
@@ -8200,6 +8347,7 @@
 2006–2010 used in <<replication>>; and the Cycle 5 data
 from 1995. In total these datasets include 30,769 respondents.
 
+[source]
 ....
     resp5 = ReadFemResp1995()
     resp6 = ReadFemResp2002()
@@ -8210,6 +8358,7 @@
 For each DataFrame, `+resp+`, I use `+cmbirth+` to compute the decade of
 birth for each respondent:
 
+[source]
 ....
     month0 = pandas.to_datetime('1899-12-15')
     dates = [month0 + pandas.DateOffset(months=cm) 
@@ -8228,6 +8377,7 @@
 due to sampling error, I resample the data, group respondents by decade,
 and plot survival curves:
 
+[source]
 ....
     for i in range(iters):
         samples = [thinkstats2.ResampleRowsWeighted(resp) 
@@ -8246,6 +8396,7 @@
 
 `+EstimateSurvivalByDecade+` plots survival curves for each cohort:
 
+[source]
 ....
 def EstimateSurvivalByDecade(resp):
     for name, group in groups:
@@ -8253,8 +8404,8 @@
         thinkplot.Plot(sf)
 ....
 
-image::figs/survival4.png[Survival curves for respondents born during
-different decades.,height=240]
+.Survival curves for respondents born during different decades.
+image::figs/survival4.png[height=240]
 
 <<survival4>> shows the results. Several patterns are
 visible:
@@ -8292,6 +8443,7 @@
 cohort. HazardFunction provides a method, `+Extend+`, that copies the
 tail from another longer HazardFunction:
 
+[source]
 ....
 # class HazardFunction
 
@@ -8310,6 +8462,7 @@
 Now we can extend the HazardFunction for each cohort, using values from
 the predecessor:
 
+[source]
 ....
 def PlotPredictionsByDecade(groups):
     hfs = []
@@ -8333,8 +8486,8 @@
 on. Then it converts each HazardFunction to a SurvivalFunction and plots
 it.
 
-image::figs/survival5.png[Survival curves for respondents born during
-different decades, with predictions for the later cohorts.,height=240]
+.Survival curves for respondents born during different decades, with predictions for the later cohorts.
+image::figs/survival5.png[height=240]
 
 <<survival5>> shows the results; I’ve removed the 50s
 cohort to make the predictions more visible. These results suggest that
@@ -8351,6 +8504,7 @@
 The first step is to extract the PMF of lifetimes. `+SurvivalFunction+`
 provides a method that does that:
 
+[source]
 ....
 # class SurvivalFunction
 
@@ -8380,6 +8534,7 @@
 "`expected`" means average. `+SurvivalFunction+` provides a method that
 does that, too:
 
+[source]
 ....
 # class SurvivalFunction
 
@@ -8412,8 +8567,8 @@
 exceeds `+t+`. By subtracting `+t+` we get the mean remaining pregnancy
 length.
 
-image::figs/survival6.png[Expected remaining pregnancy length (left) and
-years until first marriage (right).,height=240]
+.Expected remaining pregnancy length (left) and years until first marriage (right).
+image::figs/survival6.png[height=240]
 
 <<survival6>> (left) shows the expected remaining
 pregnancy length as a function of the current duration. For example,
@@ -8462,6 +8617,7 @@
 
 Here’s the code that computes and plots these functions:
 
+[source]
 ....
     rem_life1 = sf1.RemainingLifetime()
     thinkplot.Plot(rem_life1)
@@ -8581,20 +8737,20 @@
 distribution has this property; if latexmath:[X \sim \mathcal{N}~(\mu,
 \sigma^2)],
 
+[[eq:1,Equation 1]]
 [latexmath]
 ++++
-\label{eq:1}
-    X' \sim \mathcal{N}~(a \mu + b, a^{2} \sigma^2) \tag{1}
+    X' \sim \mathcal{N}~(a \mu + b, a^{2} \sigma^2) \tag*{1}
 ++++
 Normal distributions are also closed under addition. If
 latexmath:[Z = X + Y] and
 latexmath:[X \sim \mathcal{N}~(\mu_{X}, \sigma_{X}^{2})] and
 latexmath:[Y \sim \mathcal{N}~(\mu_{Y}, \sigma_{Y}^{2})] then
 
+[[eq:2,Equation 2]]
 [latexmath]
 ++++
-\label{eq:2}
-    Z \sim \mathcal{N}~(\mu_X + \mu_Y, \sigma_X^2 + \sigma_Y^2) \tag{2}
+    Z \sim \mathcal{N}~(\mu_X + \mu_Y, \sigma_X^2 + \sigma_Y^2) \tag*{2}
 ++++
 In the special case latexmath:[Z = X + X], we have
 
@@ -8605,10 +8761,10 @@
 and in general if we draw latexmath:[n] values of latexmath:[X] and add
 them up, we have
 
+[[eq:3,Equation 3]]
 [latexmath]
 ++++
-\label{eq:3}
-    Z \sim \mathcal{N}~(n \mu_X, n \sigma_X^2) \tag{3}
+    Z \sim \mathcal{N}~(n \mu_X, n \sigma_X^2) \tag*{3}
 ++++
 
 [[sampling-distributions]]
@@ -8633,14 +8789,14 @@
 ++++
 Y \sim \mathcal{N}~(n \mu, n \sigma^2)
 ++++
-using Equation #eq:3[[eq:3]]. And if we divide by latexmath:[n], the
+using <<eq:3>>. And if we divide by latexmath:[n], the
 sample mean, latexmath:[Z], is distributed
 
 [latexmath]
 ++++
 Z \sim \mathcal{N}~(\mu, \sigma^2/n)
 ++++
-using Equation #eq:1[[eq:1]] with latexmath:[a = 1/n].
+using <<eq:1>> with latexmath:[a = 1/n].
 
 The distribution of latexmath:[Z] is the sampling distribution of
 latexmath:[\bar{x}]. The mean of latexmath:[Z] is latexmath:[\mu], which
@@ -8666,6 +8822,7 @@
 provides a wrapper function that makes the SciPy function a little
 easier to use:
 
+[source]
 ....
 def EvalNormalCdfInverse(p, mu=0, sigma=1):
     return scipy.stats.norm.ppf(p, loc=mu, scale=sigma)
@@ -8695,6 +8852,7 @@
 `+Normal+` that represents a normal distribution and encodes the
 equations in the previous sections. Here’s what it looks like:
 
+[source]
 ....
 class Normal(object):
 
@@ -8716,17 +8874,18 @@
 ....
 
 `+Normal+` provides `+Sum+`, which takes a sample size, `+n+`, and
-returns the distribution of the sum of `+n+` values, using Equation
-#eq:3[[eq:3]]:
+returns the distribution of the sum of `+n+` values, using 
+<<eq:3>>:
 
 ....
     def Sum(self, n):
         return Normal(n * self.mu, n * self.sigma2)
 ....
 
-Normal also knows how to multiply and divide using Equation
-#eq:1[[eq:1]]:
+Normal also knows how to multiply and divide using 
+<<eq:1>>:
 
+[source]
 ....
     def __mul__(self, factor):
         return Normal(factor * self.mu, factor**2 * self.sigma2)
@@ -8797,6 +8956,7 @@
 To see how the Central Limit Theorem works, and when it doesn’t, let’s
 try some experiments. First, we’ll try an exponential distribution:
 
+[source]
 ....
 def MakeExpoSamples(beta=2.0, iters=1000):
     samples = []
@@ -8824,6 +8984,7 @@
 The return value is a list of `+(n, sample)+` pairs. For each pair, we
 make a normal probability plot:
 
+[source]
 ....
 def NormalPlotSamples(samples, plot=1, ylabel=''):
     for n, sample in samples:
@@ -8837,8 +8998,8 @@
 `+NormalPlotSamples+` takes the list of pairs from `+MakeExpoSamples+`
 and generates a row of normal probability plots.
 
-image::figs/normal1.png[Distributions of sums of exponential values (top
-row) and lognormal values (bottom row).,height=336]
+.Distributions of sums of exponential values (top row) and lognormal values (bottom row).
+image::figs/normal1.png[height=336]
 
 <<normal1>> (top row) shows the results. With `+n=1+`,
 the distribution of the sum is still exponential, so the normal
@@ -8852,8 +9013,8 @@
 longer to converge. With `+n=10+` the normal probability plot is nowhere
 near straight, but with `+n=100+` it is approximately normal.
 
-image::figs/normal2.png[Distributions of sums of Pareto values (top row)
-and correlated exponential values (bottom row).,height=336]
+.Distributions of sums of Pareto values (top row) and correlated exponential values (bottom row).
+image::figs/normal2.png[height=336]
 
 Pareto distributions are even more skewed than lognormal. Depending on
 the parameters, many Pareto distributions do not have finite mean and
@@ -8872,6 +9033,7 @@
 `+GenerateCorrelated+` returns an iterator of `+n+` normal values with
 serial correlation `+rho+`:
 
+[source]
 ....
 def GenerateCorrelated(rho, n):
     x = random.gauss(0, 1)
@@ -8892,6 +9054,7 @@
 `+GenerateExpoCorrelated+` takes the resulting sequence and transforms
 it to exponential:
 
+[source]
 ....
 def GenerateExpoCorrelated(rho, n):
     normal = list(GenerateCorrelated(rho, n))
@@ -8938,6 +9101,7 @@
 pregnancy lengths is the same for first babies and others. So we can
 compute the sampling distribution of the mean like this:
 
+[source]
 ....
     dist1 = SamplingDistMean(live.prglngth, len(firsts))
     dist2 = SamplingDistMean(live.prglngth, len(others))
@@ -8948,6 +9112,7 @@
 values and the sample size, and returns a Normal object representing the
 sampling distribution:
 
+[source]
 ....
 def SamplingDistMean(data, n):
     mean, var = data.mean(), data.var()
@@ -8967,8 +9132,9 @@
 
 Next, we compute the sampling distribution of the difference in the
 means. The `+Normal+` class knows how to perform subtraction using
-Equation #eq:2[[eq:2]]:
+<<eq:2>>:
 
+[source]
 ....
     def __sub__(self, other):
         return Normal(self.mu - other.mu,
@@ -9033,6 +9199,7 @@
 their correlation? `+StudentCdf+` takes the sample size, `+n+`, and
 returns the sampling distribution of correlation:
 
+[source]
 ....
 def StudentCdf(n):
     ts = np.linspace(-3, 3, 101)
@@ -9048,8 +9215,8 @@
 freedom.`" I won’t explain that term, but you can read about it at
 http://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics).
 
-image::figs/normal4.png[Sampling distribution of correlations for
-uncorrelated normal variables.,height=240]
+.Sampling distribution of correlations for uncorrelated normal variables.
+image::figs/normal4.png[height=240]
 
 To get from `+ts+` to the correlation coefficients, `+rs+`, we apply the
 inverse transform,
@@ -9071,6 +9238,7 @@
 uncorrelated. Using the analytic distribution, we can compute just how
 unlikely:
 
+[source]
 ....
     t = r * math.sqrt((n-2) / (1-r**2))
     p_value = 1 - scipy.stats.t.cdf(t, df=n-2)
@@ -9093,17 +9261,18 @@
 ++++
 One reason the chi-squared statistic is widely used is that its sampling
 distribution under the null hypothesis is analytic; by a remarkable
-coincidencefootnote:[Not really.], it is called the chi-squared
+coincidence{}footnote:[Not really.], it is called the chi-squared
 distribution. Like the t-distribution, the chi-squared CDF can be
 computed efficiently using gamma functions.
 
-image::figs/normal5.png[Sampling distribution of chi-squared statistics
-for a fair six-sided die.,height=240]
+.Sampling distribution of chi-squared statistics for a fair six-sided die.
+image::figs/normal5.png[height=240]
 
 SciPy provides an implementation of the chi-squared distribution, which
 we use to compute the sampling distribution of the chi-squared
 statistic:
 
+[source]
 ....
 def ChiSquaredCdf(n):
     xs = np.linspace(0, 25, 101)
@@ -9118,6 +9287,7 @@
 We can use this distribution to compute the p-value of the observed test
 statistic, `+chi2+`:
 
+[source]
 ....
     p_value = 1 - scipy.stats.chi2.cdf(chi2, df=n-1)
 ....
@@ -9216,7 +9386,7 @@
 Compute this distribution and use it to calculate the standard error and
 a 90% confidence interval for the difference in means.
 
-In a recent paperfootnote:["`Evidence for the persistent effects of an
+In a recent paper{}footnote:["`Evidence for the persistent effects of an
 intervention to mitigate gender-sterotypical task allocation within
 student engineering teams,`" Proceedings of the IEEE Frontiers in
 Education Conference, 2014.], Stein et al. investigate the effects of an
@@ -9244,3 +9414,17 @@
 
 Finally, estimate the change in gender gap; what is the sampling
 distribution of this change, and is it statistically significant?
+
+ifdef::backend-html5[]
+
+:sectnums!:
+
+[role=advert-bar]
+include::advert.adoc[]
+
+endif::[]
+
+ifndef::backend-html5[]
+[index]
+== Index
+endif::[]
