--- book.adoc.orig	2019-09-05 12:15:06.720874705 +1000
+++ new-book.adoc	2019-09-05 12:08:17.180106193 +1000
@@ -1,19 +1,24 @@
-= Think Stats
+= Think Stats: Exploratory Data Analysis in Python
 Allen B. Downey
+v2.0.38
+:doctype: book
 :stem: latexmath
+:toc: left
+:sectnums:
+:sectlinks:
+:sectanchors:
+:xrefstyle: short
+:source-highlighter: highlightjs
+:source-language: python
+// Define an "empty" attribute.  You can use this to provide visual separation between
+// words and adjacent inline macros, e.g., `something{_}footnote:[...]`.
+:_:
 
-3 Think Stats +
-Exploratory Data Analysis in Python
-
-Version 2.0.38
-
-3 Think Stats +
-Exploratory Data Analysis in Python
-
-Version 2.0.38
-
-Allen B. Downey +
+// Turn off section numbering for frontmatter
+:sectnums!:
 
+[colophon]
+= Colphon
 Green Tea Press
 
 Needham, Massachusetts
@@ -35,6 +40,7 @@
 
 The LaTeX source for this book is available from http://thinkstats2.com.
 
+[preface]
 == Preface
 
 This book is an introduction to the practical tools of exploratory data
@@ -274,6 +280,9 @@
 Jeff Pickhardt, Rohit Deshpande, Joanne Pratt, Lucian Ursu, Paul Glezen,
 Ting-kuang Lin, Scott Miller, Luigi Patruno.
 
+// Turn on section numbering for mainmatter
+:sectnums:
+
 [[intro]]
 == Exploratory data analysis
 
@@ -292,15 +301,15 @@
 claims. I found many examples like these:
 
 ____
-"`My two friends that have given birth recently to their first babies,
+My two friends that have given birth recently to their first babies,
 BOTH went almost 2 weeks overdue before going into labour or being
-induced.`"
+induced.
 
-"`My first one came 2 weeks late and now I think the second one is going
-to come out two weeks early!!`"
+My first one came 2 weeks late and now I think the second one is going
+to come out two weeks early!!
 
-"`I don’t think that can be true because my sister was my mother’s first
-and she was early, as with many of my cousins.`"
+I don’t think that can be true because my sister was my mother’s first
+and she was early, as with many of my cousins.
 ____
 
 Reports like these are called *anecdotal evidence* because they are
@@ -441,6 +450,7 @@
 including functions that read the Stata dictionary and the NSFG data
 file. Here’s how they are used in `+nsfg.py+`:
 
+[source]
 ....
 def ReadFemPreg(dct_file='2002FemPreg.dct',
                 dat_file='2002FemPreg.dat.gz'):
@@ -605,6 +615,7 @@
 `+nsfg.py+` includes `+CleanFemPreg+`, a function that cleans the
 variables I am planning to use.
 
+[source]
 ....
 def CleanFemPreg(df):
     df.agepreg /= 100.0
@@ -657,6 +668,7 @@
 One important note: when you add a new column to a DataFrame, you must
 use dictionary syntax, like this
 
+[source]
 ....
     # CORRECT
     df['totalwgt_lb'] = df.birthwgt_lb + df.birthwgt_oz / 16.0 
@@ -664,6 +676,7 @@
 
 Not dot notation, like this:
 
+[source]
 ....
     # WRONG!
     df.totalwgt_lb = df.birthwgt_lb + df.birthwgt_oz / 16.0 
@@ -757,6 +770,7 @@
 
 To deal with this error, I added a line to `+CleanFemPreg+`:
 
+[source]
 ....
 df.loc[df.birthwgt_lb > 20, 'birthwgt_lb'] = np.nan
 ....
@@ -781,6 +795,7 @@
 do some processing to collect the pregnancy data for each respondent.
 Here’s a function that does that:
 
+[source]
 ....
 def MakePregMap(df):
     d = defaultdict(list)
@@ -833,8 +848,14 @@
 health. At the same time, we have an obligation to consider the people
 represented by the data, and to afford them respect and gratitude.
 
+
 === Exercises
 
+:!exercise:
+{counter2:exercise-chapter}
+
+*Exercise {exercise-chapter}.{counter:exercise}*
+
 In the repository you downloaded, you should find a file named
 `+chap01ex.ipynb+`, which is an IPython notebook. You can launch IPython
 notebook from the command line like this:
@@ -864,6 +885,8 @@
 
 A solution to this exercise is in `+chap01soln.ipynb+`
 
+*Exercise {exercise-chapter}.{counter:exercise}*
+
 In the repository you downloaded, you should find a file named
 `+chap01ex.py+`; using this file as a starting place, write a function
 that reads the respondent file, `+2002FemResp.dat.gz+`.
@@ -881,6 +904,8 @@
 
 A solution to this exercise is in `+chap01soln.py+`
 
+*Exercise {exercise-chapter}.{counter:exercise}*
+
 The best way to learn about statistics is to work on a project you are
 interested in. Is there a question like, "`Do first babies arrive
 late,`" that you want to investigate?
@@ -955,6 +980,7 @@
 In Python, an efficient way to compute frequencies is with a dictionary.
 Given a sequence of values, `+t+`:
 
+[source]
 ....
 hist = {}
 for x in t:
@@ -965,6 +991,7 @@
 Alternatively, you could use the `+Counter+` class defined in the
 `+collections+` module:
 
+[source]
 ....
 from collections import Counter
 counter = Counter(t)
@@ -1021,6 +1048,7 @@
 To loop through the values in order, you can use the built-in function
 `+sorted+`:
 
+[source]
 ....
 for val in sorted(hist.Values()):
     print(val, hist.Freq(val))
@@ -1028,6 +1056,7 @@
 
 Or you can use `+Items+` to iterate through value-frequency pairs:
 
+[source]
 ....
 for val, freq in hist.Items():
      print(val, freq)
@@ -1035,10 +1064,9 @@
 
 === Plotting histograms
 
-image::figs/first_wgt_lb_hist.png[Histogram of the pound part of birth
-weight.,height=240]
-
-[#first_wgt_lb_hist]#[first_wgt_lb_hist]#
+[[first_wgt_lb_hist]]
+.Histogram of the pound part of birth weight.
+image::figs/first_wgt_lb_hist.png[height=240]
 
 For this book I wrote a module called `+thinkplot.py+` that provides
 functions for plotting Hists and other objects defined in
@@ -1057,10 +1085,9 @@
 You can read the documentation for `+thinkplot+` at
 http://greenteapress.com/thinkstats2/thinkplot.html.
 
-image::figs/first_wgt_oz_hist.png[Histogram of the ounce part of birth
-weight.,height=240]
-
-[#first_wgt_oz_hist]#[first_wgt_oz_hist]#
+[[first_wgt_oz_hist]]
+.Histogram of the ounce part of birth weight.
+image::figs/first_wgt_oz_hist.png[height=240]
 
 === NSFG variables
 
@@ -1077,13 +1104,13 @@
 into a single quantity, `+totalwgt_lb+`. In this section I use these
 variables to demonstrate some features of histograms.
 
-image::figs/first_agepreg_hist.png[Histogram of mother’s age at end of
-pregnancy.,height=240]
-
-[#first_agepreg_hist]#[first_agepreg_hist]#
+[[first_agepreg_hist]]
+.Histogram of mother’s age at end of pregnancy.
+image::figs/first_agepreg_hist.png[height=240]
 
 I’ll start by reading the data and selecting records for live births:
 
+[source]
 ....
     preg = nsfg.ReadFemPreg()
     live = preg[preg.outcome == 1]
@@ -1093,6 +1120,7 @@
 the DataFrame and returns a new DataFrame. Next I generate and plot the
 histogram of `+birthwgt_lb+` for live births.
 
+[source]
 ....
     hist = thinkstats2.Hist(live.birthwgt_lb, label='birthwgt_lb')
     thinkplot.Hist(hist)
@@ -1103,10 +1131,9 @@
 are dropped. `+label+` is a string that appears in the legend when the
 Hist is plotted.
 
-image::figs/first_prglngth_hist.png[Histogram of pregnancy length in
-weeks.,height=240]
-
-[#first_prglngth_hist]#[first_prglngth_hist]#
+[[first_prglngth_hist]]
+.Histogram of pregnancy length in weeks.
+image::figs/first_prglngth_hist.png[height=240]
 
 <<first_wgt_lb_hist>> shows the result.
 The most common value, called the *mode*, is 7 pounds. The distribution
@@ -1147,6 +1174,7 @@
 integer `+n+` and return the `+n+` largest or smallest values from the
 histogram:
 
+[source]
 ....
     for weeks, freq in hist.Smallest(10):
         print(weeks, freq)
@@ -1191,6 +1219,7 @@
 babies and others. I divided the DataFrame of live births using
 `+birthord+`, and computed their histograms:
 
+[source]
 ....
     firsts = live[live.birthord == 1]
     others = live[live.birthord != 1]
@@ -1201,6 +1230,7 @@
 
 Then I plotted their histograms on the same axis:
 
+[source]
 ....
     width = 0.45
     thinkplot.PrePlot(2)
@@ -1214,10 +1244,9 @@
 plot; it uses this information to choose an appropriate collection of
 colors.
 
-image::figs/first_nsfg_hist.png[Histogram of pregnancy
-lengths.,height=240]
-
-[#first_nsfg_hist]#[first_nsfg_hist]#
+[[first_nsfg_hist]]
+.Histogram of pregnancy lengths.
+image::figs/first_nsfg_hist.png[height=240]
 
 `+thinkplot.Hist+` normally uses `+align='center'+` so that each bar is
 centered over its value. For this figure, I use `+align='right'+` and
@@ -1316,6 +1345,7 @@
 Pandas data structures provides methods to compute mean, variance and
 standard deviation:
 
+[source]
 ....
     mean = live.prglngth.mean()
     var = live.prglngth.var()
@@ -1327,7 +1357,7 @@
 of 2-3 weeks to be common.
 
 Variance of pregnancy length is 7.3, which is hard to interpret,
-especially since the units are weekslatexmath:[^2], or "`square weeks.`"
+especially since the units are weeks{_}latexmath:[^2], or "`square weeks.`"
 Variance is useful in some calculations, but it is not a good summary
 statistic.
 
@@ -1358,6 +1388,7 @@
 the groups and latexmath:[s] is the "`pooled standard deviation`".
 Here’s the Python code that computes Cohen’s latexmath:[d]:
 
+[source]
 ....
 def CohenEffectSize(group1, group2):
     diff = group1.mean() - group2.mean()
@@ -1403,6 +1434,11 @@
 
 === Exercises
 
+:exercise!:
+{counter2:exercise-chapter}
+
+*Exercise {exercise-chapter}.{counter:exercise}*
+
 Based on the results in this chapter, suppose you were asked to
 summarize what you learned about whether first babies arrive late.
 
@@ -1415,6 +1451,8 @@
 first babies arrive late?`" Write a paragraph that uses the results in
 this chapter to answer the question clearly, precisely, and honestly.
 
+*Exercise {exercise-chapter}.{counter:exercise}*
+
 In the repository you downloaded, you should find a file named
 `+chap02ex.ipynb+`; open it. Some cells are already filled in, and you
 should execute them. Other cells give you instructions for exercises.
@@ -1422,10 +1460,13 @@
 
 A solution to this exercise is in `+chap02soln.ipynb+`
 
+
 In the repository you downloaded, you should find a file named
 `+chap02ex.py+`; you can use this file as a starting place for the
 following exercises. My solution is in `+chap02soln.py+`.
 
+*Exercise {exercise-chapter}.{counter:exercise}*
+
 The mode of a distribution is the most frequent value; see
 http://wikipedia.org/wiki/Mode_(statistics). Write a function called
 `+Mode+` that takes a Hist and returns the most frequent value.
@@ -1434,6 +1475,8 @@
 that returns a list of value-frequency pairs in descending order of
 frequency.
 
+*Exercise {exercise-chapter}.{counter:exercise}*
+
 Using the variable `+totalwgt_lb+`, investigate whether first babies are
 lighter or heavier than others. Compute Cohen’s latexmath:[d] to
 quantify the difference between the groups. How does it compare to the
@@ -1485,6 +1528,7 @@
 Given a Hist, we can make a dictionary that maps from each value to its
 probability:
 
+[source]
 ....
 n = hist.Total()
 d = {}
@@ -1581,10 +1625,9 @@
 sequence of values, computes a histogram, and plots it. Since I use Hist
 objects, I usually don’t use `+pyplot.hist+`.
 
-image::figs/probability_nsfg_pmf.png[PMF of pregnancy lengths for first
-babies and others, using bar graphs and step functions.,height=288]
-
-[#probability_nsfg_pmf]#[probability_nsfg_pmf]#
+[[probability_nsfg_pmf]]
+.PMF of pregnancy lengths for first babies and others, using bar graphs and step functions.
+image::figs/probability_nsfg_pmf.png[height=288]
 
 <<probability_nsfg_pmf>> shows PMFs of
 pregnancy length for first babies and others using bar graphs (left) and
@@ -1598,6 +1641,7 @@
 Here’s the code that generates
 <<probability_nsfg_pmf>>:
 
+[source]
 ....
     thinkplot.PrePlot(2, cols=2)
     thinkplot.Hist(first_pmf, align='right', width=width)
@@ -1636,6 +1680,7 @@
 the mode. So it makes sense to zoom in on that part of the graph, and to
 transform the data to emphasize differences:
 
+[source]
 ....
     weeks = range(35, 46)
     diffs = []
@@ -1655,10 +1700,9 @@
 babies are less likely to be born in week 39, and somewhat more likely
 to be born in weeks 41 and 42.
 
-image::figs/probability_nsfg_diffs.png[Difference, in percentage points,
-by week.,height=240]
-
-[#probability_nsfg_diffs]#[probability_nsfg_diffs]#
+[[probability_nsfg_diffs]]
+.Difference, in percentage points, by week.
+image::figs/probability_nsfg_diffs.png[height=240]
 
 For now we should hold this conclusion only tentatively. We used the
 same dataset to identify an apparent difference and then chose a
@@ -1703,6 +1747,7 @@
 PMF, compute the mean, and report that the average class size is 23.7.
 Here’s the code:
 
+[source]
 ....
     d = { 7: 8, 12: 8, 17: 14, 22: 4, 
           27: 6, 32: 12, 37: 8, 42: 3, 47: 2 }
@@ -1719,6 +1764,7 @@
 probability associated with each class size is "`biased`" by the number
 of students in the class.
 
+[source]
 ....
 def BiasPmf(pmf, label):
     new_pmf = pmf.Copy(label=label)
@@ -1736,6 +1782,7 @@
 
 Now we can plot the actual and observed distributions:
 
+[source]
 ....
     biased_pmf = BiasPmf(pmf, label='observed')
     thinkplot.PrePlot(2)
@@ -1743,10 +1790,9 @@
     thinkplot.Show(xlabel='class size', ylabel='PMF')
 ....
 
-image::figs/class_size1.png[Distribution of class sizes, actual and as
-observed by students.,height=288]
-
-[#class_size1]#[class_size1]#
+[[class_size1]]
+.Distribution of class sizes, actual and as observed by students.
+image::figs/class_size1.png[height=288]
 
 <<class_size1>> shows the result. In the biased
 distribution there are fewer small classes and more large ones. The mean
@@ -1762,6 +1808,7 @@
 use it to estimate the actual distribution. Here’s the function that
 unbiases a Pmf:
 
+[source]
 ....
 def UnbiasPmf(pmf, label):
     new_pmf = pmf.Copy(label=label)
@@ -1894,6 +1941,11 @@
 
 === Exercises
 
+:exercise!:
+{counter2:exercise-chapter}
+
+*Exercise {exercise-chapter}.{counter:exercise}*
+
 Solutions to these exercises are in `+chap03soln.ipynb+` and
 `+chap03soln.py+`
 
@@ -1912,6 +1964,8 @@
 Plot the actual and biased distributions, and compute their means. As a
 starting place, you can use `+chap03ex.ipynb+`.
 
+*Exercise {exercise-chapter}.{counter:exercise}*
+
 In <<mean>> we computed the mean of a sample by adding up
 the elements and dividing by n. If you are given a PMF, you can still
 compute the mean, but the process is slightly different:
@@ -1932,6 +1986,8 @@
 they are consistent with the methods `+Mean+` and `+Var+` provided by
 Pmf.
 
+*Exercise {exercise-chapter}.{counter:exercise}*
+
 I started with the question, "`Are first babies more likely to be
 late?`" To address it, I computed the difference in means between groups
 of babies, but I ignored the possibility that there might be a
@@ -1943,7 +1999,9 @@
 
 Hint: use `+nsfg.MakePregMap+`.
 
-[#relay]#[relay]#
+*Exercise {exercise-chapter}.{counter:exercise}*
+
+[[relay]]
 
 In most foot races, everyone starts at the same time. If you are a fast
 runner, you usually pass a lot of people at the beginning of the race,
@@ -2014,10 +2072,9 @@
 at birth in pounds. <<nsfg_birthwgt_pmf>>
 shows the PMF of these values for first babies and others.
 
-image::figs/nsfg_birthwgt_pmf.png[PMF of birth weights. This figure shows
-a limitation of PMFs: they are hard to compare visually.,height=240]
-
-[#nsfg_birthwgt_pmf]#[nsfg_birthwgt_pmf]#
+[[nsfg_birthwgt_pmf]]
+.PMF of birth weights. This figure shows a limitation of PMFs: they are hard to compare visually.
+image::figs/nsfg_birthwgt_pmf.png[height=240]
 
 Overall, these distributions resemble the bell shape of a normal
 distribution, with many values near the mean and a few values much
@@ -2070,6 +2127,7 @@
 you want to find the corresponding value, one option is to sort the
 values and search for the one you want:
 
+[source]
 ....
 def Percentile(scores, percentile_rank):
     scores.sort()
@@ -2086,6 +2144,7 @@
 approach is to use the percentile rank to compute the index of the
 corresponding percentile:
 
+[source]
 ....
 def Percentile2(scores, percentile_rank):
     scores.sort()
@@ -2114,6 +2173,7 @@
 Here’s what that looks like as a function that takes a sequence,
 `+sample+`, and a value, `+x+`:
 
+[source]
 ....
 def EvalCdf(sample, x):
     count = 0.0
@@ -2166,9 +2226,9 @@
 value in the sample, latexmath:[\mathrm{CDF}(x)] is 0. If latexmath:[x]
 is greater than the largest value, latexmath:[\mathrm{CDF}(x)] is 1.
 
-image::figs/cumulative_example_cdf.png[Example of a CDF.,height=240]
-
-[#example_cdf]#[example_cdf]#
+[[example_cdf]]
+.Example of a CDF.
+image::figs/cumulative_example_cdf.png[height=240]
 
 <<example_cdf>> is a graphical representation of
 this CDF. The CDF of a sample is a step function.
@@ -2184,15 +2244,15 @@
 * `+Value(p)+`: Given a probability `+p+`, computes the corresponding
 value, `+x+`; that is, the *inverse CDF* of `+p+`.
 
-image::figs/cumulative_prglngth_cdf.png[CDF of pregnancy
-length.,height=240]
-
-[#cumulative_prglngth_cdf]#[cumulative_prglngth_cdf]#
+[[cumulative_prglngth_cdf]]
+.CDF of pregnancy length.
+image::figs/cumulative_prglngth_cdf.png[height=240]
 
 The Cdf constructor can take as an argument a list of values, a pandas
 Series, a Hist, Pmf, or another Cdf. The following code makes a Cdf for
 the distribution of pregnancy lengths in the NSFG:
 
+[source]
 ....
     live, firsts, others = first.MakeFrames()
     cdf = thinkstats2.Cdf(live.prglngth, label='prglngth')
@@ -2201,6 +2261,7 @@
 `+thinkplot+` provides a function named `+Cdf+` that plots Cdfs as
 lines:
 
+[source]
 ....
     thinkplot.Cdf(cdf)
     thinkplot.Show(xlabel='weeks', ylabel='CDF')
@@ -2225,6 +2286,7 @@
 here is the code that plots the CDF of birth weight for first babies and
 others.
 
+[source]
 ....
     first_cdf = thinkstats2.Cdf(firsts.totalwgt_lb, label='first')
     other_cdf = thinkstats2.Cdf(others.totalwgt_lb, label='other')
@@ -2234,10 +2296,9 @@
     thinkplot.Show(xlabel='weight (pounds)', ylabel='CDF')
 ....
 
-image::figs/cumulative_birthwgt_cdf.png[CDF of birth weights for first
-babies and others.,height=240]
-
-[#cumulative_birthwgt_cdf]#[cumulative_birthwgt_cdf]#
+[[cumulative_birthwgt_cdf]]
+.CDF of birth weights for first babies and others.
+image::figs/cumulative_birthwgt_cdf.png[height=240]
 
 <<cumulative_birthwgt_cdf>> shows
 the result. Compared to
@@ -2287,6 +2348,7 @@
 
 Here’s how we can compute it. First, we make the Cdf of birth weights:
 
+[source]
 ....
     weights = live.totalwgt_lb
     cdf = thinkstats2.Cdf(weights, label='totalwgt_lb')
@@ -2295,6 +2357,7 @@
 Then we generate a sample and compute the percentile rank of each value
 in the sample.
 
+[source]
 ....
     sample = np.random.choice(weights, 100, replace=True)
     ranks = [cdf.PercentileRank(x) for x in sample]
@@ -2306,16 +2369,16 @@
 
 Finally we make and plot the Cdf of the percentile ranks.
 
+[source]
 ....
     rank_cdf = thinkstats2.Cdf(ranks)
     thinkplot.Cdf(rank_cdf)
     thinkplot.Show(xlabel='percentile rank', ylabel='CDF')
 ....
 
-image::figs/cumulative_random.png[CDF of percentile ranks for a random
-sample of birth weights.,height=240]
-
-[#cumulative_random]#[cumulative_random]#
+[[cumulative_random]]
+.CDF of percentile ranks for a random sample of birth weights.
+image::figs/cumulative_random.png[height=240]
 
 <<cumulative_random>> shows the result.
 The CDF is approximately a straight line, which means that the
@@ -2337,6 +2400,7 @@
 
 Cdf provides an implementation of this algorithm, called `+Random+`:
 
+[source]
 ....
 # class Cdf:
     def Random(self):
@@ -2360,6 +2424,7 @@
 More generally, given position and field size, we can compute percentile
 rank:
 
+[source]
 ....
 def PositionToPercentile(position, field_size):
     beat = field_size - position + 1
@@ -2378,6 +2443,7 @@
 I can answer that question by converting my percentile rank in M4049 to
 a position in M5059. Here’s the code:
 
+[source]
 ....
 def PercentileToPosition(percentile, field_size):
     beat = percentile * field_size / 100.0
@@ -2392,9 +2458,14 @@
 
 === Exercises
 
+:exercise!:
+{counter2:exercise-chapter}
+
 For the following exercises, you can start with `+chap04ex.ipynb+`. My
 solution is in `+chap04soln.ipynb+`.
 
+*Exercise {exercise-chapter}.{counter:exercise}*
+
 How much did you weigh at birth? If you don’t know, call your mother or
 someone else who knows. Using the NSFG data (all live births), compute
 the distribution of birth weights and use it to find your percentile
@@ -2403,6 +2474,8 @@
 others. If you are in the 90th percentile or higher, call your mother
 back and apologize.
 
+*Exercise {exercise-chapter}.{counter:exercise}*
+
 The numbers generated by `+random.random+` are supposed to be uniform
 between 0 and 1; that is, every value in the range should have the same
 probability.
@@ -2452,10 +2525,9 @@
 [[exponential]]
 === The exponential distribution
 
-image::figs/analytic_expo_cdf.png[CDFs of exponential distributions with
-various parameters.,height=240]
-
-[#analytic_expo_cdf]#[analytic_expo_cdf]#
+[[analytic_expo_cdf]]
+.CDFs of exponential distributions with various parameters.
+image::figs/analytic_expo_cdf.png[height=240]
 
 I’ll start with the *exponential distribution* because it is relatively
 simple. The CDF of the exponential distribution is
@@ -2482,6 +2554,7 @@
 all 44 babies was reported in the local paper; the complete dataset is
 in a file called `+babyboom.dat+`, in the `+ThinkStats2+` repository.
 
+[source]
 ....
     df = ReadBabyBoom()
     diffs = df.minutes.diff()
@@ -2495,10 +2568,9 @@
 columns `+time+`, `+sex+`, `+weight_g+`, and `+minutes+`, where
 `+minutes+` is time of birth converted to minutes since midnight.
 
-image::figs/analytic_interarrivals.png[CDF of interarrival times (left)
-and CCDF on a log-y scale (right).,height=240]
-
-[#analytic_interarrival_cdf]#[analytic_interarrival_cdf]#
+[[analytic_interarrival_cdf]]
+.CDF of interarrival times (left) and CCDF on a log-y scale (right).
+image::figs/analytic_interarrivals.png[height=240]
 
 `+diffs+` is the difference between consecutive birth times, and `+cdf+`
 is the distribution of these interarrival times.
@@ -2527,6 +2599,7 @@
 So on a log-y scale the CCDF is a straight line with slope
 latexmath:[-\lambda]. Here’s how we can generate a plot like that:
 
+[source]
 ....
     thinkplot.Cdf(cdf, complement=True)
     thinkplot.Show(xlabel='minutes',
@@ -2562,10 +2635,10 @@
 out that there is a good reason for its ubiquity, which we will get to
 in <<CLT>>.
 
-image::figs/analytic_gaussian_cdf.png[CDF of normal distributions with a
-range of parameters.,height=240]
+[[analytic_gaussian_cdf]]
+.CDF of normal distributions with a range of parameters.
+image::figs/analytic_gaussian_cdf.png[height=240]
 
-[#analytic_gaussian_cdf]#[analytic_gaussian_cdf]#
 
 The normal distribution is characterized by two parameters: the mean,
 latexmath:[\mu], and standard deviation latexmath:[\sigma]. The normal
@@ -2593,6 +2666,7 @@
 `+EvalNormalCdf+`, which takes parameters `+mu+` and `+sigma+` and
 evaluates the CDF at `+x+`:
 
+[source]
 ....
 def EvalNormalCdf(x, mu=0, sigma=1):
     return scipy.stats.norm.cdf(x, loc=mu, scale=sigma)
@@ -2609,10 +2683,9 @@
 the empirical CDF of weights for all live births and the CDF of a normal
 distribution with the same mean and variance.
 
-image::figs/analytic_birthwgt_model.png[CDF of birth weights with a
-normal model.,height=240]
-
-[#analytic_birthwgt_model]#[analytic_birthwgt_model]#
+[[analytic_birthwgt_model]]
+.CDF of birth weights with a normal model.
+image::figs/analytic_birthwgt_model.png[height=240]
 
 The normal distribution is a good model for this dataset, so if we
 summarize the distribution with the parameters latexmath:[\mu = 7.28]
@@ -2649,14 +2722,15 @@
 `+thinkstats2+` provides `+NormalProbability+`, which takes a sample and
 returns two NumPy arrays:
 
+[source]
 ....
 xs, ys = thinkstats2.NormalProbability(sample)
 ....
 
-image::figs/analytic_normal_prob_example.png[Normal probability plot for
-random samples from normal distributions.,height=240]
+[[analytic_normal_prob_example]]
+.Normal probability plot for random samples from normal distributions.
+image::figs/analytic_normal_prob_example.png[height=240]
 
-[#analytic_normal_prob_example]#[analytic_normal_prob_example]#
 
 `+ys+` contains the sorted values from `+sample+`; `+xs+` contains the
 random values from the standard normal distribution.
@@ -2672,6 +2746,7 @@
 plots a gray line that represents the model and a blue line that
 represents the data.
 
+[source]
 ....
 def MakeNormalPlot(weights):
     mean = weights.mean()
@@ -2696,10 +2771,10 @@
 the standard normal distribution and values from `+weights+`. If the
 distribution of weights is normal, the data should match the model.
 
-image::figs/analytic_birthwgt_normal.png[Normal probability plot of birth
-weights.,height=240]
+[[analytic_birthwgt_normal]]
+.Normal probability plot of birth weights.
+image::figs/analytic_birthwgt_normal.png[height=240]
 
-[#analytic_birthwgt_normal]#[analytic_birthwgt_normal]#
 
 <<analytic_birthwgt_normal>> shows
 the results for all live births, and also for full term births
@@ -2716,11 +2791,9 @@
 Whether it is good enough for practical purposes depends on the
 purposes.
 
-[[brfss]]
+[[lognormal]]
 === The lognormal distribution
 
-[#lognormal]#[lognormal]#
-
 If the logarithms of a set of values have a normal distribution, the
 values have a *lognormal distribution*. The CDF of the lognormal
 distribution is the same as the CDF of the normal distribution, with
@@ -2737,10 +2810,9 @@
 standard deviation is ugly (see
 http://wikipedia.org/wiki/Log-normal_distribution).
 
-image::figs/brfss_weight.png[CDF of adult weights on a linear scale
-(left) and log scale (right).,height=240]
-
-[#brfss_weight]#[brfss_weight]#
+[[brfss_weight]]
+.CDF of adult weights on a linear scale (left) and log scale (right).
+image::figs/brfss_weight.png[height=240]
 
 If a sample is approximately lognormal and you plot its CDF on a log-x
 scale, it will have the characteristic shape of a normal distribution.
@@ -2771,10 +2843,10 @@
 ASCII file that contains data from the BRFSS, and `+brfss.py+`, which
 reads the file and analyzes the data.
 
-image::figs/brfss_weight_normal.png[Normal probability plots for adult
-weight on a linear scale (left) and log scale (right).,height=240]
+[[brfss_weight_normal]]
+.Normal probability plots for adult weight on a linear scale (left) and log scale (right).
+image::figs/brfss_weight_normal.png[height=240]
 
-[#brfss_weight_normal]#[brfss_weight_normal]#
 
 <<brfss_weight>> (left) shows the distribution
 of adult weights on a linear scale with a normal model.
@@ -2810,10 +2882,10 @@
 shows CDFs of Pareto distributions with latexmath:[x_{m} = 0.5] and
 different values of latexmath:[\alpha].
 
-image::figs/analytic_pareto_cdf.png[CDFs of Pareto distributions with
-different parameters.,height=240]
+[[analytic_pareto_cdf]]
+.CDFs of Pareto distributions with different parameters.
+image::figs/analytic_pareto_cdf.png[height=240]
 
-[#analytic_pareto_cdf]#[analytic_pareto_cdf]#
 
 There is a simple visual test that indicates whether an empirical
 distribution fits a Pareto distribution: on a log-log scale, the CCDF
@@ -2840,10 +2912,10 @@
 U.S. Census Bureau publishes the population of every incorporated city
 and town in the United States.
 
-image::figs/populations_pareto.png[CCDFs of city and town populations, on
-a log-log scale.,height=240]
+[[populations_pareto]]
+.CCDFs of city and town populations, on a log-log scale.
+image::figs/populations_pareto.png[height=240]
 
-[#populations_pareto]#[populations_pareto]#
 
 I downloaded their data from
 http://www.census.gov/popest/data/cities/totals/2012/SUB-EST2012-3.html;
@@ -2868,11 +2940,9 @@
 The lognormal model is a better fit for the other 99%. Which model is
 appropriate depends on which part of the distribution is relevant.
 
-image::figs/populations_normal.png[CDF of city and town populations on a
-log-x scale (left), and normal probability plot of log-transformed
-populations (right).,height=240]
-
-[#populations_normal]#[populations_normal]#
+[[populations_normal]]
+.CDF of city and town populations on a log-x scale (left), and normal probability plot of log-transformed populations (right).
+image::figs/populations_normal.png[height=240]
 
 === Generating random numbers
 
@@ -2897,6 +2967,7 @@
 ++++
 So in Python we can write
 
+[source]
 ....
 def expovariate(lam):
     p = random.random()
@@ -2953,9 +3024,14 @@
 
 === Exercises
 
+:exercise!:
+{counter2:exercise-chapter}
+
 For the following exercises, you can start with `+chap05ex.ipynb+`. My
 solution is in `+chap05soln.ipynb+`.
 
+*Exercise {exercise-chapter}.{counter:exercise}*
+
 In the BRFSS (see <<lognormal>>), the
 distribution of heights is roughly normal with parameters
 latexmath:[\mu = 178] cm and latexmath:[\sigma = 7.7] cm for men, and
@@ -2965,6 +3041,8 @@
 6’1” (see http://bluemancasting.com). What percentage of the U.S. male
 population is in this range? Hint: use `+scipy.stats.norm.cdf+`.
 
+*Exercise {exercise-chapter}.{counter:exercise}*
+
 To get a feel for the Pareto distribution, let’s see how different the
 world would be if the distribution of human height were Pareto. With the
 parameters latexmath:[x_{m} = 1] m and latexmath:[\alpha = 1.7], we get
@@ -2975,7 +3053,9 @@
 billion people in Pareto world, how many do we expect to be taller than
 1 km? How tall do we expect the tallest person to be?
 
-[#weibull]#[weibull]#
+*Exercise {exercise-chapter}.{counter:exercise}*
+
+[[weibull]]
 
 The Weibull distribution is a generalization of the exponential
 distribution that comes up in failure analysis (see
@@ -2992,6 +3072,8 @@
 Use `+random.weibullvariate+` to generate a sample from a Weibull
 distribution and use it to test your transformation.
 
+*Exercise {exercise-chapter}.{counter:exercise}*
+
 For small values of latexmath:[n], we don’t expect an empirical
 distribution to fit an analytic distribution exactly. One way to
 evaluate the quality of fit is to generate a sample from an analytic
@@ -3007,6 +3089,8 @@
 Plot the distribution of the random values and compare it to the actual
 distribution. You can use `+random.expovariate+` to generate the values.
 
+*Exercise {exercise-chapter}.{counter:exercise}*
+
 In the repository for this book, you’ll find a set of data files called
 `+mystery0.dat+`, `+mystery1.dat+`, and so on. Each contains a sequence
 of random numbers generated from an analytic distribution.
@@ -3023,7 +3107,9 @@
 distribution generated each file. If you are stumped, you can look in
 `+mystery.py+`, which contains the code that generated the files.
 
-[#income]#[income]#
+*Exercise {exercise-chapter}.{counter:exercise}*
+
+[[income]]
 
 The distributions of wealth and income are sometimes modeled using
 lognormal and Pareto distributions. To see which is better, let’s look
@@ -3115,6 +3201,7 @@
 For example, `+thinkstats2+` provides a class named `+NormalPdf+` that
 evaluates the normal density function.
 
+[source]
 ....
 class NormalPdf(Pdf):
 
@@ -3138,7 +3225,7 @@
 
 The following example creates a NormalPdf with the mean and variance of
 adult female heights, in cm, from the BRFSS (see
-<<brfss>>). Then it computes the density of the
+<<lognormal>>). Then it computes the density of the
 distribution at a location one standard deviation from the mean.
 
 ....
@@ -3173,11 +3260,9 @@
 `+mu - 3*sigma+` to `+mu + 3*sigma+`. Optionally, `+MakePmf+` and
 `+Render+` can take keyword arguments `+low+`, `+high+`, and `+n+`.
 
-image::figs/pdf_example.png[A normal PDF that models adult female height
-in the U.S., and the kernel density estimate of a sample with
-latexmath:[n=500].,height=211]
-
-[#pdf_example]#[pdf_example]#
+[[pdf_example]]
+.A normal PDF that models adult female height in the U.S., and the kernel density estimate of a sample with latexmath:[n=500].
+image::figs/pdf_example.png[height=211]
 
 === Kernel density estimation
 
@@ -3188,6 +3273,7 @@
 `+scipy+` provides an implementation of KDE and `+thinkstats2+` provides
 a class called `+EstimatedPdf+` that uses it:
 
+[source]
 ....
 class EstimatedPdf(Pdf):
 
@@ -3242,10 +3328,9 @@
 
 === The distribution framework
 
-image::figs/distribution_functions.png[A framework that relates
-representations of distribution functions.,height=211]
-
-[#dist_framework]#[dist_framework]#
+[[dist_framework]]
+.A framework that relates representations of distribution functions.
+image::figs/distribution_functions.png[height=211]
 
 At this point we have seen PMFs, CDFs and PDFs; let’s take a minute to
 review. <<dist_framework>> shows how these
@@ -3298,6 +3383,7 @@
 These methods are all implemented with dictionary operations. For
 example:
 
+[source]
 ....
 # class _DictWrapper
 
@@ -3327,6 +3413,7 @@
 Pmf provides `+Normalize+`, which computes the sum of the probabilities
 and divides through by a factor:
 
+[source]
 ....
 # class Pmf
 
@@ -3373,6 +3460,7 @@
 Given a sequence, pandas Series, or dictionary, the constructor makes a
 Hist. Then it uses the Hist to initialize the attributes:
 
+[source]
 ....
         self.xs, freqs = zip(*sorted(dw.Items()))
         self.ps = np.cumsum(freqs, dtype=np.float)
@@ -3388,6 +3476,7 @@
 Here is the implementation of `+Prob+`, which takes a value and returns
 its cumulative probability:
 
+[source]
 ....
 # class Cdf
     def Prob(self, x):
@@ -3402,6 +3491,7 @@
 here is the implementation of `+Value+`, which takes a cumulative
 probability and returns the corresponding value:
 
+[source]
 ....
 # class Cdf
     def Value(self, p):
@@ -3416,6 +3506,7 @@
 consecutive cumulative probabilities. If you call the Cdf constructor
 and pass a Pmf, it computes differences by calling `+Cdf.Items+`:
 
+[source]
 ....
 # class Cdf
     def Items(self):
@@ -3448,6 +3539,7 @@
 ++++
 Or if you prefer Python notation:
 
+[source]
 ....
 def RawMoment(xs, k):
     return sum(x**k for x in xs) / len(xs)
@@ -3466,6 +3558,7 @@
 ++++
 Or in Python:
 
+[source]
 ....
 def CentralMoment(xs, k):
     mean = RawMoment(xs, 1)
@@ -3483,7 +3576,7 @@
 When you report moment-based statistics, it is important to think about
 the units. For example, if the values latexmath:[x_i] are in cm, the
 first raw moment is also in cm. But the second moment is in
-cmlatexmath:[^2], the third moment is in cmlatexmath:[^3], and so on.
+cm{_}latexmath:[^2], the third moment is in cm{_}latexmath:[^3], and so on.
 
 Because of these units, moments are hard to interpret by themselves.
 That’s why, for the second moment, it is common to report standard
@@ -3505,6 +3598,7 @@
 distribution. Given a sequence of values, latexmath:[x_i], the *sample
 skewness*, latexmath:[g_1], can be computed like this:
 
+[source]
 ....
 def StandardizedMoment(xs, k):
     var = CentralMoment(xs, 2)
@@ -3543,6 +3637,7 @@
 Where latexmath:[\bar{x}] is the sample mean, latexmath:[m] is the
 median, and latexmath:[S] is the standard deviation. Or in Python:
 
+[source]
 ....
 def Median(xs):
     cdf = thinkstats2.Cdf(xs)
@@ -3560,14 +3655,14 @@
 This statistic is *robust*, which means that it is less vulnerable to
 the effect of outliers.
 
-image::figs/density_totalwgt_kde.png[Estimated PDF of birthweight data
-from the NSFG.,height=211]
-
-[#density_totalwgt_kde]#[density_totalwgt_kde]#
+[[density_totalwgt_kde]]
+.Estimated PDF of birthweight data from the NSFG.
+image::figs/density_totalwgt_kde.png[height=211]
 
 As an example, let’s look at the skewness of birth weights in the NSFG
 pregnancy data. Here’s the code to estimate and plot the PDF:
 
+[source]
 ....
     live, firsts, others = first.MakeFrames()
     data = live.totalwgt_lb.dropna()
@@ -3582,14 +3677,14 @@
 skewness coefficients are negative: sample skewness is -0.59; Pearson’s
 median skewness is -0.23.
 
-image::figs/density_wtkg2_kde.png[Estimated PDF of adult weight data from
-the BRFSS.,height=211]
-
-[#density_wtkg2_kde]#[density_wtkg2_kde]#
+[[density_wtkg2_kde]]
+.Estimated PDF of adult weight data from the BRFSS.
+image::figs/density_wtkg2_kde.png[height=211]
 
 Now let’s compare this distribution to the distribution of adult weight
 in the BRFSS. Again, here’s the code:
 
+[source]
 ....
     df = brfss.ReadBrfss(nrows=None)
     data = df.wtkg2.dropna()
@@ -3614,8 +3709,13 @@
 
 === Exercises
 
+:exercise!:
+{counter2:exercise-chapter}
+
 A solution to this exercise is in `+chap06soln.py+`.
 
+*Exercise {exercise-chapter}.{counter:exercise}*
+
 The distribution of income is famously skewed to the right. In this
 exercise, we’ll measure how strong that skew is.
 
@@ -3701,6 +3801,7 @@
 
 Here’s the code that reads the data file and extracts height and weight:
 
+[source]
 ....
     df = brfss.ReadBrfss(nrows=None)
     sample = thinkstats2.SampleRows(df, 5000)
@@ -3708,7 +3809,7 @@
 ....
 
 `+SampleRows+` chooses a random subset of the data:
-
+[source]
 ....
 def SampleRows(df, nrows, replace=False):
     indices = np.random.choice(df.index, nrows, replace=replace)
@@ -3723,6 +3824,7 @@
 
 `+thinkplot+` provides `+Scatter+`, which makes scatter plots:
 
+[source]
 ....
     thinkplot.Scatter(heights, weights)
     thinkplot.Show(xlabel='Height (cm)',
@@ -3733,9 +3835,9 @@
 The result, in <<scatter1>> (left), shows the shape of the
 relationship. As we expected, taller people tend to be heavier.
 
-image::figs/scatter1.png[Scatter plots of weight versus height for the
-respondents in the BRFSS, unjittered (left), jittered
-(right).,height=288]
+[[scatter1]]
+.Scatter plots of weight versus height for the respondents in the BRFSS, unjittered (left), jittered (right).
+image::figs/scatter1.png[height=288]
 
 But this is not the best representation of the data, because the data
 are packed into columns. The problem is that the heights are rounded to
@@ -3748,6 +3850,7 @@
 were rounded to the nearest inch, they might be off by up to 0.5 inches
 or 1.3 cm. Similarly, the weights might be off by 0.5 kg.
 
+[source]
 ....
     heights = thinkstats2.Jitter(heights, 1.3)
     weights = thinkstats2.Jitter(weights, 0.5)
@@ -3755,6 +3858,7 @@
 
 Here’s the implementation of `+Jitter+`:
 
+[source]
 ....
 def Jitter(values, jitter=0.5):
     n = len(values)
@@ -3773,12 +3877,14 @@
 of the figure and gives disproportionate emphasis to outliers. This
 effect is called *saturation*.
 
-image::figs/scatter2.png[Scatter plot with jittering and transparency
-(left), hexbin plot (right).,height=288]
+[[scatter2]]
+.Scatter plot with jittering and transparency (left), hexbin plot (right).
+image::figs/scatter2.png[height=288]
 
 We can solve this problem with the `+alpha+` parameter, which makes the
 points partly transparent:
 
+[source]
 ....
     thinkplot.Scatter(heights, weights, alpha=0.2)
 ....
@@ -3799,6 +3905,7 @@
 divides the graph into hexagonal bins and colors each bin according to
 how many data points fall in it. `+thinkplot+` provides `+HexBin+`:
 
+[source]
 ....
     thinkplot.HexBin(heights, weights)
 ....
@@ -3822,6 +3929,7 @@
 
 NumPy and pandas provide functions for binning data:
 
+[source]
 ....
     df = df.dropna(subset=['htm3', 'wtkg2'])
     bins = np.arange(135, 210, 5)
@@ -3838,14 +3946,16 @@
 fall below the lowest bin are mapped to index 0. Values above the
 highest bin are mapped to `+len(bins)+`.
 
-image::figs/scatter3.png[Percentiles of weight for a range of height
-bins.,height=240]
+[[scatter3]]
+.Percentiles of weight for a range of height bins.
+image::figs/scatter3.png[height=240]
 
 `+groupby+` is a DataFrame method that returns a GroupBy object; used in
 a `+for+` loop, `+groups+` iterates the names of the groups and the
 DataFrames that represent them. So, for example, we can print the number
 of rows in each group like this:
 
+[source]
 ....
 for i, group in groups:
     print(i, len(group))
@@ -3853,6 +3963,7 @@
 
 Now for each group we can compute the mean height and the CDF of weight:
 
+[source]
 ....
     heights = [group.htm3.mean() for i, group in groups]
     cdfs = [thinkstats2.Cdf(group.wtkg2) for i, group in groups]
@@ -3860,6 +3971,7 @@
 
 Finally, we can plot percentiles of weight versus height:
 
+[source]
 ....
     for percent in [75, 50, 25]:
         weights = [cdf.Percentile(percent) for cdf in cdfs]
@@ -3946,6 +4058,7 @@
 orthogonal, and negative if they point in opposite directions.
 `+thinkstats2+` uses `+np.dot+` to implement `+Cov+` efficiently:
 
+[source]
 ....
 def Cov(xs, ys, meanx=None, meany=None):
     xs = np.asarray(xs)
@@ -4008,6 +4121,7 @@
 
 Here is the implementation in `+thinkstats2+`:
 
+[source]
 ....
 def Corr(xs, ys):
     xs = np.asarray(xs)
@@ -4046,10 +4160,9 @@
 If there’s a nonlinear relationship, latexmath:[\rho] understates its
 strength.
 
-image:figs/Correlation_examples.png[Examples of datasets with a range of
-correlations.,height=240]
-
-[#corr_examples]#[corr_examples]#
+[[corr_examples]]
+.Examples of datasets with a range of correlations.
+image::figs/Correlation_examples.png[height=240]
 
 <<corr_examples>> is from
 http://wikipedia.org/wiki/Correlation_and_dependence. It shows scatter
@@ -4082,6 +4195,7 @@
 `+thinkstats2+` provides a function that computes Spearman’s rank
 correlation:
 
+[source]
 ....
 def SpearmanCorr(xs, ys):
     xranks = pandas.Series(xs).rank()
@@ -4095,6 +4209,7 @@
 
 I could also use `+Series.corr+` directly and specify Spearman’s method:
 
+[source]
 ....
 def SpearmanCorr(xs, ys):
     xs = pandas.Series(xs)
@@ -4118,6 +4233,7 @@
 of skewness is to compute Pearson’s correlation with log-weight and
 height:
 
+[source]
 ....
     thinkstats2.Corr(df.htm3, np.log(df.wtkg2)))
 ....
@@ -4182,8 +4298,13 @@
 
 === Exercises
 
+:exercise!:
+{counter2:exercise-chapter}
+
 A solution to this exercise is in `+chap07soln.py+`.
 
+*Exercise {exercise-chapter}.{counter:exercise}*
+
 Using data from the NSFG, make a scatter plot of birth weight versus
 mother’s age. Plot percentiles of birth weight versus mother’s age.
 Compute Pearson’s and Spearman’s correlations. How would you
@@ -4216,6 +4337,7 @@
 natural division of subjects into groups in ways that are at least
 approximately random.
 
+[[estimation]]
 == Estimation
 
 The code for this chapter is in `+estimation.py+`. For information about
@@ -4274,6 +4396,7 @@
 Here is a function that simulates the estimation game and computes the
 root mean squared error (RMSE), which is the square root of MSE:
 
+[source]
 ....
 def Estimate1(n=7, m=1000):
     mu = 0
@@ -4298,6 +4421,7 @@
 
 Here’s the function that computes RMSE:
 
+[source]
 ....
 def RMSE(estimates, actual):
     e2 = [(estimate-actual)**2 for estimate in estimates]
@@ -4373,6 +4497,7 @@
 Here is a function that simulates the estimation game and tests the
 performance of latexmath:[S^2] and latexmath:[S_{n-1}^2]:
 
+[source]
 ....
 def Estimate2(n=7, m=1000):
     mu = 0
@@ -4401,6 +4526,7 @@
 `+MeanError+` computes the mean difference between the estimates and the
 actual value:
 
+[source]
 ....
 def MeanError(estimates, actual):
     errors = [estimate-actual for estimate in estimates]
@@ -4464,6 +4590,7 @@
 
 The following function answers that question:
 
+[source]
 ....
 def SimulateSample(mu=90, sigma=7.5, n=9, m=1000):
     means = []
@@ -4481,8 +4608,9 @@
 `+n+` is the sample size, the number of gorillas we measured. `+m+` is
 the number of times we run the simulation.
 
-image::figs/estimation1.png[Sampling distribution of latexmath:[\bar{x}],
-with confidence interval.,height=240]
+[[estimation1]]
+.Sampling distribution of latexmath:[\bar{x}], with confidence interval.
+image::figs/estimation1.png[height=240]
 
 In each iteration, we choose `+n+` values from a normal distribution
 with the given parameters, and compute the sample mean, `+xbar+`. We run
@@ -4615,6 +4743,7 @@
 To test the performance of these estimators, we can simulate the
 sampling process:
 
+[source]
 ....
 def Estimate3(n=7, m=1000):
     lam = 2
@@ -4649,9 +4778,14 @@
 
 === Exercises
 
+:exercise!:
+{counter2:exercise-chapter}
+
 For the following exercises, you might want to start with a copy of
 `+estimation.py+`. Solutions are in `+chap08soln.py+`
 
+*Exercise {exercise-chapter}.{counter:exercise}*
+
 In this chapter we used latexmath:[\bar{x}] and median to estimate
 latexmath:[\mu], and found that latexmath:[\bar{x}] yields lower MSE.
 Also, we used latexmath:[S^2] and latexmath:[S_{n-1}^2] to estimate
@@ -4662,6 +4796,8 @@
 biased estimates of latexmath:[\mu]. Also check whether latexmath:[S^2]
 or latexmath:[S_{n-1}^2] yields a lower MSE.
 
+*Exercise {exercise-chapter}.{counter:exercise}*
+
 Suppose you draw a sample with size latexmath:[n=10] from an exponential
 distribution with latexmath:[\lambda=2]. Simulate this experiment 1000
 times and plot the sampling distribution of the estimate latexmath:[L].
@@ -4671,6 +4807,8 @@
 Repeat the experiment with a few different values of latexmath:[n] and
 make a plot of standard error versus latexmath:[n].
 
+*Exercise {exercise-chapter}.{counter:exercise}*
+
 In games like hockey and soccer, the time between goals is roughly
 exponential. So you could estimate a team’s goal-scoring rate by
 observing the number of goals they score in a game. This estimation
@@ -4736,7 +4874,7 @@
 
 There are several ways we could formulate this question, including
 Fisher null hypothesis testing, Neyman-Pearson decision theory, and
-Bayesian inferencefootnote:[For more about Bayesian inference, see the
+Bayesian inference{_}footnote:[For more about Bayesian inference, see the
 sequel to this book, _Think Bayes_.]. What I present here is a subset of
 all three that makes up most of what people use in practice, which I
 will call *classical hypothesis testing*.
@@ -4782,6 +4920,7 @@
 `+thinkstats2+` provides `+HypothesisTest+`, a class that represents the
 structure of a classical hypothesis test. Here is the definition:
 
+[source]
 ....
 class HypothesisTest(object):
 
@@ -4824,13 +4963,14 @@
 fraction of elements in `+test_stats+` that exceed or equal the observed
 test statistic, `+self.actual+`.
 
-As a simple examplefootnote:[Adapted from MacKay, _Information Theory,
+As a simple example{_}footnote:[Adapted from MacKay, _Information Theory,
 Inference, and Learning Algorithms_, 2003.], suppose we toss a coin 250
 times and see 140 heads and 110 tails. Based on this result, we might
 suspect that the coin is biased; that is, more likely to land heads. To
 test this hypothesis, we compute the probability of seeing such a
 difference if the coin is actually fair:
 
+[source]
 ....
 class CoinTest(thinkstats2.HypothesisTest):
 
@@ -4858,6 +4998,7 @@
 
 Now all we have to do is instantiate `+CoinTest+` and call `+PValue+`:
 
+[source]
 ....
     ct = CoinTest((140, 110))
     pvalue = ct.PValue()
@@ -4895,6 +5036,7 @@
 *permutation*; that is, we can take values for first babies and others
 and shuffle them, treating the two groups as one big group:
 
+[source]
 ....
 class DiffMeansPermute(thinkstats2.HypothesisTest):
 
@@ -4927,6 +5069,7 @@
 
 To test the difference in pregnancy length, we run:
 
+[source]
 ....
     live, firsts, others = first.MakeFrames()
     data = firsts.prglngth.values, others.prglngth.values
@@ -4941,12 +5084,14 @@
 a difference as big as the observed effect about 17% of the time. So
 this effect is not statistically significant.
 
-image::figs/hypothesis1.png[CDF of difference in mean pregnancy length
-under the null hypothesis.,height=240]
+[[hypothesis1]]
+.CDF of difference in mean pregnancy length under the null hypothesis.
+image::figs/hypothesis1.png[height=240]
 
 `+HypothesisTest+` provides `+PlotCdf+`, which plots the distribution of
 the test statistic and a gray line indicating the observed effect size:
 
+[source]
 ....
     ht.PlotCdf()
     thinkplot.Show(xlabel='test statistic',
@@ -4974,6 +5119,7 @@
 then we would not take the absolute value of the difference; instead we
 would use this test statistic:
 
+[source]
 ....
 class DiffMeansOneSided(DiffMeansPermute):
 
@@ -5004,6 +5150,7 @@
 to be on time. So we might hypothesize that the standard deviation is
 higher. Here’s how we can test that:
 
+[source]
 ....
 class DiffStdPermute(DiffMeansPermute):
 
@@ -5035,6 +5182,7 @@
 world where the distributions of age and birth weight are the same, but
 where the variables are unrelated:
 
+[source]
 ....
 class CorrelationPermute(thinkstats2.HypothesisTest):
 
@@ -5055,6 +5203,7 @@
 
 Here’s the code that reads the data and runs the test:
 
+[source]
 ....
     live, firsts, others = first.MakeFrames()
     live = live.dropna(subset=['agepreg', 'totalwgt_lb'])
@@ -5083,7 +5232,7 @@
 confiscate the die, but now you have to prove that it is crooked. You
 roll the die 60 times and get the following results:
 
-[cols="<,^,^,^,^,^,^",options="header",]
+[cols="<,^,^,^,^,^,^",options="header,autowidth",]
 |===
 |Value |1 |2 |3 |4 |5 |6
 |Frequency |8 |9 |19 |5 |8 |11
@@ -5102,6 +5251,7 @@
 
 Here’s a version of `+HypothesisTest+` that answers that question:
 
+[source]
 ....
 class DiceTest(thinkstats2.HypothesisTest):
 
@@ -5147,6 +5297,7 @@
 Where latexmath:[O_i] are the observed frequencies and latexmath:[E_i]
 are the expected frequencies. Here’s the Python code:
 
+[source]
 ....
 class DiceChiTest(DiceTest):
 
@@ -5187,6 +5338,7 @@
 
 The code combines elements from previous examples:
 
+[source]
 ....
 class PregLengthTest(thinkstats2.HypothesisTest):
 
@@ -5217,6 +5369,7 @@
 
 Here’s the code that computes the test statistic:
 
+[source]
 ....
 # class PregLengthTest:
 
@@ -5293,6 +5446,7 @@
 accurate, we can use the observed samples as a model of the population
 and run hypothesis tests with simulated data:
 
+[source]
 ....
 def FalseNegRate(data, num_runs=100):
     group1, group2 = data
@@ -5318,6 +5472,7 @@
 `+Resample+` takes a sequence and draws a sample with the same length,
 with replacement:
 
+[source]
 ....
 def Resample(xs):
     return np.random.choice(xs, len(xs), replace=True)
@@ -5325,6 +5480,7 @@
 
 Here’s the code that tests pregnancy lengths:
 
+[source]
 ....
     live, firsts, others = first.MakeFrames()
     data = firsts.prglngth.values, others.prglngth.values
@@ -5399,8 +5555,13 @@
 
 === Exercises
 
+:exercise!:
+{counter2:exercise-chapter}
+
 A solution to these exercises is in `+chap09soln.py+`.
 
+*Exercise {exercise-chapter}.{counter:exercise}*
+
 As sample size increases, the power of a hypothesis test increases,
 which means it is more likely to be positive if the effect is real.
 Conversely, as sample size decreases, the test is less likely to be
@@ -5414,6 +5575,8 @@
 What happens to the p-values of these tests as sample size decreases?
 What is the smallest sample size that yields a positive test?
 
+*Exercise {exercise-chapter}.{counter:exercise}*
+
 In <<testdiff>>, we simulated the null hypothesis by
 permutation; that is, we treated the observed values as if they
 represented the entire population, and randomly assigned the members of
@@ -5482,6 +5645,7 @@
 But unless the correlation is perfect, this prediction is only
 approximate. The vertical deviation from the line, or *residual*, is
 
+[source]
 ....
 res = ys - (inter + slope * xs)
 ....
@@ -5528,6 +5692,7 @@
 `+thinkstats2+` provides simple functions that demonstrate linear least
 squares:
 
+[source]
 ....
 def LeastSquares(xs, ys):
     meanx, varx = MeanVar(xs)
@@ -5547,6 +5712,7 @@
 `+thinkstats2+` also provides `+FitLine+`, which takes `+inter+` and
 `+slope+` and returns the fitted line for a sequence of `+xs+`.
 
+[source]
 ....
 def FitLine(xs, inter, slope):
     fit_xs = np.sort(xs)
@@ -5557,6 +5723,7 @@
 We can use these functions to compute the least squares fit for birth
 weight as a function of mother’s age.
 
+[source]
 ....
     live, firsts, others = first.MakeFrames()
     live = live.dropna(subset=['agepreg', 'totalwgt_lb'])
@@ -5578,8 +5745,9 @@
 year old mother is 7.3 pounds. The slope is 0.27 ounces per year, or
 0.17 pounds per decade.
 
-image::figs/linear1.png[Scatter plot of birth weight and mother’s age
-with a linear fit.,height=240]
+[[linear1]]
+.Scatter plot of birth weight and mother’s age with a linear fit.
+image::figs/linear1.png[height=240]
 
 <<linear1>> shows a scatter plot of birth weight and age
 along with the fitted line. It’s a good idea to look at a figure like
@@ -5591,6 +5759,7 @@
 Another useful test is to plot the residuals. `+thinkstats2+` provides a
 function that computes residuals:
 
+[source]
 ....
 def Residuals(xs, ys, inter, slope):
     xs = np.asarray(xs)
@@ -5603,7 +5772,9 @@
 `+inter+` and `+slope+`. It returns the differences between the actual
 values and the fitted line.
 
-image::figs/linear2.png[Residuals of the linear fit.,height=240]
+[[linear2]]
+.Residuals of the linear fit.
+image::figs/linear2.png[height=240]
 
 To visualize the residuals, I group respondents by age and compute
 percentiles in each group, as we saw in
@@ -5640,6 +5811,7 @@
 observed pregnancies as if they were the entire population and draw
 samples, with replacement, from the observed sample.
 
+[source]
 ....
 def SamplingDistributions(live, iters=101):
     t = []
@@ -5661,6 +5833,7 @@
 `+thinkstats2+` also provides `+ResampleRows+`, which returns a sample
 the same size as the original:
 
+[source]
 ....
 def ResampleRows(df):
     return SampleRows(df, len(df), replace=True)
@@ -5673,6 +5846,7 @@
 I summarize the sampling distributions by printing the standard error
 and confidence interval:
 
+[source]
 ....
 def Summarize(estimates, actual=None):
     mean = thinkstats2.Mean(estimates)
@@ -5696,6 +5870,7 @@
 the fitted lines, or for a less cluttered representation, plot a 90%
 confidence interval for each age. Here’s the code:
 
+[source]
 ....
 def PlotConfidenceIntervals(xs, inters, slopes,
                             percent=90, **options):
@@ -5721,9 +5896,9 @@
 interval, it selects the 5th and 95th percentiles. `+FillBetween+` draws
 a polygon that fills the space between two lines.
 
-image::figs/linear3.png[50% and 90% confidence intervals showing
-variability in the fitted line due to sampling error of `+inter+` and
-`+slope+`.,height=240]
+[[linear3]]
+.50% and 90% confidence intervals showing variability in the fitted line due to sampling error of `+inter+` and `+slope+`.
+image::figs/linear3.png[height=240]
 
 <<linear3>> shows the 50% and 90% confidence intervals
 for curves fitted to birth weight as a function of mother’s age. The
@@ -5751,6 +5926,7 @@
 determination*, usually denoted latexmath:[R^2] and called
 "`R-squared`":
 
+[source]
 ....
 def CoefDetermination(ys, res):
     return 1 - Var(res) / Var(ys)
@@ -5827,6 +6003,7 @@
 can model the birth weights as random variations around their mean.
 Here’s a HypothesisTest for this model:
 
+[source]
 ....
 class SlopeTest(thinkstats2.HypothesisTest):
 
@@ -5854,6 +6031,7 @@
 
 Here’s the code that runs the hypothesis test:
 
+[source]
 ....
     live, firsts, others = first.MakeFrames()
     live = live.dropna(subset=['agepreg', 'totalwgt_lb'])
@@ -5878,10 +6056,9 @@
 distributions are identical. The distributions are also symmetric, for
 reasons we will see in <<CLT>>.
 
-image::figs/linear4.png[The sampling distribution of the estimated slope
-and the distribution of slopes generated under the null hypothesis. The
-vertical lines are at 0 and the observed slope, 0.017
-lbs/year.,height=240]
+[[linear4]]
+.The sampling distribution of the estimated slope and the distribution of slopes generated under the null hypothesis. The vertical lines are at 0 and the observed slope, 0.017 lbs/year.
+image::figs/linear4.png[height=240]
 
 So we could estimate the p-value two ways:
 
@@ -5900,6 +6077,7 @@
 Here’s the code that estimates the p-value of the slope using the
 sampling distribution:
 
+[source]
 ....
     inters, slopes = SamplingDistributions(live, iters=1001)
     slope_cdf = thinkstats2.Cdf(slopes)
@@ -5945,6 +6123,7 @@
 according to the weights in `+finalwgt+`, and returns a DataFrame
 containing the resampled rows:
 
+[source]
 ....
 def ResampleRowsWeighted(df, column='finalwgt'):
     weights = df[column]
@@ -5965,6 +6144,7 @@
 Now we can compare the effect of resampling with and without weights.
 Without weights, we generate the sampling distribution like this:
 
+[source]
 ....
     estimates = [ResampleRows(live).totalwgt_lb.mean()
                  for _ in range(iters)]
@@ -5972,6 +6152,7 @@
 
 With weights, it looks like this:
 
+[source]
 ....
     estimates = [ResampleRowsWeighted(live).totalwgt_lb.mean()
                  for _ in range(iters)]
@@ -5979,10 +6160,9 @@
 
 The following table summarizes the results:
 
-[cols="<,^,^,^",]
+[cols="<,^,^,^",options="header,autowidth"]
 |===
-| |mean birth |standard |90% CI
-| |weight (lbs) |error |
+| |mean birth weight (lbs) |standard error |90% CI
 |Unweighted |7.27 |0.014 |(7.24, 7.29)
 |Weighted |7.35 |0.014 |(7.32, 7.37)
 |===
@@ -5995,8 +6175,13 @@
 
 === Exercises
 
+:exercise!:
+{counter2:exercise-chapter}
+
 A solution to this exercise is in `+chap10soln.ipynb+`
 
+*Exercise {exercise-chapter}.{counter:exercise}*
+
 Using the data from the BRFSS, compute the linear least squares fit for
 log(weight) versus height. How would you best present the estimated
 parameters for a model like this where one of the variables is
@@ -6081,6 +6266,7 @@
 As an example, I’ll run the model from the previous chapter with
 StatsModels:
 
+[source]
 ....
     import statsmodels.formula.api as smf
 
@@ -6107,6 +6293,7 @@
 that maps from variable names to their parameters, so we can get the
 intercept and slope like this:
 
+[source]
 ....
     inter = results.params['Intercept']
     slope = results.params['agepreg']
@@ -6119,6 +6306,7 @@
 p-values, so we can check whether the estimated slope is statistically
 significant:
 
+[source]
 ....
     slope_pvalue = results.pvalues['agepreg']
 ....
@@ -6138,6 +6326,7 @@
 The results object provides `+summary()+`, which represents the results
 in a readable format.
 
+[source]
 ....
     print(results.summary())
 ....
@@ -6178,6 +6367,7 @@
 plausible. Then we’ll use multiple regression to investigate more
 carefully. First, let’s see how big the difference in weight is:
 
+[source]
 ....
 diff_weight = firsts.totalwgt_lb.mean() - others.totalwgt_lb.mean()
 ....
@@ -6185,6 +6375,7 @@
 First babies are 0.125 lbs lighter, or 2 ounces. And the difference in
 ages:
 
+[source]
 ....
 diff_age = firsts.agepreg.mean() - others.agepreg.mean()
 ....
@@ -6192,6 +6383,7 @@
 The mothers of first babies are 3.59 years younger. Running the linear
 model again, we get the change in birth weight as a function of age:
 
+[source]
 ....
 results = smf.ols('totalwgt_lb ~ agepreg', data=live).fit()
 slope = results.params['agepreg']
@@ -6201,6 +6393,7 @@
 difference in ages, we get the expected difference in birth weight for
 first babies and others, due to mother’s age:
 
+[source]
 ....
 slope * diff_age
 ....
@@ -6212,6 +6405,7 @@
 Using multiple regression, we can explore these relationships more
 systematically.
 
+[source]
 ....
     live['isfirst'] = live.birthord == 1
     formula = 'totalwgt_lb ~ isfirst'
@@ -6281,6 +6475,7 @@
 One option is to create a column, `+agepreg2+`, that contains the
 squares of the ages:
 
+[source]
 ....
     live['agepreg2'] = live.agepreg**2
     formula = 'totalwgt_lb ~ isfirst + agepreg + agepreg2'
@@ -6313,7 +6508,7 @@
 
 The following table summarizes the results of these regressions:
 
-[cols="<,^,^,^,^",options="header",]
+[cols="<,^,^,^,^",options="header,autowidth",]
 |===
 | |isfirst |agepreg |agepreg2 |latexmath:[R^2]
 |Model 1 |-0.125 * |– |– |0.002
@@ -6369,6 +6564,7 @@
 https://en.wikipedia.org/wiki/Join_(SQL)). Join is implemented as a
 DataFrame method, so we can perform the operation like this:
 
+[source]
 ....
     live = live[live.prglngth>30]
     resp = chap01soln.ReadFemResp()
@@ -6400,6 +6596,7 @@
 than a second on an ordinary desktop computer. Now we can start testing
 variables.
 
+[source]
 ....
     t = []
     for name in join.columns:
@@ -6438,6 +6635,7 @@
 The next step is to sort the results and select the variables that yield
 the highest values of latexmath:[R^2].
 
+[source]
 ....
     t.sort(reverse=True)
     for mse, name in t[:30]:
@@ -6492,6 +6690,7 @@
 Having identified potential explanatory variables, I tested a few models
 and settled on this one:
 
+[source]
 ....
     formula = ('totalwgt_lb ~ agepreg + C(race) + babysex==1 + '
                'nbrnaliv>1 + paydu==1 + totincr')
@@ -6598,12 +6797,14 @@
 Odds and probabilities are different representations of the same
 information. Given a probability, you can compute the odds like this:
 
+[source]
 ....
     o = p / (1-p)
 ....
 
 Given odds in favor, you can convert to probability like this:
 
+[source]
 ....
     p = o / (o+1)
 ....
@@ -6623,6 +6824,7 @@
 latexmath:[x_2]. We can compute the predicted value of
 latexmath:[\log o], and then convert to a probability:
 
+[source]
 ....
     o = np.exp(log_o)
     p = o / (o+1)
@@ -6707,6 +6909,7 @@
 
 Again, I load the NSFG data and select pregnancies longer than 30 weeks:
 
+[source]
 ....
     live, firsts, others = first.MakeFrames()
     df = live[live.prglngth>30]
@@ -6716,6 +6919,7 @@
 boolean), so I create a new column named `+boy+`, using `+astype(int)+`
 to convert to binary integers:
 
+[source]
 ....
     df['boy'] = (df.babysex==1).astype(int)
 ....
@@ -6725,6 +6929,7 @@
 see if these effects appear in the NSFG data. I’ll start with the
 mother’s age:
 
+[source]
 ....
     import statsmodels.formula.api as smf
 
@@ -6741,6 +6946,7 @@
 they are NumPy arrays, it is sometimes convenient to convert them to
 DataFrames:
 
+[source]
 ....
     endog = pandas.DataFrame(model.endog, columns=[model.endog_names])
     exog = pandas.DataFrame(model.exog, columns=model.exog_names)
@@ -6766,6 +6972,7 @@
 comparing models. For example, here’s a model that includes several
 factors believed to be associated with sex ratio:
 
+[source]
 ....
     formula = 'boy ~ agepreg + hpagelb + birthord + C(race)'
     model = smf.logit(formula, data=df)
@@ -6800,6 +7007,7 @@
 strategy is to guess "`boy`" every time. The accuracy of this strategy
 is just the fraction of boys:
 
+[source]
 ....
     actual = endog['boy']
     baseline = actual.mean()
@@ -6810,6 +7018,7 @@
 
 Here’s how we compute the accuracy of the model:
 
+[source]
 ....
     predict = (results.predict() >= 0.5)
     true_pos = predict * actual
@@ -6824,6 +7033,7 @@
 Similarly, `+true_neg+` indicates the cases where we guess "`girl`" and
 get it right. Accuracy is the fraction of correct guesses:
 
+[source]
 ....
     acc = (sum(true_pos) + sum(true_neg)) / len(actual)
 ....
@@ -6837,6 +7047,7 @@
 pool. Suppose your friend is 35 years old and white, her husband is 39,
 and they are expecting their third child:
 
+[source]
 ....
     columns = ['agepreg', 'hpagelb', 'birthord', 'race']
     new = pandas.DataFrame([[35, 39, 3, 2]], columns=columns)
@@ -6850,8 +7061,13 @@
 
 === Exercises
 
+:exercise!:
+{counter2:exercise-chapter}
+
 My solution to these exercises is in `+chap11soln.ipynb+`.
 
+*Exercise {exercise-chapter}.{counter:exercise}*
+
 Suppose one of your co-workers is expecting a baby and you are
 participating in an office pool to predict the date of birth. Assuming
 that bets are placed during the 30th week of pregnancy, what variables
@@ -6859,6 +7075,8 @@
 variables that are known before the birth, and likely to be available to
 the people in the pool.
 
+*Exercise {exercise-chapter}.{counter:exercise}*
+
 The Trivers-Willard hypothesis suggests that for many mammals the sex
 ratio depends on "`maternal condition`"; that is, factors like the
 mother’s age, size, health, and social status. See
@@ -6872,6 +7090,8 @@
 in the pregnancy and respondent files. Can you find any factors with a
 substantial effect?
 
+*Exercise {exercise-chapter}.{counter:exercise}*
+
 If the quantity you want to predict is a count, you can use Poisson
 regression, which is implemented in StatsModels with a function called
 `+poisson+`. It works the same way as `+ols+` and `+logit+`. As an
@@ -6882,6 +7102,8 @@
 graduate whose annual household income exceeds $75,000. How many
 children would you predict she has born?
 
+*Exercise {exercise-chapter}.{counter:exercise}*
+
 If the quantity you want to predict is categorical, you can use
 multinomial logistic regression, which is implemented in StatsModels
 with a function called `+mnlogit+`. As an exercise, let’s use it to
@@ -6962,6 +7184,7 @@
 The data I downloaded from Mr. Jones’s site is in the repository for
 this book. The following code reads it into a pandas DataFrame:
 
+[source]
 ....
     transactions = pandas.read_csv('mj-clean.csv', parse_dates=[5])
 ....
@@ -6993,6 +7216,7 @@
 by reported quality, and then transform each group into an equally
 spaced series by computing the mean daily price per gram.
 
+[source]
 ....
 def GroupByQualityAndDay(transactions):
     groups = transactions.groupby('quality')
@@ -7011,6 +7235,7 @@
 The loop iterates through the groups and calls `+GroupByDay+`, which
 computes the daily average price and returns a new DataFrame:
 
+[source]
 ....
 def GroupByDay(transactions, func=np.mean):
     grouped = transactions[['date', 'ppg']].groupby('date')
@@ -7051,6 +7276,7 @@
 DataFrame of daily prices. Here’s the code I use to plot the three time
 series:
 
+[source]
 ....
     thinkplot.PrePlot(rows=3)
     for i, (name, daily) in enumerate(dailies.items()):
@@ -7074,8 +7300,9 @@
 Since the labels on the x-axis are dates, I use `+pyplot.xticks+` to
 rotate the "`ticks`" 30 degrees, making them more readable.
 
-image::figs/timeseries1.png[Time series of daily price per gram for high,
-medium, and low quality cannabis.,width=336]
+[[timeseries1]]
+.Time series of daily price per gram for high, medium, and low quality cannabis.
+image::figs/timeseries1.png[width=336]
 
 <<timeseries1>> shows the result. One apparent feature in
 these plots is a gap around November 2013. It’s possible that data
@@ -7098,6 +7325,7 @@
 of daily prices and computes a least squares fit, returning the model
 and results objects from StatsModels:
 
+[source]
 ....
 def RunLinearModel(daily):
     model = smf.ols('ppg ~ years', data=daily)
@@ -7107,6 +7335,7 @@
 
 Then we can iterate through the qualities and fit a model to each:
 
+[source]
 ....
     for name, daily in dailies.items():
         model, results = RunLinearModel(daily)
@@ -7116,7 +7345,7 @@
 
 Here are the results:
 
-[cols="<,<,<,^",options="header",]
+[cols="<,<,<,^",options="header,autowidth",]
 |===
 |quality |intercept |slope |latexmath:[R^2]
 |high |13.450 |-0.708 |0.444
@@ -7138,6 +7367,7 @@
 
 The following code plots the observed prices and the fitted values:
 
+[source]
 ....
 def PlotFittedValues(model, results, label=''):
     years = model.exog[:,1]
@@ -7150,8 +7380,9 @@
 `+exog+` and `+endog+`, NumPy arrays with the exogenous (explanatory)
 and endogenous (dependent) variables.
 
-image::figs/timeseries2.png[Time series of daily price per gram for high
-quality cannabis, and a linear least squares fit.,height=240]
+[[timeseries2]]
+.Time series of daily price per gram for high quality cannabis, and a linear least squares fit.
+image::figs/timeseries2.png[height=240]
 
 `+PlotFittedValues+` makes a scatter plot of the data points and a line
 plot of the fitted values. <<timeseries2>> shows the
@@ -7218,6 +7449,7 @@
 represent this missing data explicitly. We can do that by "`reindexing`"
 the DataFrame:
 
+[source]
 ....
     dates = pandas.date_range(daily.index.min(), daily.index.max())
     reindexed = daily.reindex(dates)
@@ -7230,6 +7462,7 @@
 
 Now we can plot the rolling mean like this:
 
+[source]
 ....
     roll_mean = pandas.rolling_mean(reindexed.ppg, 30)
     thinkplot.Plot(roll_mean.index, roll_mean)
@@ -7238,8 +7471,9 @@
 The window size is 30, so each value in `+roll_mean+` is the mean of 30
 values from `+reindexed.ppg+`.
 
-image::figs/timeseries10.png[Daily price and a rolling mean (left) and
-exponentially-weighted moving average (right).,height=240]
+[[timeseries10]]
+.Daily price and a rolling mean (left) and exponentially-weighted moving average (right).
+image::figs/timeseries10.png[height=240]
 
 <<timeseries10>> (left) shows the result. The rolling
 mean seems to do a good job of smoothing out the noise and extracting
@@ -7253,6 +7487,7 @@
 the weights for previous values drop off exponentially. Second, the
 pandas implementation of EWMA handles missing values better.
 
+[source]
 ....
     ewma = pandas.ewma(reindexed.ppg, span=30)
     thinkplot.Plot(ewma.index, ewma)
@@ -7281,6 +7516,7 @@
 A simple and common way to fill missing data is to use a moving average.
 The Series method `+fillna+` does just what we want:
 
+[source]
 ....
     reindexed.ppg.fillna(ewma, inplace=True)
 ....
@@ -7292,6 +7528,7 @@
 A drawback of this method is that it understates the noise in the
 series. We can solve that problem by adding in resampled residuals:
 
+[source]
 ....
     resid = (reindexed.ppg - ewma).dropna()
     fake_data = ewma + thinkstats2.Resample(resid, len(reindexed))
@@ -7303,7 +7540,9 @@
 random sample of residuals. Finally, `+fillna+` replaces `+nan+` with
 values from `+fake_data+`.
 
-image::figs/timeseries8.png[Daily price with filled data.,height=240]
+[[timeseries8]]
+.Daily price with filled data.
+image::figs/timeseries8.png[height=240]
 
 <<timeseries8>> shows the result. The filled data is
 visually similar to the actual values. Since the resampled residuals are
@@ -7322,6 +7561,7 @@
 interval called a *lag*, and then compute the correlation of the shifted
 series with the original:
 
+[source]
 ....
 def SerialCorr(series, lag=1):
     xs = series[lag:]
@@ -7344,6 +7584,7 @@
 subtract away the trend. For example, we can compute the residual of the
 EWMA and then compute its serial correlation:
 
+[source]
 ....
     ewma = pandas.ewma(reindexed.ppg, span=30)
     resid = reindexed.ppg - ewma
@@ -7358,7 +7599,7 @@
 To check for weekly, monthly, and yearly seasonality, I ran the analysis
 again with different lags. Here are the results:
 
-[cols="^,^,^,^",options="header",]
+[cols="^,^,^,^",options="header,autowidth",]
 |===
 |lag |high |medium |low
 |1 |-0.029 |-0.014 |0.034
@@ -7385,6 +7626,7 @@
 analysis, including `+acf+`, which computes the autocorrelation
 function:
 
+[source]
 ....
     import statsmodels.tsa.stattools as smtsa
     acf = smtsa.acf(filled.resid, nlags=365, unbiased=True)
@@ -7405,9 +7647,9 @@
 With `+lag=0+`, `+acf+` computes the correlation of the series with
 itself, which is always 1.
 
-image::figs/timeseries9.png[Autocorrelation function for daily prices
-(left), and daily prices with a simulated weekly seasonality
-(right).,height=240]
+[[timeseries9]]
+.Autocorrelation function for daily prices (left), and daily prices with a simulated weekly seasonality (right).
+image::figs/timeseries9.png[height=240]
 
 <<timeseries9>> (left) shows autocorrelation functions
 for the three quality categories, with `+nlags=40+`. The gray region
@@ -7430,6 +7672,7 @@
 fall on Friday or Saturday and add a random amount to the price, chosen
 from a uniform distribution from $0 to $2.
 
+[source]
 ....
 def AddWeeklySeasonality(daily):
     frisat = (daily.index.dayofweek==4) | (daily.index.dayofweek==5)
@@ -7461,6 +7704,7 @@
 which takes a DataFrame containing the explanatory variables and returns
 a sequence of predictions. Here’s the code:
 
+[source]
 ....
 def GenerateSimplePrediction(results, years):
     n = len(years)
@@ -7509,6 +7753,7 @@
 parameters are correct, but the random residuals could have been
 different. Here is a function that runs the simulations:
 
+[source]
 ....
 def SimulateResults(daily, iters=101):
     model, results = RunLinearModel(daily)
@@ -7537,6 +7782,7 @@
 
 The next step is to use the simulated results to generate predictions:
 
+[source]
 ....
 def GeneratePredictions(result_seq, years, add_resid=False):
     n = len(years)
@@ -7560,12 +7806,14 @@
 prediction. `+GeneratePredictions+` iterates through the sequence of
 RegressionResults and generates a sequence of predictions.
 
-image::figs/timeseries4.png[Predictions based on linear fits, showing
-variation due to sampling error and prediction error.,height=240]
+[[timeseries4]]
+.Predictions based on linear fits, showing variation due to sampling error and prediction error.
+image::figs/timeseries4.png[height=240]
 
 Finally, here’s the code that plots a 90% confidence interval for the
 predictions:
 
+[source]
 ....
 def PlotPredictions(daily, years, iters=101, percent=90):
     result_seq = SimulateResults(daily, iters=iters)
@@ -7613,8 +7861,9 @@
 `+SimulateResults+` to use intervals of observation with different start
 and end dates. My implementation is in `+timeseries.py+`.
 
-image::figs/timeseries5.png[Predictions based on linear fits, showing
-variation due to the interval of observation.,height=240]
+[[timeseries5]]
+.Predictions based on linear fits, showing variation due to the interval of observation.
+image::figs/timeseries5.png[height=240]
 
 <<timeseries5>> shows the result for the medium quality
 category. The lightest gray area shows a confidence interval that
@@ -7642,8 +7891,13 @@
 
 === Exercises
 
+:exercise!:
+{counter2:exercise-chapter}
+
 My solution to these exercises is in `+chap12soln.py+`.
 
+*Exercise {exercise-chapter}.{counter:exercise}*
+
 The linear model I used in this chapter has the obvious drawback that it
 is linear, and there is no reason to expect prices to change linearly
 over time. We can add flexibility to the model by adding a quadratic
@@ -7655,6 +7909,8 @@
 should be able to reuse code in `+timeseries.py+` to generate
 predictions.
 
+*Exercise {exercise-chapter}.{counter:exercise}*
+
 Write a definition for a class named `+SerialCorrelationTest+` that
 extends `+HypothesisTest+` from <<hypotest>>. It should
 take a series and a lag as data, compute the serial correlation of the
@@ -7665,6 +7921,8 @@
 is statistically significant. Also test the residuals of the linear
 model and (if you did the previous exercise), the quadratic model.
 
+*Exercise {exercise-chapter}.{counter:exercise}*
+
 There are several ways to extend the EWMA model to generate predictions.
 One of the simplest is something like this:
 
@@ -7750,6 +8008,7 @@
 For example, in the NSFG dataset, we know the duration of 11189 complete
 pregnancies. We can read this data and compute the CDF:
 
+[source]
 ....
     preg = nsfg.ReadFemPreg()
     complete = preg.query('outcome in [1, 3, 4]').prglngth
@@ -7764,13 +8023,15 @@
 The DataFrame method `+query+` takes a boolean expression and evaluates
 it for each row, selecting the rows that yield True.
 
-image::figs/survival1.png[Cdf and survival curve for pregnancy length
-(top), hazard curve (bottom).,height=288]
+[[survival1]]
+.Cdf and survival curve for pregnancy length (top), hazard curve (bottom).
+image::figs/survival1.png[height=288]
 
 <<survival1>> (top) shows the CDF of pregnancy length and
 its complement, the survival curve. To represent the survival curve, I
 define an object that wraps a Cdf and adapts the interface:
 
+[source]
 ....
 class SurvivalFunction(object):
     def __init__(self, cdf, label=''):
@@ -7794,6 +8055,7 @@
 We can instantiate a `+SurvivalFunction+` by passing the CDF of
 lifetimes:
 
+[source]
 ....
     sf = SurvivalFunction(cdf)
 ....
@@ -7801,6 +8063,7 @@
 `+SurvivalFunction+` also provides `+__getitem__+` and `+Prob+`, which
 evaluates the survival curve:
 
+[source]
 ....
 # class SurvivalFunction
 
@@ -7827,6 +8090,7 @@
 `+SurvivalFunction+` provides `+Render+`, so we can plot `+sf+` using
 the functions in `+thinkplot+`:
 
+[source]
 ....
     thinkplot.Plot(sf)
 ....
@@ -7854,6 +8118,7 @@
 `+SurvivalFunction+` provides `+MakeHazard+`, which calculates the
 hazard function:
 
+[source]
 ....
 # class SurvivalFunction
 
@@ -7869,6 +8134,7 @@
 
 The `+HazardFunction+` object is a wrapper for a pandas Series:
 
+[source]
 ....
 class HazardFunction(object):
 
@@ -7955,6 +8221,7 @@
 
 Here’s the code:
 
+[source]
 ....
 def EstimateHazardFunction(complete, ongoing, label=''):
 
@@ -8033,6 +8300,7 @@
 First, we read the respondent file and replace invalid values of
 `+cmmarrhx+`:
 
+[source]
 ....
     resp = chap01soln.ReadFemResp()
     resp.cmmarrhx.replace([9997, 9998, 9999], np.nan, inplace=True)
@@ -8041,6 +8309,7 @@
 Then we compute each respondent’s age when married and age when
 interviewed:
 
+[source]
 ....
     resp['agemarry'] = (resp.cmmarrhx - resp.cmbirth) / 12.0
     resp['age'] = (resp.cmintvw - resp.cmbirth) / 12.0
@@ -8050,6 +8319,7 @@
 have been married, and `+ongoing+`, which is the age at interview for
 women who have not:
 
+[source]
 ....
     complete = resp[resp.evrmarry==1].agemarry
     ongoing = resp[resp.evrmarry==0].age
@@ -8057,6 +8327,7 @@
 
 Finally we compute the hazard function.
 
+[source]
 ....
     hf = EstimateHazardFunction(complete, ongoing)
 ....
@@ -8082,6 +8353,7 @@
 The `+HazardFunction+` class provides `+MakeSurvival+`, which computes
 this product:
 
+[source]
 ....
 # class HazardFunction:
 
@@ -8101,8 +8373,9 @@
 compute the complement of `+ss+`, make a Cdf, and then instantiate a
 SurvivalFunction object.
 
-image::figs/survival2.png[Hazard function for age at first marriage (top)
-and survival curve (bottom).,height=240]
+[[survival2]]
+.Hazard function for age at first marriage (top) and survival curve (bottom).
+image::figs/survival2.png[height=240]
 
 <<survival2>> (bottom) shows the result. The survival
 curve is steepest between 25 and 35, when most women get married.
@@ -8137,6 +8410,7 @@
 
 We can quantify sampling error by resampling. Here’s the code:
 
+[source]
 ....
 def ResampleSurvival(resp, iters=101):
     low, high = resp.agemarry.min(), resp.agemarry.max()
@@ -8168,9 +8442,9 @@
 `+PercentileRows+` takes this sequence and computes the 5th and 95th
 percentiles, returning a 90% confidence interval for the survival curve.
 
-image::figs/survival3.png[Survival curve for age at first marriage (dark
-line) and a 90% confidence interval based on weighted resampling (gray
-line).,height=240]
+[[survival3]]
+.Survival curve for age at first marriage (dark line) and a 90% confidence interval based on weighted resampling (gray line).
+image::figs/survival3.png[height=240]
 
 <<survival3>> shows the result along with the survival
 curve we estimated in the previous section. The confidence interval
@@ -8200,6 +8474,7 @@
 2006–2010 used in <<replication>>; and the Cycle 5 data
 from 1995. In total these datasets include 30,769 respondents.
 
+[source]
 ....
     resp5 = ReadFemResp1995()
     resp6 = ReadFemResp2002()
@@ -8210,6 +8485,7 @@
 For each DataFrame, `+resp+`, I use `+cmbirth+` to compute the decade of
 birth for each respondent:
 
+[source]
 ....
     month0 = pandas.to_datetime('1899-12-15')
     dates = [month0 + pandas.DateOffset(months=cm) 
@@ -8228,6 +8504,7 @@
 due to sampling error, I resample the data, group respondents by decade,
 and plot survival curves:
 
+[source]
 ....
     for i in range(iters):
         samples = [thinkstats2.ResampleRowsWeighted(resp) 
@@ -8246,6 +8523,7 @@
 
 `+EstimateSurvivalByDecade+` plots survival curves for each cohort:
 
+[source]
 ....
 def EstimateSurvivalByDecade(resp):
     for name, group in groups:
@@ -8253,8 +8531,9 @@
         thinkplot.Plot(sf)
 ....
 
-image::figs/survival4.png[Survival curves for respondents born during
-different decades.,height=240]
+[[survival4]]
+.Survival curves for respondents born during different decades.
+image::figs/survival4.png[height=240]
 
 <<survival4>> shows the results. Several patterns are
 visible:
@@ -8292,6 +8571,7 @@
 cohort. HazardFunction provides a method, `+Extend+`, that copies the
 tail from another longer HazardFunction:
 
+[source]
 ....
 # class HazardFunction
 
@@ -8310,6 +8590,7 @@
 Now we can extend the HazardFunction for each cohort, using values from
 the predecessor:
 
+[source]
 ....
 def PlotPredictionsByDecade(groups):
     hfs = []
@@ -8333,8 +8614,9 @@
 on. Then it converts each HazardFunction to a SurvivalFunction and plots
 it.
 
-image::figs/survival5.png[Survival curves for respondents born during
-different decades, with predictions for the later cohorts.,height=240]
+[[survival5]]
+.Survival curves for respondents born during different decades, with predictions for the later cohorts.
+image::figs/survival5.png[height=240]
 
 <<survival5>> shows the results; I’ve removed the 50s
 cohort to make the predictions more visible. These results suggest that
@@ -8351,6 +8633,7 @@
 The first step is to extract the PMF of lifetimes. `+SurvivalFunction+`
 provides a method that does that:
 
+[source]
 ....
 # class SurvivalFunction
 
@@ -8380,6 +8663,7 @@
 "`expected`" means average. `+SurvivalFunction+` provides a method that
 does that, too:
 
+[source]
 ....
 # class SurvivalFunction
 
@@ -8412,8 +8696,9 @@
 exceeds `+t+`. By subtracting `+t+` we get the mean remaining pregnancy
 length.
 
-image::figs/survival6.png[Expected remaining pregnancy length (left) and
-years until first marriage (right).,height=240]
+[[survival6]]
+.Expected remaining pregnancy length (left) and years until first marriage (right).
+image::figs/survival6.png[height=240]
 
 <<survival6>> (left) shows the expected remaining
 pregnancy length as a function of the current duration. For example,
@@ -8462,6 +8747,7 @@
 
 Here’s the code that computes and plots these functions:
 
+[source]
 ....
     rem_life1 = sf1.RemainingLifetime()
     thinkplot.Plot(rem_life1)
@@ -8479,8 +8765,13 @@
 
 === Exercises
 
+:exercise!:
+{counter2:exercise-chapter}
+
 My solution to this exercise is in `+chap13soln.py+`.
 
+*Exercise {exercise-chapter}.{counter:exercise}*
+
 In NSFG Cycles 6 and 7, the variable `+cmdivorcx+` contains the date of
 divorce for the respondent’s first marriage, if applicable, encoded in
 century-months.
@@ -8581,20 +8872,20 @@
 distribution has this property; if latexmath:[X \sim \mathcal{N}~(\mu,
 \sigma^2)],
 
+[[eq:1,Equation 1]]
 [latexmath]
 ++++
-\label{eq:1}
-    X' \sim \mathcal{N}~(a \mu + b, a^{2} \sigma^2) \tag{1}
+    X' \sim \mathcal{N}~(a \mu + b, a^{2} \sigma^2) \tag*{1}
 ++++
 Normal distributions are also closed under addition. If
 latexmath:[Z = X + Y] and
 latexmath:[X \sim \mathcal{N}~(\mu_{X}, \sigma_{X}^{2})] and
 latexmath:[Y \sim \mathcal{N}~(\mu_{Y}, \sigma_{Y}^{2})] then
 
+[[eq:2,Equation 2]]
 [latexmath]
 ++++
-\label{eq:2}
-    Z \sim \mathcal{N}~(\mu_X + \mu_Y, \sigma_X^2 + \sigma_Y^2) \tag{2}
+    Z \sim \mathcal{N}~(\mu_X + \mu_Y, \sigma_X^2 + \sigma_Y^2) \tag*{2}
 ++++
 In the special case latexmath:[Z = X + X], we have
 
@@ -8605,10 +8896,10 @@
 and in general if we draw latexmath:[n] values of latexmath:[X] and add
 them up, we have
 
+[[eq:3,Equation 3]]
 [latexmath]
 ++++
-\label{eq:3}
-    Z \sim \mathcal{N}~(n \mu_X, n \sigma_X^2) \tag{3}
+    Z \sim \mathcal{N}~(n \mu_X, n \sigma_X^2) \tag*{3}
 ++++
 
 [[sampling-distributions]]
@@ -8633,14 +8924,14 @@
 ++++
 Y \sim \mathcal{N}~(n \mu, n \sigma^2)
 ++++
-using Equation #eq:3[[eq:3]]. And if we divide by latexmath:[n], the
+using <<eq:3>>. And if we divide by latexmath:[n], the
 sample mean, latexmath:[Z], is distributed
 
 [latexmath]
 ++++
 Z \sim \mathcal{N}~(\mu, \sigma^2/n)
 ++++
-using Equation #eq:1[[eq:1]] with latexmath:[a = 1/n].
+using <<eq:1>> with latexmath:[a = 1/n].
 
 The distribution of latexmath:[Z] is the sampling distribution of
 latexmath:[\bar{x}]. The mean of latexmath:[Z] is latexmath:[\mu], which
@@ -8666,6 +8957,7 @@
 provides a wrapper function that makes the SciPy function a little
 easier to use:
 
+[source]
 ....
 def EvalNormalCdfInverse(p, mu=0, sigma=1):
     return scipy.stats.norm.ppf(p, loc=mu, scale=sigma)
@@ -8695,6 +8987,7 @@
 `+Normal+` that represents a normal distribution and encodes the
 equations in the previous sections. Here’s what it looks like:
 
+[source]
 ....
 class Normal(object):
 
@@ -8716,17 +9009,18 @@
 ....
 
 `+Normal+` provides `+Sum+`, which takes a sample size, `+n+`, and
-returns the distribution of the sum of `+n+` values, using Equation
-#eq:3[[eq:3]]:
+returns the distribution of the sum of `+n+` values, using 
+<<eq:3>>:
 
 ....
     def Sum(self, n):
         return Normal(n * self.mu, n * self.sigma2)
 ....
 
-Normal also knows how to multiply and divide using Equation
-#eq:1[[eq:1]]:
+Normal also knows how to multiply and divide using 
+<<eq:1>>:
 
+[source]
 ....
     def __mul__(self, factor):
         return Normal(factor * self.mu, factor**2 * self.sigma2)
@@ -8797,6 +9091,7 @@
 To see how the Central Limit Theorem works, and when it doesn’t, let’s
 try some experiments. First, we’ll try an exponential distribution:
 
+[source]
 ....
 def MakeExpoSamples(beta=2.0, iters=1000):
     samples = []
@@ -8824,6 +9119,7 @@
 The return value is a list of `+(n, sample)+` pairs. For each pair, we
 make a normal probability plot:
 
+[source]
 ....
 def NormalPlotSamples(samples, plot=1, ylabel=''):
     for n, sample in samples:
@@ -8837,8 +9133,9 @@
 `+NormalPlotSamples+` takes the list of pairs from `+MakeExpoSamples+`
 and generates a row of normal probability plots.
 
-image::figs/normal1.png[Distributions of sums of exponential values (top
-row) and lognormal values (bottom row).,height=336]
+[[normal1]]
+.Distributions of sums of exponential values (top row) and lognormal values (bottom row).
+image::figs/normal1.png[height=336]
 
 <<normal1>> (top row) shows the results. With `+n=1+`,
 the distribution of the sum is still exponential, so the normal
@@ -8852,8 +9149,9 @@
 longer to converge. With `+n=10+` the normal probability plot is nowhere
 near straight, but with `+n=100+` it is approximately normal.
 
-image::figs/normal2.png[Distributions of sums of Pareto values (top row)
-and correlated exponential values (bottom row).,height=336]
+[[normal2]]
+.Distributions of sums of Pareto values (top row) and correlated exponential values (bottom row).
+image::figs/normal2.png[height=336]
 
 Pareto distributions are even more skewed than lognormal. Depending on
 the parameters, many Pareto distributions do not have finite mean and
@@ -8872,6 +9170,7 @@
 `+GenerateCorrelated+` returns an iterator of `+n+` normal values with
 serial correlation `+rho+`:
 
+[source]
 ....
 def GenerateCorrelated(rho, n):
     x = random.gauss(0, 1)
@@ -8892,6 +9191,7 @@
 `+GenerateExpoCorrelated+` takes the resulting sequence and transforms
 it to exponential:
 
+[source]
 ....
 def GenerateExpoCorrelated(rho, n):
     normal = list(GenerateCorrelated(rho, n))
@@ -8938,6 +9238,7 @@
 pregnancy lengths is the same for first babies and others. So we can
 compute the sampling distribution of the mean like this:
 
+[source]
 ....
     dist1 = SamplingDistMean(live.prglngth, len(firsts))
     dist2 = SamplingDistMean(live.prglngth, len(others))
@@ -8948,6 +9249,7 @@
 values and the sample size, and returns a Normal object representing the
 sampling distribution:
 
+[source]
 ....
 def SamplingDistMean(data, n):
     mean, var = data.mean(), data.var()
@@ -8967,8 +9269,9 @@
 
 Next, we compute the sampling distribution of the difference in the
 means. The `+Normal+` class knows how to perform subtraction using
-Equation #eq:2[[eq:2]]:
+<<eq:2>>:
 
+[source]
 ....
     def __sub__(self, other):
         return Normal(self.mu - other.mu,
@@ -9033,6 +9336,7 @@
 their correlation? `+StudentCdf+` takes the sample size, `+n+`, and
 returns the sampling distribution of correlation:
 
+[source]
 ....
 def StudentCdf(n):
     ts = np.linspace(-3, 3, 101)
@@ -9048,8 +9352,9 @@
 freedom.`" I won’t explain that term, but you can read about it at
 http://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics).
 
-image::figs/normal4.png[Sampling distribution of correlations for
-uncorrelated normal variables.,height=240]
+[[normal4]]
+.Sampling distribution of correlations for uncorrelated normal variables.
+image::figs/normal4.png[height=240]
 
 To get from `+ts+` to the correlation coefficients, `+rs+`, we apply the
 inverse transform,
@@ -9071,6 +9376,7 @@
 uncorrelated. Using the analytic distribution, we can compute just how
 unlikely:
 
+[source]
 ....
     t = r * math.sqrt((n-2) / (1-r**2))
     p_value = 1 - scipy.stats.t.cdf(t, df=n-2)
@@ -9093,17 +9399,19 @@
 ++++
 One reason the chi-squared statistic is widely used is that its sampling
 distribution under the null hypothesis is analytic; by a remarkable
-coincidencefootnote:[Not really.], it is called the chi-squared
+coincidence{_}footnote:[Not really.], it is called the chi-squared
 distribution. Like the t-distribution, the chi-squared CDF can be
 computed efficiently using gamma functions.
 
-image::figs/normal5.png[Sampling distribution of chi-squared statistics
-for a fair six-sided die.,height=240]
+[[normal5]]
+.Sampling distribution of chi-squared statistics for a fair six-sided die.
+image::figs/normal5.png[height=240]
 
 SciPy provides an implementation of the chi-squared distribution, which
 we use to compute the sampling distribution of the chi-squared
 statistic:
 
+[source]
 ....
 def ChiSquaredCdf(n):
     xs = np.linspace(0, 25, 101)
@@ -9118,6 +9426,7 @@
 We can use this distribution to compute the p-value of the observed test
 statistic, `+chi2+`:
 
+[source]
 ....
     p_value = 1 - scipy.stats.chi2.cdf(chi2, df=n-1)
 ....
@@ -9170,9 +9479,16 @@
 
 === Exercises
 
+:exercise!:
+{counter2:exercise-chapter}
+
 A solution to these exercises is in `+chap14soln.py+`
 
-[#log_clt]#[log_clt]# In <<lognormal>>, we saw
+*Exercise {exercise-chapter}.{counter:exercise}*
+
+[[log_clt]]
+
+In <<lognormal>>, we saw
 that the distribution of adult weights is approximately lognormal. One
 possible explanation is that the weight a person gains each year is
 proportional to their current weight. In that case, adult weight is the
@@ -9202,6 +9518,8 @@
 product. What value of latexmath:[n] is needed to converge to a
 lognormal distribution?
 
+*Exercise {exercise-chapter}.{counter:exercise}*
+
 In <<usingCLT>> we used the Central Limit Theorem to
 find the sampling distribution of the difference in means,
 latexmath:[\delta], under the null hypothesis that both samples are
@@ -9216,7 +9534,9 @@
 Compute this distribution and use it to calculate the standard error and
 a 90% confidence interval for the difference in means.
 
-In a recent paperfootnote:["`Evidence for the persistent effects of an
+*Exercise {exercise-chapter}.{counter:exercise}*
+
+In a recent paper{_}footnote:["`Evidence for the persistent effects of an
 intervention to mitigate gender-sterotypical task allocation within
 student engineering teams,`" Proceedings of the IEEE Frontiers in
 Education Conference, 2014.], Stein et al. investigate the effects of an
@@ -9244,3 +9564,18 @@
 
 Finally, estimate the change in gender gap; what is the sampling
 distribution of this change, and is it statistically significant?
+
+ifdef::backend-html5[]
+
+// Dont number the advert bar
+:sectnums!:
+
+[role=advert-bar]
+include::advert.adoc[]
+
+endif::[]
+
+ifndef::backend-html5[]
+[index]
+== Index
+endif::[]
