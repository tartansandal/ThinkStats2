<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<!--[if IE]><meta http-equiv="X-UA-Compatible" content="IE=edge"><![endif]-->
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Asciidoctor 1.5.6.1">
<meta name="author" content="Allen B. Downey">
<title>Think Stats: Exploratory Data Analysis in Python</title>
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700">
<style>
/* Asciidoctor default stylesheet | MIT License | http://asciidoctor.org */
/* Remove comment around @import statement below when using as a custom stylesheet */
/*@import "https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700";*/
article,aside,details,figcaption,figure,footer,header,hgroup,main,nav,section,summary{display:block}
audio,canvas,video{display:inline-block}
audio:not([controls]){display:none;height:0}
[hidden],template{display:none}
script{display:none!important}
html{font-family:sans-serif;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}
a{background:transparent}
a:focus{outline:thin dotted}
a:active,a:hover{outline:0}
h1{font-size:2em;margin:.67em 0}
abbr[title]{border-bottom:1px dotted}
b,strong{font-weight:bold}
dfn{font-style:italic}
hr{-moz-box-sizing:content-box;box-sizing:content-box;height:0}
mark{background:#ff0;color:#000}
code,kbd,pre,samp{font-family:monospace;font-size:1em}
pre{white-space:pre-wrap}
q{quotes:"\201C" "\201D" "\2018" "\2019"}
small{font-size:80%}
sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}
sup{top:-.5em}
sub{bottom:-.25em}
img{border:0}
svg:not(:root){overflow:hidden}
figure{margin:0}
fieldset{border:1px solid silver;margin:0 2px;padding:.35em .625em .75em}
legend{border:0;padding:0}
button,input,select,textarea{font-family:inherit;font-size:100%;margin:0}
button,input{line-height:normal}
button,select{text-transform:none}
button,html input[type="button"],input[type="reset"],input[type="submit"]{-webkit-appearance:button;cursor:pointer}
button[disabled],html input[disabled]{cursor:default}
input[type="checkbox"],input[type="radio"]{box-sizing:border-box;padding:0}
input[type="search"]{-webkit-appearance:textfield;-moz-box-sizing:content-box;-webkit-box-sizing:content-box;box-sizing:content-box}
input[type="search"]::-webkit-search-cancel-button,input[type="search"]::-webkit-search-decoration{-webkit-appearance:none}
button::-moz-focus-inner,input::-moz-focus-inner{border:0;padding:0}
textarea{overflow:auto;vertical-align:top}
table{border-collapse:collapse;border-spacing:0}
*,*:before,*:after{-moz-box-sizing:border-box;-webkit-box-sizing:border-box;box-sizing:border-box}
html,body{font-size:100%}
body{background:#fff;color:rgba(0,0,0,.8);padding:0;margin:0;font-family:"Noto Serif","DejaVu Serif",serif;font-weight:400;font-style:normal;line-height:1;position:relative;cursor:auto;tab-size:4;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased}
a:hover{cursor:pointer}
img,object,embed{max-width:100%;height:auto}
object,embed{height:100%}
img{-ms-interpolation-mode:bicubic}
.left{float:left!important}
.right{float:right!important}
.text-left{text-align:left!important}
.text-right{text-align:right!important}
.text-center{text-align:center!important}
.text-justify{text-align:justify!important}
.hide{display:none}
img,object,svg{display:inline-block;vertical-align:middle}
textarea{height:auto;min-height:50px}
select{width:100%}
.center{margin-left:auto;margin-right:auto}
.spread{width:100%}
p.lead,.paragraph.lead>p,#preamble>.sectionbody>.paragraph:first-of-type p{font-size:1.21875em;line-height:1.6}
.subheader,.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{line-height:1.45;color:#7a2518;font-weight:400;margin-top:0;margin-bottom:.25em}
div,dl,dt,dd,ul,ol,li,h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6,pre,form,p,blockquote,th,td{margin:0;padding:0;direction:ltr}
a{color:#2156a5;text-decoration:underline;line-height:inherit}
a:hover,a:focus{color:#1d4b8f}
a img{border:none}
p{font-family:inherit;font-weight:400;font-size:1em;line-height:1.6;margin-bottom:1.25em;text-rendering:optimizeLegibility}
p aside{font-size:.875em;line-height:1.35;font-style:italic}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{font-family:"Open Sans","DejaVu Sans",sans-serif;font-weight:300;font-style:normal;color:#ba3925;text-rendering:optimizeLegibility;margin-top:1em;margin-bottom:.5em;line-height:1.0125em}
h1 small,h2 small,h3 small,#toctitle small,.sidebarblock>.content>.title small,h4 small,h5 small,h6 small{font-size:60%;color:#e99b8f;line-height:0}
h1{font-size:2.125em}
h2{font-size:1.6875em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.375em}
h4,h5{font-size:1.125em}
h6{font-size:1em}
hr{border:solid #ddddd8;border-width:1px 0 0;clear:both;margin:1.25em 0 1.1875em;height:0}
em,i{font-style:italic;line-height:inherit}
strong,b{font-weight:bold;line-height:inherit}
small{font-size:60%;line-height:inherit}
code{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;font-weight:400;color:rgba(0,0,0,.9)}
ul,ol,dl{font-size:1em;line-height:1.6;margin-bottom:1.25em;list-style-position:outside;font-family:inherit}
ul,ol{margin-left:1.5em}
ul li ul,ul li ol{margin-left:1.25em;margin-bottom:0;font-size:1em}
ul.square li ul,ul.circle li ul,ul.disc li ul{list-style:inherit}
ul.square{list-style-type:square}
ul.circle{list-style-type:circle}
ul.disc{list-style-type:disc}
ol li ul,ol li ol{margin-left:1.25em;margin-bottom:0}
dl dt{margin-bottom:.3125em;font-weight:bold}
dl dd{margin-bottom:1.25em}
abbr,acronym{text-transform:uppercase;font-size:90%;color:rgba(0,0,0,.8);border-bottom:1px dotted #ddd;cursor:help}
abbr{text-transform:none}
blockquote{margin:0 0 1.25em;padding:.5625em 1.25em 0 1.1875em;border-left:1px solid #ddd}
blockquote cite{display:block;font-size:.9375em;color:rgba(0,0,0,.6)}
blockquote cite:before{content:"\2014 \0020"}
blockquote cite a,blockquote cite a:visited{color:rgba(0,0,0,.6)}
blockquote,blockquote p{line-height:1.6;color:rgba(0,0,0,.85)}
@media only screen and (min-width:768px){h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2}
h1{font-size:2.75em}
h2{font-size:2.3125em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.6875em}
h4{font-size:1.4375em}}
table{background:#fff;margin-bottom:1.25em;border:solid 1px #dedede}
table thead,table tfoot{background:#f7f8f7;font-weight:bold}
table thead tr th,table thead tr td,table tfoot tr th,table tfoot tr td{padding:.5em .625em .625em;font-size:inherit;color:rgba(0,0,0,.8);text-align:left}
table tr th,table tr td{padding:.5625em .625em;font-size:inherit;color:rgba(0,0,0,.8)}
table tr.even,table tr.alt,table tr:nth-of-type(even){background:#f8f8f7}
table thead tr th,table tfoot tr th,table tbody tr td,table tr td,table tfoot tr td{display:table-cell;line-height:1.6}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2;word-spacing:-.05em}
h1 strong,h2 strong,h3 strong,#toctitle strong,.sidebarblock>.content>.title strong,h4 strong,h5 strong,h6 strong{font-weight:400}
.clearfix:before,.clearfix:after,.float-group:before,.float-group:after{content:" ";display:table}
.clearfix:after,.float-group:after{clear:both}
*:not(pre)>code{font-size:.9375em;font-style:normal!important;letter-spacing:0;padding:.1em .5ex;word-spacing:-.15em;background-color:#f7f7f8;-webkit-border-radius:4px;border-radius:4px;line-height:1.45;text-rendering:optimizeSpeed;word-wrap:break-word}
*:not(pre)>code.nobreak{word-wrap:normal}
*:not(pre)>code.nowrap{white-space:nowrap}
pre,pre>code{line-height:1.45;color:rgba(0,0,0,.9);font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;font-weight:400;text-rendering:optimizeSpeed}
em em{font-style:normal}
strong strong{font-weight:400}
.keyseq{color:rgba(51,51,51,.8)}
kbd{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;display:inline-block;color:rgba(0,0,0,.8);font-size:.65em;line-height:1.45;background-color:#f7f7f7;border:1px solid #ccc;-webkit-border-radius:3px;border-radius:3px;-webkit-box-shadow:0 1px 0 rgba(0,0,0,.2),0 0 0 .1em white inset;box-shadow:0 1px 0 rgba(0,0,0,.2),0 0 0 .1em #fff inset;margin:0 .15em;padding:.2em .5em;vertical-align:middle;position:relative;top:-.1em;white-space:nowrap}
.keyseq kbd:first-child{margin-left:0}
.keyseq kbd:last-child{margin-right:0}
.menuseq,.menuref{color:#000}
.menuseq b:not(.caret),.menuref{font-weight:inherit}
.menuseq{word-spacing:-.02em}
.menuseq b.caret{font-size:1.25em;line-height:.8}
.menuseq i.caret{font-weight:bold;text-align:center;width:.45em}
b.button:before,b.button:after{position:relative;top:-1px;font-weight:400}
b.button:before{content:"[";padding:0 3px 0 2px}
b.button:after{content:"]";padding:0 2px 0 3px}
p a>code:hover{color:rgba(0,0,0,.9)}
#header,#content,#footnotes,#footer{width:100%;margin-left:auto;margin-right:auto;margin-top:0;margin-bottom:0;max-width:62.5em;*zoom:1;position:relative;padding-left:.9375em;padding-right:.9375em}
#header:before,#header:after,#content:before,#content:after,#footnotes:before,#footnotes:after,#footer:before,#footer:after{content:" ";display:table}
#header:after,#content:after,#footnotes:after,#footer:after{clear:both}
#content{margin-top:1.25em}
#content:before{content:none}
#header>h1:first-child{color:rgba(0,0,0,.85);margin-top:2.25rem;margin-bottom:0}
#header>h1:first-child+#toc{margin-top:8px;border-top:1px solid #ddddd8}
#header>h1:only-child,body.toc2 #header>h1:nth-last-child(2){border-bottom:1px solid #ddddd8;padding-bottom:8px}
#header .details{border-bottom:1px solid #ddddd8;line-height:1.45;padding-top:.25em;padding-bottom:.25em;padding-left:.25em;color:rgba(0,0,0,.6);display:-ms-flexbox;display:-webkit-flex;display:flex;-ms-flex-flow:row wrap;-webkit-flex-flow:row wrap;flex-flow:row wrap}
#header .details span:first-child{margin-left:-.125em}
#header .details span.email a{color:rgba(0,0,0,.85)}
#header .details br{display:none}
#header .details br+span:before{content:"\00a0\2013\00a0"}
#header .details br+span.author:before{content:"\00a0\22c5\00a0";color:rgba(0,0,0,.85)}
#header .details br+span#revremark:before{content:"\00a0|\00a0"}
#header #revnumber{text-transform:capitalize}
#header #revnumber:after{content:"\00a0"}
#content>h1:first-child:not([class]){color:rgba(0,0,0,.85);border-bottom:1px solid #ddddd8;padding-bottom:8px;margin-top:0;padding-top:1rem;margin-bottom:1.25rem}
#toc{border-bottom:1px solid #efefed;padding-bottom:.5em}
#toc>ul{margin-left:.125em}
#toc ul.sectlevel0>li>a{font-style:italic}
#toc ul.sectlevel0 ul.sectlevel1{margin:.5em 0}
#toc ul{font-family:"Open Sans","DejaVu Sans",sans-serif;list-style-type:none}
#toc li{line-height:1.3334;margin-top:.3334em}
#toc a{text-decoration:none}
#toc a:active{text-decoration:underline}
#toctitle{color:#7a2518;font-size:1.2em}
@media only screen and (min-width:768px){#toctitle{font-size:1.375em}
body.toc2{padding-left:15em;padding-right:0}
#toc.toc2{margin-top:0!important;background-color:#f8f8f7;position:fixed;width:15em;left:0;top:0;border-right:1px solid #efefed;border-top-width:0!important;border-bottom-width:0!important;z-index:1000;padding:1.25em 1em;height:100%;overflow:auto}
#toc.toc2 #toctitle{margin-top:0;margin-bottom:.8rem;font-size:1.2em}
#toc.toc2>ul{font-size:.9em;margin-bottom:0}
#toc.toc2 ul ul{margin-left:0;padding-left:1em}
#toc.toc2 ul.sectlevel0 ul.sectlevel1{padding-left:0;margin-top:.5em;margin-bottom:.5em}
body.toc2.toc-right{padding-left:0;padding-right:15em}
body.toc2.toc-right #toc.toc2{border-right-width:0;border-left:1px solid #efefed;left:auto;right:0}}
@media only screen and (min-width:1280px){body.toc2{padding-left:20em;padding-right:0}
#toc.toc2{width:20em}
#toc.toc2 #toctitle{font-size:1.375em}
#toc.toc2>ul{font-size:.95em}
#toc.toc2 ul ul{padding-left:1.25em}
body.toc2.toc-right{padding-left:0;padding-right:20em}}
#content #toc{border-style:solid;border-width:1px;border-color:#e0e0dc;margin-bottom:1.25em;padding:1.25em;background:#f8f8f7;-webkit-border-radius:4px;border-radius:4px}
#content #toc>:first-child{margin-top:0}
#content #toc>:last-child{margin-bottom:0}
#footer{max-width:100%;background-color:rgba(0,0,0,.8);padding:1.25em}
#footer-text{color:rgba(255,255,255,.8);line-height:1.44}
.sect1{padding-bottom:.625em}
@media only screen and (min-width:768px){.sect1{padding-bottom:1.25em}}
.sect1+.sect1{border-top:1px solid #efefed}
#content h1>a.anchor,h2>a.anchor,h3>a.anchor,#toctitle>a.anchor,.sidebarblock>.content>.title>a.anchor,h4>a.anchor,h5>a.anchor,h6>a.anchor{position:absolute;z-index:1001;width:1.5ex;margin-left:-1.5ex;display:block;text-decoration:none!important;visibility:hidden;text-align:center;font-weight:400}
#content h1>a.anchor:before,h2>a.anchor:before,h3>a.anchor:before,#toctitle>a.anchor:before,.sidebarblock>.content>.title>a.anchor:before,h4>a.anchor:before,h5>a.anchor:before,h6>a.anchor:before{content:"\00A7";font-size:.85em;display:block;padding-top:.1em}
#content h1:hover>a.anchor,#content h1>a.anchor:hover,h2:hover>a.anchor,h2>a.anchor:hover,h3:hover>a.anchor,#toctitle:hover>a.anchor,.sidebarblock>.content>.title:hover>a.anchor,h3>a.anchor:hover,#toctitle>a.anchor:hover,.sidebarblock>.content>.title>a.anchor:hover,h4:hover>a.anchor,h4>a.anchor:hover,h5:hover>a.anchor,h5>a.anchor:hover,h6:hover>a.anchor,h6>a.anchor:hover{visibility:visible}
#content h1>a.link,h2>a.link,h3>a.link,#toctitle>a.link,.sidebarblock>.content>.title>a.link,h4>a.link,h5>a.link,h6>a.link{color:#ba3925;text-decoration:none}
#content h1>a.link:hover,h2>a.link:hover,h3>a.link:hover,#toctitle>a.link:hover,.sidebarblock>.content>.title>a.link:hover,h4>a.link:hover,h5>a.link:hover,h6>a.link:hover{color:#a53221}
.audioblock,.imageblock,.literalblock,.listingblock,.stemblock,.videoblock{margin-bottom:1.25em}
.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{text-rendering:optimizeLegibility;text-align:left;font-family:"Noto Serif","DejaVu Serif",serif;font-size:1rem;font-style:italic}
table.tableblock>caption.title{white-space:nowrap;overflow:visible;max-width:0}
.paragraph.lead>p,#preamble>.sectionbody>.paragraph:first-of-type p{color:rgba(0,0,0,.85)}
table.tableblock #preamble>.sectionbody>.paragraph:first-of-type p{font-size:inherit}
.admonitionblock>table{border-collapse:separate;border:0;background:none;width:100%}
.admonitionblock>table td.icon{text-align:center;width:80px}
.admonitionblock>table td.icon img{max-width:initial}
.admonitionblock>table td.icon .title{font-weight:bold;font-family:"Open Sans","DejaVu Sans",sans-serif;text-transform:uppercase}
.admonitionblock>table td.content{padding-left:1.125em;padding-right:1.25em;border-left:1px solid #ddddd8;color:rgba(0,0,0,.6)}
.admonitionblock>table td.content>:last-child>:last-child{margin-bottom:0}
.exampleblock>.content{border-style:solid;border-width:1px;border-color:#e6e6e6;margin-bottom:1.25em;padding:1.25em;background:#fff;-webkit-border-radius:4px;border-radius:4px}
.exampleblock>.content>:first-child{margin-top:0}
.exampleblock>.content>:last-child{margin-bottom:0}
.sidebarblock{border-style:solid;border-width:1px;border-color:#e0e0dc;margin-bottom:1.25em;padding:1.25em;background:#f8f8f7;-webkit-border-radius:4px;border-radius:4px}
.sidebarblock>:first-child{margin-top:0}
.sidebarblock>:last-child{margin-bottom:0}
.sidebarblock>.content>.title{color:#7a2518;margin-top:0;text-align:center}
.exampleblock>.content>:last-child>:last-child,.exampleblock>.content .olist>ol>li:last-child>:last-child,.exampleblock>.content .ulist>ul>li:last-child>:last-child,.exampleblock>.content .qlist>ol>li:last-child>:last-child,.sidebarblock>.content>:last-child>:last-child,.sidebarblock>.content .olist>ol>li:last-child>:last-child,.sidebarblock>.content .ulist>ul>li:last-child>:last-child,.sidebarblock>.content .qlist>ol>li:last-child>:last-child{margin-bottom:0}
.literalblock pre,.listingblock pre:not(.highlight),.listingblock pre[class="highlight"],.listingblock pre[class^="highlight "],.listingblock pre.CodeRay,.listingblock pre.prettyprint{background:#f7f7f8}
.sidebarblock .literalblock pre,.sidebarblock .listingblock pre:not(.highlight),.sidebarblock .listingblock pre[class="highlight"],.sidebarblock .listingblock pre[class^="highlight "],.sidebarblock .listingblock pre.CodeRay,.sidebarblock .listingblock pre.prettyprint{background:#f2f1f1}
.literalblock pre,.literalblock pre[class],.listingblock pre,.listingblock pre[class]{-webkit-border-radius:4px;border-radius:4px;word-wrap:break-word;padding:1em;font-size:.8125em}
.literalblock pre.nowrap,.literalblock pre[class].nowrap,.listingblock pre.nowrap,.listingblock pre[class].nowrap{overflow-x:auto;white-space:pre;word-wrap:normal}
@media only screen and (min-width:768px){.literalblock pre,.literalblock pre[class],.listingblock pre,.listingblock pre[class]{font-size:.90625em}}
@media only screen and (min-width:1280px){.literalblock pre,.literalblock pre[class],.listingblock pre,.listingblock pre[class]{font-size:1em}}
.literalblock.output pre{color:#f7f7f8;background-color:rgba(0,0,0,.9)}
.listingblock pre.highlightjs{padding:0}
.listingblock pre.highlightjs>code{padding:1em;-webkit-border-radius:4px;border-radius:4px}
.listingblock pre.prettyprint{border-width:0}
.listingblock>.content{position:relative}
.listingblock code[data-lang]:before{display:none;content:attr(data-lang);position:absolute;font-size:.75em;top:.425rem;right:.5rem;line-height:1;text-transform:uppercase;color:#999}
.listingblock:hover code[data-lang]:before{display:block}
.listingblock.terminal pre .command:before{content:attr(data-prompt);padding-right:.5em;color:#999}
.listingblock.terminal pre .command:not([data-prompt]):before{content:"$"}
table.pyhltable{border-collapse:separate;border:0;margin-bottom:0;background:none}
table.pyhltable td{vertical-align:top;padding-top:0;padding-bottom:0;line-height:1.45}
table.pyhltable td.code{padding-left:.75em;padding-right:0}
pre.pygments .lineno,table.pyhltable td:not(.code){color:#999;padding-left:0;padding-right:.5em;border-right:1px solid #ddddd8}
pre.pygments .lineno{display:inline-block;margin-right:.25em}
table.pyhltable .linenodiv{background:none!important;padding-right:0!important}
.quoteblock{margin:0 1em 1.25em 1.5em;display:table}
.quoteblock>.title{margin-left:-1.5em;margin-bottom:.75em}
.quoteblock blockquote,.quoteblock blockquote p{color:rgba(0,0,0,.85);font-size:1.15rem;line-height:1.75;word-spacing:.1em;letter-spacing:0;font-style:italic;text-align:justify}
.quoteblock blockquote{margin:0;padding:0;border:0}
.quoteblock blockquote:before{content:"\201c";float:left;font-size:2.75em;font-weight:bold;line-height:.6em;margin-left:-.6em;color:#7a2518;text-shadow:0 1px 2px rgba(0,0,0,.1)}
.quoteblock blockquote>.paragraph:last-child p{margin-bottom:0}
.quoteblock .attribution{margin-top:.5em;margin-right:.5ex;text-align:right}
.quoteblock .quoteblock{margin-left:0;margin-right:0;padding:.5em 0;border-left:3px solid rgba(0,0,0,.6)}
.quoteblock .quoteblock blockquote{padding:0 0 0 .75em}
.quoteblock .quoteblock blockquote:before{display:none}
.verseblock{margin:0 1em 1.25em 1em}
.verseblock pre{font-family:"Open Sans","DejaVu Sans",sans;font-size:1.15rem;color:rgba(0,0,0,.85);font-weight:300;text-rendering:optimizeLegibility}
.verseblock pre strong{font-weight:400}
.verseblock .attribution{margin-top:1.25rem;margin-left:.5ex}
.quoteblock .attribution,.verseblock .attribution{font-size:.9375em;line-height:1.45;font-style:italic}
.quoteblock .attribution br,.verseblock .attribution br{display:none}
.quoteblock .attribution cite,.verseblock .attribution cite{display:block;letter-spacing:-.025em;color:rgba(0,0,0,.6)}
.quoteblock.abstract{margin:0 0 1.25em 0;display:block}
.quoteblock.abstract blockquote,.quoteblock.abstract blockquote p{text-align:left;word-spacing:0}
.quoteblock.abstract blockquote:before,.quoteblock.abstract blockquote p:first-of-type:before{display:none}
table.tableblock{max-width:100%;border-collapse:separate}
table.tableblock td>.paragraph:last-child p>p:last-child,table.tableblock th>p:last-child,table.tableblock td>p:last-child{margin-bottom:0}
table.tableblock,th.tableblock,td.tableblock{border:0 solid #dedede}
table.grid-all>thead>tr>.tableblock,table.grid-all>tbody>tr>.tableblock{border-width:0 1px 1px 0}
table.grid-all>tfoot>tr>.tableblock{border-width:1px 1px 0 0}
table.grid-cols>*>tr>.tableblock{border-width:0 1px 0 0}
table.grid-rows>thead>tr>.tableblock,table.grid-rows>tbody>tr>.tableblock{border-width:0 0 1px 0}
table.grid-rows>tfoot>tr>.tableblock{border-width:1px 0 0 0}
table.grid-all>*>tr>.tableblock:last-child,table.grid-cols>*>tr>.tableblock:last-child{border-right-width:0}
table.grid-all>tbody>tr:last-child>.tableblock,table.grid-all>thead:last-child>tr>.tableblock,table.grid-rows>tbody>tr:last-child>.tableblock,table.grid-rows>thead:last-child>tr>.tableblock{border-bottom-width:0}
table.frame-all{border-width:1px}
table.frame-sides{border-width:0 1px}
table.frame-topbot{border-width:1px 0}
th.halign-left,td.halign-left{text-align:left}
th.halign-right,td.halign-right{text-align:right}
th.halign-center,td.halign-center{text-align:center}
th.valign-top,td.valign-top{vertical-align:top}
th.valign-bottom,td.valign-bottom{vertical-align:bottom}
th.valign-middle,td.valign-middle{vertical-align:middle}
table thead th,table tfoot th{font-weight:bold}
tbody tr th{display:table-cell;line-height:1.6;background:#f7f8f7}
tbody tr th,tbody tr th p,tfoot tr th,tfoot tr th p{color:rgba(0,0,0,.8);font-weight:bold}
p.tableblock>code:only-child{background:none;padding:0}
p.tableblock{font-size:1em}
td>div.verse{white-space:pre}
ol{margin-left:1.75em}
ul li ol{margin-left:1.5em}
dl dd{margin-left:1.125em}
dl dd:last-child,dl dd:last-child>:last-child{margin-bottom:0}
ol>li p,ul>li p,ul dd,ol dd,.olist .olist,.ulist .ulist,.ulist .olist,.olist .ulist{margin-bottom:.625em}
ul.checklist,ul.none,ol.none,ul.no-bullet,ol.no-bullet,ol.unnumbered,ul.unstyled,ol.unstyled{list-style-type:none}
ul.no-bullet,ol.no-bullet,ol.unnumbered{margin-left:.625em}
ul.unstyled,ol.unstyled{margin-left:0}
ul.checklist{margin-left:.625em}
ul.checklist li>p:first-child>.fa-square-o:first-child,ul.checklist li>p:first-child>.fa-check-square-o:first-child{width:1.25em;font-size:.8em;position:relative;bottom:.125em}
ul.checklist li>p:first-child>input[type="checkbox"]:first-child{margin-right:.25em}
ul.inline{margin:0 auto .625em auto;margin-left:-1.375em;margin-right:0;padding:0;list-style:none;overflow:hidden}
ul.inline>li{list-style:none;float:left;margin-left:1.375em;display:block}
ul.inline>li>*{display:block}
.unstyled dl dt{font-weight:400;font-style:normal}
ol.arabic{list-style-type:decimal}
ol.decimal{list-style-type:decimal-leading-zero}
ol.loweralpha{list-style-type:lower-alpha}
ol.upperalpha{list-style-type:upper-alpha}
ol.lowerroman{list-style-type:lower-roman}
ol.upperroman{list-style-type:upper-roman}
ol.lowergreek{list-style-type:lower-greek}
.hdlist>table,.colist>table{border:0;background:none}
.hdlist>table>tbody>tr,.colist>table>tbody>tr{background:none}
td.hdlist1,td.hdlist2{vertical-align:top;padding:0 .625em}
td.hdlist1{font-weight:bold;padding-bottom:1.25em}
.literalblock+.colist,.listingblock+.colist{margin-top:-.5em}
.colist>table tr>td:first-of-type{padding:.4em .75em 0 .75em;line-height:1;vertical-align:top}
.colist>table tr>td:first-of-type img{max-width:initial}
.colist>table tr>td:last-of-type{padding:.25em 0}
.thumb,.th{line-height:0;display:inline-block;border:solid 4px #fff;-webkit-box-shadow:0 0 0 1px #ddd;box-shadow:0 0 0 1px #ddd}
.imageblock.left,.imageblock[style*="float: left"]{margin:.25em .625em 1.25em 0}
.imageblock.right,.imageblock[style*="float: right"]{margin:.25em 0 1.25em .625em}
.imageblock>.title{margin-bottom:0}
.imageblock.thumb,.imageblock.th{border-width:6px}
.imageblock.thumb>.title,.imageblock.th>.title{padding:0 .125em}
.image.left,.image.right{margin-top:.25em;margin-bottom:.25em;display:inline-block;line-height:0}
.image.left{margin-right:.625em}
.image.right{margin-left:.625em}
a.image{text-decoration:none;display:inline-block}
a.image object{pointer-events:none}
sup.footnote,sup.footnoteref{font-size:.875em;position:static;vertical-align:super}
sup.footnote a,sup.footnoteref a{text-decoration:none}
sup.footnote a:active,sup.footnoteref a:active{text-decoration:underline}
#footnotes{padding-top:.75em;padding-bottom:.75em;margin-bottom:.625em}
#footnotes hr{width:20%;min-width:6.25em;margin:-.25em 0 .75em 0;border-width:1px 0 0 0}
#footnotes .footnote{padding:0 .375em 0 .225em;line-height:1.3334;font-size:.875em;margin-left:1.2em;text-indent:-1.05em;margin-bottom:.2em}
#footnotes .footnote a:first-of-type{font-weight:bold;text-decoration:none}
#footnotes .footnote:last-of-type{margin-bottom:0}
#content #footnotes{margin-top:-.625em;margin-bottom:0;padding:.75em 0}
.gist .file-data>table{border:0;background:#fff;width:100%;margin-bottom:0}
.gist .file-data>table td.line-data{width:99%}
div.unbreakable{page-break-inside:avoid}
.big{font-size:larger}
.small{font-size:smaller}
.underline{text-decoration:underline}
.overline{text-decoration:overline}
.line-through{text-decoration:line-through}
.aqua{color:#00bfbf}
.aqua-background{background-color:#00fafa}
.black{color:#000}
.black-background{background-color:#000}
.blue{color:#0000bf}
.blue-background{background-color:#0000fa}
.fuchsia{color:#bf00bf}
.fuchsia-background{background-color:#fa00fa}
.gray{color:#606060}
.gray-background{background-color:#7d7d7d}
.green{color:#006000}
.green-background{background-color:#007d00}
.lime{color:#00bf00}
.lime-background{background-color:#00fa00}
.maroon{color:#600000}
.maroon-background{background-color:#7d0000}
.navy{color:#000060}
.navy-background{background-color:#00007d}
.olive{color:#606000}
.olive-background{background-color:#7d7d00}
.purple{color:#600060}
.purple-background{background-color:#7d007d}
.red{color:#bf0000}
.red-background{background-color:#fa0000}
.silver{color:#909090}
.silver-background{background-color:#bcbcbc}
.teal{color:#006060}
.teal-background{background-color:#007d7d}
.white{color:#bfbfbf}
.white-background{background-color:#fafafa}
.yellow{color:#bfbf00}
.yellow-background{background-color:#fafa00}
span.icon>.fa{cursor:default}
a span.icon>.fa{cursor:inherit}
.admonitionblock td.icon [class^="fa icon-"]{font-size:2.5em;text-shadow:1px 1px 2px rgba(0,0,0,.5);cursor:default}
.admonitionblock td.icon .icon-note:before{content:"\f05a";color:#19407c}
.admonitionblock td.icon .icon-tip:before{content:"\f0eb";text-shadow:1px 1px 2px rgba(155,155,0,.8);color:#111}
.admonitionblock td.icon .icon-warning:before{content:"\f071";color:#bf6900}
.admonitionblock td.icon .icon-caution:before{content:"\f06d";color:#bf3400}
.admonitionblock td.icon .icon-important:before{content:"\f06a";color:#bf0000}
.conum[data-value]{display:inline-block;color:#fff!important;background-color:rgba(0,0,0,.8);-webkit-border-radius:100px;border-radius:100px;text-align:center;font-size:.75em;width:1.67em;height:1.67em;line-height:1.67em;font-family:"Open Sans","DejaVu Sans",sans-serif;font-style:normal;font-weight:bold}
.conum[data-value] *{color:#fff!important}
.conum[data-value]+b{display:none}
.conum[data-value]:after{content:attr(data-value)}
pre .conum[data-value]{position:relative;top:-.125em}
b.conum *{color:inherit!important}
.conum:not([data-value]):empty{display:none}
dt,th.tableblock,td.content,div.footnote{text-rendering:optimizeLegibility}
h1,h2,p,td.content,span.alt{letter-spacing:-.01em}
p strong,td.content strong,div.footnote strong{letter-spacing:-.005em}
p,blockquote,dt,td.content,span.alt{font-size:1.0625rem}
p{margin-bottom:1.25rem}
.sidebarblock p,.sidebarblock dt,.sidebarblock td.content,p.tableblock{font-size:1em}
.exampleblock>.content{background-color:#fffef7;border-color:#e0e0dc;-webkit-box-shadow:0 1px 4px #e0e0dc;box-shadow:0 1px 4px #e0e0dc}
.print-only{display:none!important}
@media print{@page{margin:1.25cm .75cm}
*{-webkit-box-shadow:none!important;box-shadow:none!important;text-shadow:none!important}
a{color:inherit!important;text-decoration:underline!important}
a.bare,a[href^="#"],a[href^="mailto:"]{text-decoration:none!important}
a[href^="http:"]:not(.bare):after,a[href^="https:"]:not(.bare):after{content:"(" attr(href) ")";display:inline-block;font-size:.875em;padding-left:.25em}
abbr[title]:after{content:" (" attr(title) ")"}
pre,blockquote,tr,img,object,svg{page-break-inside:avoid}
thead{display:table-header-group}
svg{max-width:100%}
p,blockquote,dt,td.content{font-size:1em;orphans:3;widows:3}
h2,h3,#toctitle,.sidebarblock>.content>.title{page-break-after:avoid}
#toc,.sidebarblock,.exampleblock>.content{background:none!important}
#toc{border-bottom:1px solid #ddddd8!important;padding-bottom:0!important}
.sect1{padding-bottom:0!important}
.sect1+.sect1{border:0!important}
#header>h1:first-child{margin-top:1.25rem}
body.book #header{text-align:center}
body.book #header>h1:first-child{border:0!important;margin:2.5em 0 1em 0}
body.book #header .details{border:0!important;display:block;padding:0!important}
body.book #header .details span:first-child{margin-left:0!important}
body.book #header .details br{display:block}
body.book #header .details br+span:before{content:none!important}
body.book #toc{border:0!important;text-align:left!important;padding:0!important;margin:0!important}
body.book #toc,body.book #preamble,body.book h1.sect0,body.book .sect1>h2{page-break-before:always}
.listingblock code[data-lang]:before{display:block}
#footer{background:none!important;padding:0 .9375em}
#footer-text{color:rgba(0,0,0,.6)!important;font-size:.9em}
.hide-on-print{display:none!important}
.print-only{display:block!important}
.hide-for-print{display:none!important}
.show-for-print{display:inherit!important}}
</style>
</head>
<body class="book toc2 toc-left">
<div id="header">
<h1>Think Stats: Exploratory Data Analysis in Python</h1>
<div class="details">
<span id="author" class="author">Allen B. Downey</span><br>
<span id="revnumber">version 2.0.38</span>
</div>
<div id="toc" class="toc2">
<div id="toctitle">Table of Contents</div>
<ul class="sectlevel1">
<li><a href="#_colphon">Colphon</a></li>
<li><a href="#_preface">Preface</a>
<ul class="sectlevel2">
<li><a href="#_how_i_wrote_this_book">How I wrote this book</a></li>
<li><a href="#code">Using the code</a></li>
<li><a href="#_contributor_list">Contributor List</a></li>
</ul>
</li>
<li><a href="#intro">1. Exploratory data analysis</a>
<ul class="sectlevel2">
<li><a href="#_a_statistical_approach">1.1. A statistical approach</a></li>
<li><a href="#nsfg">1.2. The National Survey of Family Growth</a></li>
<li><a href="#_importing_the_data">1.3. Importing the data</a></li>
<li><a href="#dataframe">1.4. DataFrames</a></li>
<li><a href="#_variables">1.5. Variables</a></li>
<li><a href="#cleaning">1.6. Transformation</a></li>
<li><a href="#_validation">1.7. Validation</a></li>
<li><a href="#_interpretation">1.8. Interpretation</a></li>
<li><a href="#_exercises">1.9. Exercises</a></li>
<li><a href="#_glossary">1.10. Glossary</a></li>
</ul>
</li>
<li><a href="#descriptive">2. Distributions</a>
<ul class="sectlevel2">
<li><a href="#_histograms">2.1. Histograms</a></li>
<li><a href="#_representing_histograms">2.2. Representing histograms</a></li>
<li><a href="#_plotting_histograms">2.3. Plotting histograms</a></li>
<li><a href="#_nsfg_variables">2.4. NSFG variables</a></li>
<li><a href="#_outliers">2.5. Outliers</a></li>
<li><a href="#_first_babies">2.6. First babies</a></li>
<li><a href="#mean">2.7. Summarizing distributions</a></li>
<li><a href="#_variance">2.8. Variance</a></li>
<li><a href="#_effect_size">2.9. Effect size</a></li>
<li><a href="#_reporting_results">2.10. Reporting results</a></li>
<li><a href="#_exercises_2">2.11. Exercises</a></li>
<li><a href="#_glossary_2">2.12. Glossary</a></li>
</ul>
</li>
<li><a href="#_probability_mass_functions">3. Probability mass functions</a>
<ul class="sectlevel2">
<li><a href="#_pmfs">3.1. Pmfs</a></li>
<li><a href="#_plotting_pmfs">3.2. Plotting PMFs</a></li>
<li><a href="#visualization">3.3. Other visualizations</a></li>
<li><a href="#_the_class_size_paradox">3.4. The class size paradox</a></li>
<li><a href="#_dataframe_indexing">3.5. DataFrame indexing</a></li>
<li><a href="#_exercises_3">3.6. Exercises</a></li>
<li><a href="#_glossary_3">3.7. Glossary</a></li>
</ul>
</li>
<li><a href="#cumulative">4. Cumulative distribution functions</a>
<ul class="sectlevel2">
<li><a href="#_the_limits_of_pmfs">4.1. The limits of PMFs</a></li>
<li><a href="#_percentiles">4.2. Percentiles</a></li>
<li><a href="#_cdfs">4.3. CDFs</a></li>
<li><a href="#_representing_cdfs">4.4. Representing CDFs</a></li>
<li><a href="#birth_weights">4.5. Comparing CDFs</a></li>
<li><a href="#_percentile_based_statistics">4.6. Percentile-based statistics</a></li>
<li><a href="#random">4.7. Random numbers</a></li>
<li><a href="#_comparing_percentile_ranks">4.8. Comparing percentile ranks</a></li>
<li><a href="#_exercises_4">4.9. Exercises</a></li>
<li><a href="#_glossary_4">4.10. Glossary</a></li>
</ul>
</li>
<li><a href="#modeling">5. Modeling distributions</a>
<ul class="sectlevel2">
<li><a href="#exponential">5.1. The exponential distribution</a></li>
<li><a href="#normal">5.2. The normal distribution</a></li>
<li><a href="#_normal_probability_plot">5.3. Normal probability plot</a></li>
<li><a href="#lognormal">5.4. The lognormal distribution</a></li>
<li><a href="#_the_pareto_distribution">5.5. The Pareto distribution</a></li>
<li><a href="#_generating_random_numbers">5.6. Generating random numbers</a></li>
<li><a href="#_why_model">5.7. Why model?</a></li>
<li><a href="#_exercises_5">5.8. Exercises</a></li>
<li><a href="#_glossary_5">5.9. Glossary</a></li>
</ul>
</li>
<li><a href="#density">6. Probability density functions</a>
<ul class="sectlevel2">
<li><a href="#_pdfs">6.1. PDFs</a></li>
<li><a href="#_kernel_density_estimation">6.2. Kernel density estimation</a></li>
<li><a href="#_the_distribution_framework">6.3. The distribution framework</a></li>
<li><a href="#_hist_implementation">6.4. Hist implementation</a></li>
<li><a href="#_pmf_implementation">6.5. Pmf implementation</a></li>
<li><a href="#_cdf_implementation">6.6. Cdf implementation</a></li>
<li><a href="#_moments">6.7. Moments</a></li>
<li><a href="#_skewness">6.8. Skewness</a></li>
<li><a href="#_exercises_6">6.9. Exercises</a></li>
<li><a href="#_glossary_6">6.10. Glossary</a></li>
</ul>
</li>
<li><a href="#_relationships_between_variables">7. Relationships between variables</a>
<ul class="sectlevel2">
<li><a href="#_scatter_plots">7.1. Scatter plots</a></li>
<li><a href="#characterizing">7.2. Characterizing relationships</a></li>
<li><a href="#_correlation">7.3. Correlation</a></li>
<li><a href="#_covariance">7.4. Covariance</a></li>
<li><a href="#_pearson_s_correlation">7.5. Pearson’s correlation</a></li>
<li><a href="#_nonlinear_relationships">7.6. Nonlinear relationships</a></li>
<li><a href="#_spearman_s_rank_correlation">7.7. Spearman’s rank correlation</a></li>
<li><a href="#_correlation_and_causation">7.8. Correlation and causation</a></li>
<li><a href="#_exercises_7">7.9. Exercises</a></li>
<li><a href="#_glossary_7">7.10. Glossary</a></li>
</ul>
</li>
<li><a href="#estimation">8. Estimation</a>
<ul class="sectlevel2">
<li><a href="#_the_estimation_game">8.1. The estimation game</a></li>
<li><a href="#_guess_the_variance">8.2. Guess the variance</a></li>
<li><a href="#gorilla">8.3. Sampling distributions</a></li>
<li><a href="#_sampling_bias">8.4. Sampling bias</a></li>
<li><a href="#_exponential_distributions">8.5. Exponential distributions</a></li>
<li><a href="#_exercises_8">8.6. Exercises</a></li>
<li><a href="#_glossary_8">8.7. Glossary</a></li>
</ul>
</li>
<li><a href="#testing">9. Hypothesis testing</a>
<ul class="sectlevel2">
<li><a href="#_classical_hypothesis_testing">9.1. Classical hypothesis testing</a></li>
<li><a href="#hypotest">9.2. HypothesisTest</a></li>
<li><a href="#testdiff">9.3. Testing a difference in means</a></li>
<li><a href="#_other_test_statistics">9.4. Other test statistics</a></li>
<li><a href="#corrtest">9.5. Testing a correlation</a></li>
<li><a href="#casino">9.6. Testing proportions</a></li>
<li><a href="#casino2">9.7. Chi-squared tests</a></li>
<li><a href="#_first_babies_again">9.8. First babies again</a></li>
<li><a href="#_errors">9.9. Errors</a></li>
<li><a href="#_power">9.10. Power</a></li>
<li><a href="#_replication">9.11. Replication</a></li>
<li><a href="#_exercises_9">9.12. Exercises</a></li>
<li><a href="#_glossary_9">9.13. Glossary</a></li>
</ul>
</li>
<li><a href="#linear">10. Linear least squares</a>
<ul class="sectlevel2">
<li><a href="#_least_squares_fit">10.1. Least squares fit</a></li>
<li><a href="#_implementation">10.2. Implementation</a></li>
<li><a href="#_residuals">10.3. Residuals</a></li>
<li><a href="#regest">10.4. Estimation</a></li>
<li><a href="#goodness">10.5. Goodness of fit</a></li>
<li><a href="#_testing_a_linear_model">10.6. Testing a linear model</a></li>
<li><a href="#weighted">10.7. Weighted resampling</a></li>
<li><a href="#_exercises_10">10.8. Exercises</a></li>
<li><a href="#_glossary_10">10.9. Glossary</a></li>
</ul>
</li>
<li><a href="#_regression">11. Regression</a>
<ul class="sectlevel2">
<li><a href="#_statsmodels">11.1. StatsModels</a></li>
<li><a href="#multiple">11.2. Multiple regression</a></li>
<li><a href="#nonlinear">11.3. Nonlinear relationships</a></li>
<li><a href="#mining">11.4. Data mining</a></li>
<li><a href="#_prediction">11.5. Prediction</a></li>
<li><a href="#_logistic_regression">11.6. Logistic regression</a></li>
<li><a href="#_estimating_parameters">11.7. Estimating parameters</a></li>
<li><a href="#implementation">11.8. Implementation</a></li>
<li><a href="#_accuracy">11.9. Accuracy</a></li>
<li><a href="#_exercises_11">11.10. Exercises</a></li>
<li><a href="#_glossary_11">11.11. Glossary</a></li>
</ul>
</li>
<li><a href="#_time_series_analysis">12. Time series analysis</a>
<ul class="sectlevel2">
<li><a href="#_importing_and_cleaning">12.1. Importing and cleaning</a></li>
<li><a href="#_plotting">12.2. Plotting</a></li>
<li><a href="#timeregress">12.3. Linear regression</a></li>
<li><a href="#_moving_averages">12.4. Moving averages</a></li>
<li><a href="#_missing_values">12.5. Missing values</a></li>
<li><a href="#_serial_correlation">12.6. Serial correlation</a></li>
<li><a href="#_autocorrelation">12.7. Autocorrelation</a></li>
<li><a href="#_prediction_2">12.8. Prediction</a></li>
<li><a href="#_further_reading">12.9. Further reading</a></li>
<li><a href="#_exercises_12">12.10. Exercises</a></li>
<li><a href="#_glossary_12">12.11. Glossary</a></li>
</ul>
</li>
<li><a href="#_survival_analysis">13. Survival analysis</a>
<ul class="sectlevel2">
<li><a href="#survival">13.1. Survival curves</a></li>
<li><a href="#hazard">13.2. Hazard function</a></li>
<li><a href="#_inferring_survival_curves">13.3. Inferring survival curves</a></li>
<li><a href="#_kaplan_meier_estimation">13.4. Kaplan-Meier estimation</a></li>
<li><a href="#_the_marriage_curve">13.5. The marriage curve</a></li>
<li><a href="#_estimating_the_survival_curve">13.6. Estimating the survival curve</a></li>
<li><a href="#_confidence_intervals">13.7. Confidence intervals</a></li>
<li><a href="#_cohort_effects">13.8. Cohort effects</a></li>
<li><a href="#_extrapolation">13.9. Extrapolation</a></li>
<li><a href="#_expected_remaining_lifetime">13.10. Expected remaining lifetime</a></li>
<li><a href="#_exercises_13">13.11. Exercises</a></li>
<li><a href="#_glossary_13">13.12. Glossary</a></li>
</ul>
</li>
<li><a href="#analysis">14. Analytic methods</a>
<ul class="sectlevel2">
<li><a href="#why_normal">14.1. Normal distributions</a></li>
<li><a href="#sampling-distributions">14.2. Sampling distributions</a></li>
<li><a href="#_representing_normal_distributions">14.3. Representing normal distributions</a></li>
<li><a href="#CLT">14.4. Central limit theorem</a></li>
<li><a href="#_testing_the_clt">14.5. Testing the CLT</a></li>
<li><a href="#usingCLT">14.6. Applying the CLT</a></li>
<li><a href="#_correlation_test">14.7. Correlation test</a></li>
<li><a href="#_chi_squared_test">14.8. Chi-squared test</a></li>
<li><a href="#_discussion">14.9. Discussion</a></li>
<li><a href="#_exercises_14">14.10. Exercises</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div id="content">
<div class="sect1">
<h2 id="_colphon"><a class="anchor" href="#_colphon"></a><a class="link" href="#_colphon">Colphon</a></h2>
<div class="sectionbody">
<div class="paragraph">
<p>Green Tea Press</p>
</div>
<div class="paragraph">
<p>Needham, Massachusetts</p>
</div>
<div class="paragraph">
<p>Copyright © 2014 Allen B. Downey.</p>
</div>
<div class="paragraph">
<p>Green Tea Press<br>
9 Washburn Ave<br>
Needham MA 02492</p>
</div>
<div class="paragraph">
<p>Permission is granted to copy, distribute, and/or modify this document
under the terms of the Creative Commons
Attribution-NonCommercial-ShareAlike 4.0 International License, which is
available at <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/" class="bare">http://creativecommons.org/licenses/by-nc-sa/4.0/</a>.</p>
</div>
<div class="paragraph">
<p>The original form of this book is LaTeX source code. Compiling this code
has the effect of generating a device-independent representation of a
textbook, which can be converted to other formats and printed.</p>
</div>
<div class="paragraph">
<p>The LaTeX source for this book is available from <a href="http://thinkstats2.com" class="bare">http://thinkstats2.com</a>.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_preface"><a class="anchor" href="#_preface"></a><a class="link" href="#_preface">Preface</a></h2>
<div class="sectionbody">
<div class="paragraph">
<p>This book is an introduction to the practical tools of exploratory data
analysis. The organization of the book follows the process I use when I
start working with a dataset:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Importing and cleaning: Whatever format the data is in, it usually
takes some time and effort to read the data, clean and transform it, and
check that everything made it through the translation process intact.</p>
</li>
<li>
<p>Single variable explorations: I usually start by examining one
variable at a time, finding out what the variables mean, looking at
distributions of the values, and choosing appropriate summary
statistics.</p>
</li>
<li>
<p>Pair-wise explorations: To identify possible relationships between
variables, I look at tables and scatter plots, and compute correlations
and linear fits.</p>
</li>
<li>
<p>Multivariate analysis: If there are apparent relationships between
variables, I use multiple regression to add control variables and
investigate more complex relationships.</p>
</li>
<li>
<p>Estimation and hypothesis testing: When reporting statistical results,
it is important to answer three questions: How big is the effect? How
much variability should we expect if we run the same measurement again?
Is it possible that the apparent effect is due to chance?</p>
</li>
<li>
<p>Visualization: During exploration, visualization is an important tool
for finding possible relationships and effects. Then if an apparent
effect holds up to scrutiny, visualization is an effective way to
communicate results.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>This book takes a computational approach, which has several advantages
over mathematical approaches:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>I present most ideas using Python code, rather than mathematical
notation. In general, Python code is more readable; also, because it is
executable, readers can download it, run it, and modify it.</p>
</li>
<li>
<p>Each chapter includes exercises readers can do to develop and solidify
their learning. When you write programs, you express your understanding
in code; while you are debugging the program, you are also correcting
your understanding.</p>
</li>
<li>
<p>Some exercises involve experiments to test statistical behavior. For
example, you can explore the Central Limit Theorem (CLT) by generating
random samples and computing their sums. The resulting visualizations
demonstrate why the CLT works and when it doesn’t.</p>
</li>
<li>
<p>Some ideas that are hard to grasp mathematically are easy to
understand by simulation. For example, we approximate p-values by
running random simulations, which reinforces the meaning of the p-value.</p>
</li>
<li>
<p>Because the book is based on a general-purpose programming language
(Python), readers can import data from almost any source. They are not
limited to datasets that have been cleaned and formatted for a
particular statistics tool.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The book lends itself to a project-based approach. In my class, students
work on a semester-long project that requires them to pose a statistical
question, find a dataset that can address it, and apply each of the
techniques they learn to their own data.</p>
</div>
<div class="paragraph">
<p>To demonstrate my approach to statistical analysis, the book presents a
case study that runs through all of the chapters. It uses data from two
sources:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The National Survey of Family Growth (NSFG), conducted by the U.S.
Centers for Disease Control and Prevention (CDC) to gather &#8220;information
on family life, marriage and divorce, pregnancy, infertility, use of
contraception, and men’s and women’s health.&#8221; (See
<a href="http://cdc.gov/nchs/nsfg.htm." class="bare">http://cdc.gov/nchs/nsfg.htm.</a>)</p>
</li>
<li>
<p>The Behavioral Risk Factor Surveillance System (BRFSS), conducted by
the National Center for Chronic Disease Prevention and Health Promotion
to &#8220;track health conditions and risk behaviors in the United States.&#8221;
(See <a href="http://cdc.gov/BRFSS/." class="bare">http://cdc.gov/BRFSS/.</a>)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Other examples use data from the IRS, the U.S. Census, and the Boston
Marathon.</p>
</div>
<div class="paragraph">
<p>This second edition of <em>Think Stats</em> includes the chapters from the
first edition, many of them substantially revised, and new chapters on
regression, time series analysis, survival analysis, and analytic
methods. The previous edition did not use pandas, SciPy, or StatsModels,
so all of that material is new.</p>
</div>
<div class="sect2">
<h3 id="_how_i_wrote_this_book"><a class="anchor" href="#_how_i_wrote_this_book"></a><a class="link" href="#_how_i_wrote_this_book">How I wrote this book</a></h3>
<div class="paragraph">
<p>When people write a new textbook, they usually start by reading a stack
of old textbooks. As a result, most books contain the same material in
pretty much the same order.</p>
</div>
<div class="paragraph">
<p>I did not do that. In fact, I used almost no printed material while I
was writing this book, for several reasons:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>My goal was to explore a new approach to this material, so I didn’t
want much exposure to existing approaches.</p>
</li>
<li>
<p>Since I am making this book available under a free license, I wanted
to make sure that no part of it was encumbered by copyright
restrictions.</p>
</li>
<li>
<p>Many readers of my books don’t have access to libraries of printed
material, so I tried to make references to resources that are freely
available on the Internet.</p>
</li>
<li>
<p>Some proponents of old media think that the exclusive use of
electronic resources is lazy and unreliable. They might be right about
the first part, but I think they are wrong about the second, so I wanted
to test my theory.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The resource I used more than any other is Wikipedia. In general, the
articles I read on statistical topics were very good (although I made a
few small changes along the way). I include references to Wikipedia
pages throughout the book and I encourage you to follow those links; in
many cases, the Wikipedia page picks up where my description leaves off.
The vocabulary and notation in this book are generally consistent with
Wikipedia, unless I had a good reason to deviate. Other resources I
found useful were Wolfram MathWorld and the Reddit statistics forum,
<a href="http://www.reddit.com/r/statistics" class="bare">http://www.reddit.com/r/statistics</a>.</p>
</div>
</div>
<div class="sect2">
<h3 id="code"><a class="anchor" href="#code"></a><a class="link" href="#code">Using the code</a></h3>
<div class="paragraph">
<p>The code and data used in this book are available from
<a href="https://github.com/AllenDowney/ThinkStats2" class="bare">https://github.com/AllenDowney/ThinkStats2</a>. Git is a version control
system that allows you to keep track of the files that make up a
project. A collection of files under Git’s control is called a
<strong>repository</strong>. GitHub is a hosting service that provides storage for Git
repositories and a convenient web interface.</p>
</div>
<div class="paragraph">
<p>The GitHub homepage for my repository provides several ways to work with
the code:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>You can create a copy of my repository on GitHub by pressing the Fork
button. If you don’t already have a GitHub account, you’ll need to
create one. After forking, you’ll have your own repository on GitHub
that you can use to keep track of code you write while working on this
book. Then you can clone the repo, which means that you make a copy of
the files on your computer.</p>
</li>
<li>
<p>Or you could clone my repository. You don’t need a GitHub account to
do this, but you won’t be able to write your changes back to GitHub.</p>
</li>
<li>
<p>If you don’t want to use Git at all, you can download the files in a
Zip file using the button in the lower-right corner of the GitHub page.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>All of the code is written to work in both Python 2 and Python 3 with no
translation.</p>
</div>
<div class="paragraph">
<p>I developed this book using Anaconda from Continuum Analytics, which is
a free Python distribution that includes all the packages you’ll need to
run the code (and lots more). I found Anaconda easy to install. By
default it does a user-level installation, not system-level, so you
don’t need administrative privileges. And it supports both Python 2 and
Python 3. You can download Anaconda from <a href="http://continuum.io/downloads" class="bare">http://continuum.io/downloads</a>.</p>
</div>
<div class="paragraph">
<p>If you don’t want to use Anaconda, you will need the following packages:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>pandas for representing and analyzing data, <a href="http://pandas.pydata.org/" class="bare">http://pandas.pydata.org/</a>;</p>
</li>
<li>
<p>NumPy for basic numerical computation, <a href="http://www.numpy.org/" class="bare">http://www.numpy.org/</a>;</p>
</li>
<li>
<p>SciPy for scientific computation including statistics,
<a href="http://www.scipy.org/" class="bare">http://www.scipy.org/</a>;</p>
</li>
<li>
<p>StatsModels for regression and other statistical analysis,
<a href="http://statsmodels.sourceforge.net/" class="bare">http://statsmodels.sourceforge.net/</a>; and</p>
</li>
<li>
<p>matplotlib for visualization, <a href="http://matplotlib.org/" class="bare">http://matplotlib.org/</a>.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Although these are commonly used packages, they are not included with
all Python installations, and they can be hard to install in some
environments. If you have trouble installing them, I strongly recommend
using Anaconda or one of the other Python distributions that include
these packages.</p>
</div>
<div class="paragraph">
<p>After you clone the repository or unzip the zip file, you should have a
folder called ThinkStats2/code with a file called nsfg.py. If you run
nsfg.py, it should read a data file, run some tests, and print a message
like, &#8220;All tests passed.&#8221; If you get import errors, it probably means
there are packages you need to install.</p>
</div>
<div class="paragraph">
<p>Most exercises use Python scripts, but some also use the IPython
notebook. If you have not used IPython notebook before, I suggest you
start with the documentation at
<a href="http://ipython.org/ipython-doc/stable/notebook/notebook.html" class="bare">http://ipython.org/ipython-doc/stable/notebook/notebook.html</a>.</p>
</div>
<div class="paragraph">
<p>I wrote this book assuming that the reader is familiar with core Python,
including object-oriented features, but not pandas, NumPy, and SciPy. If
you are already familiar with these modules, you can skip a few
sections.</p>
</div>
<div class="paragraph">
<p>I assume that the reader knows basic mathematics, including logarithms,
for example, and summations. I refer to calculus concepts in a few
places, but you don’t have to do any calculus.</p>
</div>
<div class="paragraph">
<p>If you have never studied statistics, I think this book is a good place
to start. And if you have taken a traditional statistics class, I hope
this book will help repair the damage.</p>
</div>
<div class="paragraph">
<p>—</p>
</div>
<div class="paragraph">
<p>Allen B. Downey is a Professor of Computer Science at the Franklin W.
Olin College of Engineering in Needham, MA.</p>
</div>
</div>
<div class="sect2">
<h3 id="_contributor_list"><a class="anchor" href="#_contributor_list"></a><a class="link" href="#_contributor_list">Contributor List</a></h3>
<div class="paragraph">
<p>If you have a suggestion or correction, please send email to
<a href="mailto:downey@allendowney.com">downey@allendowney.com</a>. If I make a change based on your feedback, I
will add you to the contributor list (unless you ask to be omitted).</p>
</div>
<div class="paragraph">
<p>If you include at least part of the sentence the error appears in, that
makes it easy for me to search. Page and section numbers are fine, too,
but not quite as easy to work with. Thanks!</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Lisa Downey and June Downey read an early draft and made many
corrections and suggestions.</p>
</li>
<li>
<p>Steven Zhang found several errors.</p>
</li>
<li>
<p>Andy Pethan and Molly Farison helped debug some of the solutions, and
Molly spotted several typos.</p>
</li>
<li>
<p>Dr. Nikolas Akerblom knows how big a Hyracotherium is.</p>
</li>
<li>
<p>Alex Morrow clarified one of the code examples.</p>
</li>
<li>
<p>Jonathan Street caught an error in the nick of time.</p>
</li>
<li>
<p>Many thanks to Kevin Smith and Tim Arnold for their work on plasTeX,
which I used to convert this book to DocBook.</p>
</li>
<li>
<p>George Caplan sent several suggestions for improving clarity.</p>
</li>
<li>
<p>Julian Ceipek found an error and a number of typos.</p>
</li>
<li>
<p>Stijn Debrouwere, Leo Marihart III, Jonathan Hammler, and Kent Johnson
found errors in the first print edition.</p>
</li>
<li>
<p>Jörg Beyer found typos in the book and made many corrections in the
docstrings of the accompanying code.</p>
</li>
<li>
<p>Tommie Gannert sent a patch file with a number of corrections.</p>
</li>
<li>
<p>Christoph Lendenmann submitted several errata.</p>
</li>
<li>
<p>Michael Kearney sent me many excellent suggestions.</p>
</li>
<li>
<p>Alex Birch made a number of helpful suggestions.</p>
</li>
<li>
<p>Lindsey Vanderlyn, Griffin Tschurwald, and Ben Small read an early
version of this book and found many errors.</p>
</li>
<li>
<p>John Roth, Carol Willing, and Carol Novitsky performed technical
reviews of the book. They found many errors and made many helpful
suggestions.</p>
</li>
<li>
<p>David Palmer sent many helpful suggestions and corrections.</p>
</li>
<li>
<p>Erik Kulyk found many typos.</p>
</li>
<li>
<p>Nir Soffer sent several excellent pull requests for both the book and
the supporting code.</p>
</li>
<li>
<p>GitHub user flothesof sent a number of corrections.</p>
</li>
<li>
<p>Toshiaki Kurokawa, who is working on the Japanese translation of this
book, has sent many corrections and helpful suggestions.</p>
</li>
<li>
<p>Benjamin White suggested more idiomatic Pandas code.</p>
</li>
<li>
<p>Takashi Sato spotted an code error.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Other people who found typos and similar errors are Andrew Heine, Gábor
Lipták, Dan Kearney, Alexander Gryzlov, Martin Veillette, Haitao Ma,
Jeff Pickhardt, Rohit Deshpande, Joanne Pratt, Lucian Ursu, Paul Glezen,
Ting-kuang Lin, Scott Miller, Luigi Patruno.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="intro"><a class="anchor" href="#intro"></a><a class="link" href="#intro">1. Exploratory data analysis</a></h2>
<div class="sectionbody">
<div class="paragraph">
<p>The thesis of this book is that data combined with practical methods can
answer questions and guide decisions under uncertainty.</p>
</div>
<div class="paragraph">
<p>As an example, I present a case study motivated by a question I heard
when my wife and I were expecting our first child: do first babies tend
to arrive late?</p>
</div>
<div class="paragraph">
<p>If you Google this question, you will find plenty of discussion. Some
people claim it’s true, others say it’s a myth, and some people say it’s
the other way around: first babies come early.</p>
</div>
<div class="paragraph">
<p>In many of these discussions, people provide data to support their
claims. I found many examples like these:</p>
</div>
<div class="quoteblock">
<blockquote>
<div class="paragraph">
<p>My two friends that have given birth recently to their first babies,
BOTH went almost 2 weeks overdue before going into labour or being
induced.</p>
</div>
<div class="paragraph">
<p>My first one came 2 weeks late and now I think the second one is going
to come out two weeks early!!</p>
</div>
<div class="paragraph">
<p>I don’t think that can be true because my sister was my mother’s first
and she was early, as with many of my cousins.</p>
</div>
</blockquote>
</div>
<div class="paragraph">
<p>Reports like these are called <strong>anecdotal evidence</strong> because they are
based on data that is unpublished and usually personal. In casual
conversation, there is nothing wrong with anecdotes, so I don’t mean to
pick on the people I quoted.</p>
</div>
<div class="paragraph">
<p>But we might want evidence that is more persuasive and an answer that is
more reliable. By those standards, anecdotal evidence usually fails,
because:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Small number of observations: If pregnancy length is longer for first
babies, the difference is probably small compared to natural variation.
In that case, we might have to compare a large number of pregnancies to
be sure that a difference exists.</p>
</li>
<li>
<p>Selection bias: People who join a discussion of this question might be
interested because their first babies were late. In that case the
process of selecting data would bias the results.</p>
</li>
<li>
<p>Confirmation bias: People who believe the claim might be more likely
to contribute examples that confirm it. People who doubt the claim are
more likely to cite counterexamples.</p>
</li>
<li>
<p>Inaccuracy: Anecdotes are often personal stories, and often
misremembered, misrepresented, repeated inaccurately, etc.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>So how can we do better?</p>
</div>
<div class="sect2">
<h3 id="_a_statistical_approach"><a class="anchor" href="#_a_statistical_approach"></a><a class="link" href="#_a_statistical_approach">1.1. A statistical approach</a></h3>
<div class="paragraph">
<p>To address the limitations of anecdotes, we will use the tools of
statistics, which include:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Data collection: We will use data from a large national survey that
was designed explicitly with the goal of generating statistically valid
inferences about the U.S. population.</p>
</li>
<li>
<p>Descriptive statistics: We will generate statistics that summarize the
data concisely, and evaluate different ways to visualize data.</p>
</li>
<li>
<p>Exploratory data analysis: We will look for patterns, differences, and
other features that address the questions we are interested in. At the
same time we will check for inconsistencies and identify limitations.</p>
</li>
<li>
<p>Estimation: We will use data from a sample to estimate characteristics
of the general population.</p>
</li>
<li>
<p>Hypothesis testing: Where we see apparent effects, like a difference
between two groups, we will evaluate whether the effect might have
happened by chance.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>By performing these steps with care to avoid pitfalls, we can reach
conclusions that are more justifiable and more likely to be correct.</p>
</div>
</div>
<div class="sect2">
<h3 id="nsfg"><a class="anchor" href="#nsfg"></a><a class="link" href="#nsfg">1.2. The National Survey of Family Growth</a></h3>
<div class="paragraph">
<p>Since 1973 the U.S. Centers for Disease Control and Prevention (CDC)
have conducted the National Survey of Family Growth (NSFG), which is
intended to gather &#8220;information on family life, marriage and divorce,
pregnancy, infertility, use of contraception, and men’s and women’s
health. The survey results are used &#8230;&#8203; to plan health services and
health education programs, and to do statistical studies of families,
fertility, and health.&#8221; See <a href="http://cdc.gov/nchs/nsfg.htm" class="bare">http://cdc.gov/nchs/nsfg.htm</a>.</p>
</div>
<div class="paragraph">
<p>We will use data collected by this survey to investigate whether first
babies tend to come late, and other questions. In order to use this data
effectively, we have to understand the design of the study.</p>
</div>
<div class="paragraph">
<p>The NSFG is a <strong>cross-sectional</strong> study, which means that it captures a
snapshot of a group at a point in time. The most common alternative is a
<strong>longitudinal</strong> study, which observes a group repeatedly over a period of
time.</p>
</div>
<div class="paragraph">
<p>The NSFG has been conducted seven times; each deployment is called a
<strong>cycle</strong>. We will use data from Cycle 6, which was conducted from January
2002 to March 2003.</p>
</div>
<div class="paragraph">
<p>The goal of the survey is to draw conclusions about a <strong>population</strong>; the
target population of the NSFG is people in the United States aged 15-44.
Ideally surveys would collect data from every member of the population,
but that’s seldom possible. Instead we collect data from a subset of the
population called a <strong>sample</strong>. The people who participate in a survey are
called <strong>respondents</strong>.</p>
</div>
<div class="paragraph">
<p>In general, cross-sectional studies are meant to be <strong>representative</strong>,
which means that every member of the target population has an equal
chance of participating. That ideal is hard to achieve in practice, but
people who conduct surveys come as close as they can.</p>
</div>
<div class="paragraph">
<p>The NSFG is not representative; instead it is deliberately
<strong>oversampled</strong>. The designers of the study recruited three
groups—Hispanics, African-Americans and teenagers—at rates higher than
their representation in the U.S. population, in order to make sure that
the number of respondents in each of these groups is large enough to
draw valid statistical inferences.</p>
</div>
<div class="paragraph">
<p>Of course, the drawback of oversampling is that it is not as easy to
draw conclusions about the general population based on statistics from
the survey. We will come back to this point later.</p>
</div>
<div class="paragraph">
<p>When working with this kind of data, it is important to be familiar with
the <strong>codebook</strong>, which documents the design of the study, the survey
questions, and the encoding of the responses. The codebook and user’s
guide for the NSFG data are available from
<a href="http://www.cdc.gov/nchs/nsfg/nsfg_cycle6.htm" class="bare">http://www.cdc.gov/nchs/nsfg/nsfg_cycle6.htm</a></p>
</div>
</div>
<div class="sect2">
<h3 id="_importing_the_data"><a class="anchor" href="#_importing_the_data"></a><a class="link" href="#_importing_the_data">1.3. Importing the data</a></h3>
<div class="paragraph">
<p>The code and data used in this book are available from
<a href="https://github.com/AllenDowney/ThinkStats2" class="bare">https://github.com/AllenDowney/ThinkStats2</a>. For information about
downloading and working with this code, see Section <a href="#code">Using the code</a>.</p>
</div>
<div class="paragraph">
<p>Once you download the code, you should have a file called
ThinkStats2/code/nsfg.py. If you run it, it should read a data file, run
some tests, and print a message like, &#8220;All tests passed.&#8221;</p>
</div>
<div class="paragraph">
<p>Let’s see what it does. Pregnancy data from Cycle 6 of the NSFG is in a
file called 2002FemPreg.dat.gz; it is a gzip-compressed data file in
plain text (ASCII), with fixed width columns. Each line in the file is a
<strong>record</strong> that contains data about one pregnancy.</p>
</div>
<div class="paragraph">
<p>The format of the file is documented in 2002FemPreg.dct, which is a
Stata dictionary file. Stata is a statistical software system; a
&#8220;dictionary&#8221; in this context is a list of variable names, types, and
indices that identify where in each line to find each variable.</p>
</div>
<div class="paragraph">
<p>For example, here are a few lines from 2002FemPreg.dct:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>infile dictionary {
  _column(1)  str12  caseid    %12s  "RESPONDENT ID NUMBER"
  _column(13) byte   pregordr   %2f  "PREGNANCY ORDER (NUMBER)"
}</pre>
</div>
</div>
<div class="paragraph">
<p>This dictionary describes two variables: caseid is a 12-character string
that represents the respondent ID; pregordr is a one-byte integer that
indicates which pregnancy this record describes for this respondent.</p>
</div>
<div class="paragraph">
<p>The code you downloaded includes thinkstats2.py, which is a Python
module that contains many classes and functions used in this book,
including functions that read the Stata dictionary and the NSFG data
file. Here’s how they are used in nsfg.py:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def ReadFemPreg(dct_file='2002FemPreg.dct',
                dat_file='2002FemPreg.dat.gz'):
    dct = thinkstats2.ReadStataDct(dct_file)
    df = dct.ReadFixedWidth(dat_file, compression='gzip')
    CleanFemPreg(df)
    return df</code></pre>
</div>
</div>
<div class="paragraph">
<p>ReadStataDct takes the name of the dictionary file and returns dct, a
FixedWidthVariables object that contains the information from the
dictionary file. dct provides ReadFixedWidth, which reads the data file.</p>
</div>
</div>
<div class="sect2">
<h3 id="dataframe"><a class="anchor" href="#dataframe"></a><a class="link" href="#dataframe">1.4. DataFrames</a></h3>
<div class="paragraph">
<p>The result of ReadFixedWidth is a DataFrame, which is the fundamental
data structure provided by pandas, which is a Python data and statistics
package we’ll use throughout this book. A DataFrame contains a row for
each record, in this case one row per pregnancy, and a column for each
variable.</p>
</div>
<div class="paragraph">
<p>In addition to the data, a DataFrame also contains the variable names
and their types, and it provides methods for accessing and modifying the
data.</p>
</div>
<div class="paragraph">
<p>If you print df you get a truncated view of the rows and columns, and
the shape of the DataFrame, which is 13593 rows/records and 244
columns/variables.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; import nsfg
&gt;&gt;&gt; df = nsfg.ReadFemPreg()
&gt;&gt;&gt; df
...
[13593 rows x 244 columns]</pre>
</div>
</div>
<div class="paragraph">
<p>The DataFrame is too big to display, so the output is truncated. The
last line reports the number of rows and columns.</p>
</div>
<div class="paragraph">
<p>The attribute columns returns a sequence of column names as Unicode
strings:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; df.columns
Index([u'caseid', u'pregordr', u'howpreg_n', u'howpreg_p', ... ])</pre>
</div>
</div>
<div class="paragraph">
<p>The result is an Index, which is another pandas data structure. We’ll
learn more about Index later, but for now we’ll treat it like a list:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; df.columns[1]
'pregordr'</pre>
</div>
</div>
<div class="paragraph">
<p>To access a column from a DataFrame, you can use the column name as a
key:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; pregordr = df['pregordr']
&gt;&gt;&gt; type(pregordr)
&lt;class 'pandas.core.series.Series'&gt;</pre>
</div>
</div>
<div class="paragraph">
<p>The result is a Series, yet another pandas data structure. A Series is
like a Python list with some additional features. When you print a
Series, you get the indices and the corresponding values:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; pregordr
0     1
1     2
2     1
3     2
...
13590    3
13591    4
13592    5
Name: pregordr, Length: 13593, dtype: int64</pre>
</div>
</div>
<div class="paragraph">
<p>In this example the indices are integers from 0 to 13592, but in general
they can be any sortable type. The elements are also integers, but they
can be any type.</p>
</div>
<div class="paragraph">
<p>The last line includes the variable name, Series length, and data type;
int64 is one of the types provided by NumPy. If you run this example on
a 32-bit machine you might see int32.</p>
</div>
<div class="paragraph">
<p>You can access the elements of a Series using integer indices and
slices:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; pregordr[0]
1
&gt;&gt;&gt; pregordr[2:5]
2    1
3    2
4    3
Name: pregordr, dtype: int64</pre>
</div>
</div>
<div class="paragraph">
<p>The result of the index operator is an int64; the result of the slice is
another Series.</p>
</div>
<div class="paragraph">
<p>You can also access the columns of a DataFrame using dot notation:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; pregordr = df.pregordr</pre>
</div>
</div>
<div class="paragraph">
<p>This notation only works if the column name is a valid Python
identifier, so it has to begin with a letter, can’t contain spaces, etc.</p>
</div>
</div>
<div class="sect2">
<h3 id="_variables"><a class="anchor" href="#_variables"></a><a class="link" href="#_variables">1.5. Variables</a></h3>
<div class="paragraph">
<p>We have already seen two variables in the NSFG dataset, caseid and
pregordr, and we have seen that there are 244 variables in total. For
the explorations in this book, I use the following variables:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>caseid is the integer ID of the respondent.</p>
</li>
<li>
<p>prglngth is the integer duration of the pregnancy in weeks.</p>
</li>
<li>
<p>outcome is an integer code for the outcome of the pregnancy. The code
1 indicates a live birth.</p>
</li>
<li>
<p>pregordr is a pregnancy serial number; for example, the code for a
respondent’s first pregnancy is 1, for the second pregnancy is 2, and so
on.</p>
</li>
<li>
<p>birthord is a serial number for live births; the code for a
respondent’s first child is 1, and so on. For outcomes other than live
birth, this field is blank.</p>
</li>
<li>
<p><code>birthwgt_lb</code> and <code>birthwgt_oz</code> contain the pounds and ounces
parts of the birth weight of the baby.</p>
</li>
<li>
<p>agepreg is the mother’s age at the end of the pregnancy.</p>
</li>
<li>
<p>finalwgt is the statistical weight associated with the respondent. It
is a floating-point value that indicates the number of people in the
U.S. population this respondent represents.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>If you read the codebook carefully, you will see that many of the
variables are <strong>recodes</strong>, which means that they are not part of the <strong>raw
data</strong> collected by the survey; they are calculated using the raw data.</p>
</div>
<div class="paragraph">
<p>For example, prglngth for live births is equal to the raw variable
wksgest (weeks of gestation) if it is available; otherwise it is
estimated using mosgest * 4.33 (months of gestation times the average
number of weeks in a month).</p>
</div>
<div class="paragraph">
<p>Recodes are often based on logic that checks the consistency and
accuracy of the data. In general it is a good idea to use recodes when
they are available, unless there is a compelling reason to process the
raw data yourself.</p>
</div>
</div>
<div class="sect2">
<h3 id="cleaning"><a class="anchor" href="#cleaning"></a><a class="link" href="#cleaning">1.6. Transformation</a></h3>
<div class="paragraph">
<p>When you import data like this, you often have to check for errors, deal
with special values, convert data into different formats, and perform
calculations. These operations are called <strong>data cleaning</strong>.</p>
</div>
<div class="paragraph">
<p>nsfg.py includes CleanFemPreg, a function that cleans the variables I am
planning to use.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def CleanFemPreg(df):
    df.agepreg /= 100.0

    na_vals = [97, 98, 99]
    df.birthwgt_lb.replace(na_vals, np.nan, inplace=True)
    df.birthwgt_oz.replace(na_vals, np.nan, inplace=True)

    df['totalwgt_lb'] = df.birthwgt_lb + df.birthwgt_oz / 16.0</code></pre>
</div>
</div>
<div class="paragraph">
<p>agepreg contains the mother’s age at the end of the pregnancy. In the
data file, agepreg is encoded as an integer number of centiyears. So the
first line divides each element of agepreg by 100, yielding a
floating-point value in years.</p>
</div>
<div class="paragraph">
<p><code>birthwgt_lb</code> and <code>birthwgt_oz</code> contain the weight of the baby, in
pounds and ounces, for pregnancies that end in live birth. In addition
it uses several special codes:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>97      NOT ASCERTAINED
98      REFUSED
99      DON'T KNOW</pre>
</div>
</div>
<div class="paragraph">
<p>Special values encoded as numbers are <em>dangerous</em> because if they are
not handled properly, they can generate bogus results, like a 99-pound
baby. The replace method replaces these values with np.nan, a special
floating-point value that represents &#8220;not a number.&#8221; The inplace flag
tells replace to modify the existing Series rather than create a new
one.</p>
</div>
<div class="paragraph">
<p>As part of the IEEE floating-point standard, all mathematical operations
return nan if either argument is nan:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; np.nan / 100.0
nan</pre>
</div>
</div>
<div class="paragraph">
<p>So computations with nan tend to do the right thing, and most pandas
functions handle nan appropriately. But dealing with missing data will
be a recurring issue.</p>
</div>
<div class="paragraph">
<p>The last line of CleanFemPreg creates a new column <code>totalwgt_lb</code> that
combines pounds and ounces into a single quantity, in pounds.</p>
</div>
<div class="paragraph">
<p>One important note: when you add a new column to a DataFrame, you must
use dictionary syntax, like this</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    # CORRECT
    df['totalwgt_lb'] = df.birthwgt_lb + df.birthwgt_oz / 16.0</code></pre>
</div>
</div>
<div class="paragraph">
<p>Not dot notation, like this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    # WRONG!
    df.totalwgt_lb = df.birthwgt_lb + df.birthwgt_oz / 16.0</code></pre>
</div>
</div>
<div class="paragraph">
<p>The version with dot notation adds an attribute to the DataFrame object,
but that attribute is not treated as a new column.</p>
</div>
</div>
<div class="sect2">
<h3 id="_validation"><a class="anchor" href="#_validation"></a><a class="link" href="#_validation">1.7. Validation</a></h3>
<div class="paragraph">
<p>When data is exported from one software environment and imported into
another, errors might be introduced. And when you are getting familiar
with a new dataset, you might interpret data incorrectly or introduce
other misunderstandings. If you take time to validate the data, you can
save time later and avoid errors.</p>
</div>
<div class="paragraph">
<p>One way to validate data is to compute basic statistics and compare them
with published results. For example, the NSFG codebook includes tables
that summarize each variable. Here is the table for outcome, which
encodes the outcome of each pregnancy:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>value   label                  Total
1       LIVE BIRTH              9148
2       INDUCED ABORTION        1862
3       STILLBIRTH               120
4       MISCARRIAGE             1921
5       ECTOPIC PREGNANCY        190
6       CURRENT PREGNANCY        352</pre>
</div>
</div>
<div class="paragraph">
<p>The Series class provides a method, <code>value_counts</code>, that counts the
number of times each value appears. If we select the outcome Series from
the DataFrame, we can use <code>value_counts</code> to compare with the published
data:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; df.outcome.value_counts().sort_index()
1    9148
2    1862
3     120
4    1921
5     190
6     352</pre>
</div>
</div>
<div class="paragraph">
<p>The result of <code>value_counts</code> is a Series; <code>sort_index()</code> sorts the
Series by index, so the values appear in order.</p>
</div>
<div class="paragraph">
<p>Comparing the results with the published table, it looks like the values
in outcome are correct. Similarly, here is the published table for
<code>birthwgt_lb</code></p>
</div>
<div class="literalblock">
<div class="content">
<pre>value   label                  Total
.       INAPPLICABLE            4449
0-5     UNDER 6 POUNDS          1125
6       6 POUNDS                2223
7       7 POUNDS                3049
8       8 POUNDS                1889
9-95    9 POUNDS OR MORE         799</pre>
</div>
</div>
<div class="paragraph">
<p>And here are the value counts:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; df.birthwgt_lb.value_counts(sort=False)
0        8
1       40
2       53
3       98
4      229
5      697
6     2223
7     3049
8     1889
9      623
10     132
11      26
12      10
13       3
14       3
15       1
51       1</pre>
</div>
</div>
<div class="paragraph">
<p>The counts for 6, 7, and 8 pounds check out, and if you add up the
counts for 0-5 and 9-95, they check out, too. But if you look more
closely, you will notice one value that has to be an error, a 51 pound
baby!</p>
</div>
<div class="paragraph">
<p>To deal with this error, I added a line to CleanFemPreg:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">df.loc[df.birthwgt_lb &gt; 20, 'birthwgt_lb'] = np.nan</code></pre>
</div>
</div>
<div class="paragraph">
<p>This statement replaces invalid values with np.nan. The attribute loc
provides several ways to select rows and columns from a DataFrame. In
this example, the first expression in brackets is the row indexer; the
second expression selects the column.</p>
</div>
<div class="paragraph">
<p>The expression <code>df.birthwgt_lb &gt; 20</code> yields a Series of type bool,
where True indicates that the condition is true. When a boolean Series
is used as an index, it selects only the elements that satisfy the
condition.</p>
</div>
</div>
<div class="sect2">
<h3 id="_interpretation"><a class="anchor" href="#_interpretation"></a><a class="link" href="#_interpretation">1.8. Interpretation</a></h3>
<div class="paragraph">
<p>To work with data effectively, you have to think on two levels at the
same time: the level of statistics and the level of context.</p>
</div>
<div class="paragraph">
<p>As an example, let’s look at the sequence of outcomes for a few
respondents. Because of the way the data files are organized, we have to
do some processing to collect the pregnancy data for each respondent.
Here’s a function that does that:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def MakePregMap(df):
    d = defaultdict(list)
    for index, caseid in df.caseid.iteritems():
        d[caseid].append(index)
    return d</code></pre>
</div>
</div>
<div class="paragraph">
<p>df is the DataFrame with pregnancy data. The iteritems method enumerates
the index (row number) and caseid for each pregnancy.</p>
</div>
<div class="paragraph">
<p>d is a dictionary that maps from each case ID to a list of indices. If
you are not familiar with defaultdict, it is in the Python collections
module. Using d, we can look up a respondent and get the indices of that
respondent’s pregnancies.</p>
</div>
<div class="paragraph">
<p>This example looks up one respondent and prints a list of outcomes for
her pregnancies:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; caseid = 10229
&gt;&gt;&gt; preg_map = nsfg.MakePregMap(df)
&gt;&gt;&gt; indices = preg_map[caseid]
&gt;&gt;&gt; df.outcome[indices].values
[4 4 4 4 4 4 1]</pre>
</div>
</div>
<div class="paragraph">
<p>indices is the list of indices for pregnancies corresponding to
respondent 10229.</p>
</div>
<div class="paragraph">
<p>Using this list as an index into df.outcome selects the indicated rows
and yields a Series. Instead of printing the whole Series, I selected
the values attribute, which is a NumPy array.</p>
</div>
<div class="paragraph">
<p>The outcome code 1 indicates a live birth. Code 4 indicates a
miscarriage; that is, a pregnancy that ended spontaneously, usually with
no known medical cause.</p>
</div>
<div class="paragraph">
<p>Statistically this respondent is not unusual. Miscarriages are common
and there are other respondents who reported as many or more.</p>
</div>
<div class="paragraph">
<p>But remembering the context, this data tells the story of a woman who
was pregnant six times, each time ending in miscarriage. Her seventh and
most recent pregnancy ended in a live birth. If we consider this data
with empathy, it is natural to be moved by the story it tells.</p>
</div>
<div class="paragraph">
<p>Each record in the NSFG dataset represents a person who provided honest
answers to many personal and difficult questions. We can use this data
to answer statistical questions about family life, reproduction, and
health. At the same time, we have an obligation to consider the people
represented by the data, and to afford them respect and gratitude.</p>
</div>
</div>
<div class="sect2">
<h3 id="_exercises"><a class="anchor" href="#_exercises"></a><a class="link" href="#_exercises">1.9. Exercises</a></h3>
<div class="paragraph">
<p>In the repository you downloaded, you should find a file named
<code>chap01ex.ipynb</code>, which is an IPython notebook. You can launch IPython
notebook from the command line like this:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>$ ipython notebook &amp;</pre>
</div>
</div>
<div class="paragraph">
<p>If IPython is installed, it should launch a server that runs in the
background and open a browser to view the notebook. If you are not
familiar with IPython, I suggest you start at
<a href="http://ipython.org/ipython-doc/stable/notebook/notebook.html" class="bare">http://ipython.org/ipython-doc/stable/notebook/notebook.html</a>.</p>
</div>
<div class="paragraph">
<p>To launch the IPython notebook server, run:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>$ ipython notebook &amp;</pre>
</div>
</div>
<div class="paragraph">
<p>It should open a new browser window, but if not, the startup message
provides a URL you can load in a browser, usually <a href="http://localhost:8888" class="bare">http://localhost:8888</a>.
The new window should list the notebooks in the repository.</p>
</div>
<div class="paragraph">
<p>Open <code>chap01ex.ipynb</code>. Some cells are already filled in, and you
should execute them. Other cells give you instructions for exercises you
should try.</p>
</div>
<div class="paragraph">
<p>A solution to this exercise is in <code>chap01soln.ipynb</code></p>
</div>
<div class="paragraph">
<p>In the repository you downloaded, you should find a file named
<code>chap01ex.py</code>; using this file as a starting place, write a function
that reads the respondent file, 2002FemResp.dat.gz.</p>
</div>
<div class="paragraph">
<p>The variable pregnum is a recode that indicates how many times each
respondent has been pregnant. Print the value counts for this variable
and compare them to the published results in the NSFG codebook.</p>
</div>
<div class="paragraph">
<p>You can also cross-validate the respondent and pregnancy files by
comparing pregnum for each respondent with the number of records in the
pregnancy file.</p>
</div>
<div class="paragraph">
<p>You can use nsfg.MakePregMap to make a dictionary that maps from each
caseid to a list of indices into the pregnancy DataFrame.</p>
</div>
<div class="paragraph">
<p>A solution to this exercise is in <code>chap01soln.py</code></p>
</div>
<div class="paragraph">
<p>The best way to learn about statistics is to work on a project you are
interested in. Is there a question like, &#8220;Do first babies arrive
late,&#8221; that you want to investigate?</p>
</div>
<div class="paragraph">
<p>Think about questions you find personally interesting, or items of
conventional wisdom, or controversial topics, or questions that have
political consequences, and see if you can formulate a question that
lends itself to statistical inquiry.</p>
</div>
<div class="paragraph">
<p>Look for data to help you address the question. Governments are good
sources because data from public research is often freely available.
Good places to start include <a href="http://www.data.gov/" class="bare">http://www.data.gov/</a>, and
<a href="http://www.science.gov/" class="bare">http://www.science.gov/</a>, and in the United Kingdom, <a href="http://data.gov.uk/" class="bare">http://data.gov.uk/</a>.</p>
</div>
<div class="paragraph">
<p>Two of my favorite data sets are the General Social Survey at
<a href="http://www3.norc.org/gss+website/" class="bare">http://www3.norc.org/gss+website/</a>, and the European Social Survey at
<a href="http://www.europeansocialsurvey.org/" class="bare">http://www.europeansocialsurvey.org/</a>.</p>
</div>
<div class="paragraph">
<p>If it seems like someone has already answered your question, look
closely to see whether the answer is justified. There might be flaws in
the data or the analysis that make the conclusion unreliable. In that
case you could perform a different analysis of the same data, or look
for a better source of data.</p>
</div>
<div class="paragraph">
<p>If you find a published paper that addresses your question, you should
be able to get the raw data. Many authors make their data available on
the web, but for sensitive data you might have to write to the authors,
provide information about how you plan to use the data, or agree to
certain terms of use. Be persistent!</p>
</div>
</div>
<div class="sect2">
<h3 id="_glossary"><a class="anchor" href="#_glossary"></a><a class="link" href="#_glossary">1.10. Glossary</a></h3>
<div class="ulist">
<ul>
<li>
<p><strong>anecdotal evidence</strong>: Evidence, often personal, that is collected
casually rather than by a well-designed study.</p>
</li>
<li>
<p><strong>population</strong>: A group we are interested in studying. &#8220;Population&#8221;
often refers to a group of people, but the term is used for other
subjects, too.</p>
</li>
<li>
<p><strong>cross-sectional study</strong>: A study that collects data about a population
at a particular point in time.</p>
</li>
<li>
<p><strong>cycle</strong>: In a repeated cross-sectional study, each repetition of the
study is called a cycle.</p>
</li>
<li>
<p><strong>longitudinal study</strong>: A study that follows a population over time,
collecting data from the same group repeatedly.</p>
</li>
<li>
<p><strong>record</strong>: In a dataset, a collection of information about a single
person or other subject.</p>
</li>
<li>
<p><strong>respondent</strong>: A person who responds to a survey.</p>
</li>
<li>
<p><strong>sample</strong>: The subset of a population used to collect data.</p>
</li>
<li>
<p><strong>representative</strong>: A sample is representative if every member of the
population has the same chance of being in the sample.</p>
</li>
<li>
<p><strong>oversampling</strong>: The technique of increasing the representation of a
sub-population in order to avoid errors due to small sample sizes.</p>
</li>
<li>
<p><strong>raw data</strong>: Values collected and recorded with little or no checking,
calculation or interpretation.</p>
</li>
<li>
<p><strong>recode</strong>: A value that is generated by calculation and other logic
applied to raw data.</p>
</li>
<li>
<p><strong>data cleaning</strong>: Processes that include validating data, identifying
errors, translating between data types and representations, etc.</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="descriptive"><a class="anchor" href="#descriptive"></a><a class="link" href="#descriptive">2. Distributions</a></h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_histograms"><a class="anchor" href="#_histograms"></a><a class="link" href="#_histograms">2.1. Histograms</a></h3>
<div class="paragraph">
<p>One of the best ways to describe a variable is to report the values that
appear in the dataset and how many times each value appears. This
description is called the <strong>distribution</strong> of the variable.</p>
</div>
<div class="paragraph">
<p>The most common representation of a distribution is a <strong>histogram</strong>, which
is a graph that shows the <strong>frequency</strong> of each value. In this context,
&#8220;frequency&#8221; means the number of times the value appears.</p>
</div>
<div class="paragraph">
<p>In Python, an efficient way to compute frequencies is with a dictionary.
Given a sequence of values, t:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">hist = {}
for x in t:
    hist[x] = hist.get(x, 0) + 1</code></pre>
</div>
</div>
<div class="paragraph">
<p>The result is a dictionary that maps from values to frequencies.
Alternatively, you could use the Counter class defined in the
collections module:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">from collections import Counter
counter = Counter(t)</code></pre>
</div>
</div>
<div class="paragraph">
<p>The result is a Counter object, which is a subclass of dictionary.</p>
</div>
<div class="paragraph">
<p>Another option is to use the pandas method <code>value_counts</code>, which we
saw in the previous chapter. But for this book I created a class, Hist,
that represents histograms and provides the methods that operate on
them.</p>
</div>
</div>
<div class="sect2">
<h3 id="_representing_histograms"><a class="anchor" href="#_representing_histograms"></a><a class="link" href="#_representing_histograms">2.2. Representing histograms</a></h3>
<div class="paragraph">
<p>The Hist constructor can take a sequence, dictionary, pandas Series, or
another Hist. You can instantiate a Hist object like this:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; import thinkstats2
&gt;&gt;&gt; hist = thinkstats2.Hist([1, 2, 2, 3, 5])
&gt;&gt;&gt; hist
Hist({1: 1, 2: 2, 3: 1, 5: 1})</pre>
</div>
</div>
<div class="paragraph">
<p>Hist objects provide Freq, which takes a value and returns its
frequency:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; hist.Freq(2)
2</pre>
</div>
</div>
<div class="paragraph">
<p>The bracket operator does the same thing:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; hist[2]
2</pre>
</div>
</div>
<div class="paragraph">
<p>If you look up a value that has never appeared, the frequency is 0.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; hist.Freq(4)
0</pre>
</div>
</div>
<div class="paragraph">
<p>Values returns an unsorted list of the values in the Hist:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; hist.Values()
[1, 5, 3, 2]</pre>
</div>
</div>
<div class="paragraph">
<p>To loop through the values in order, you can use the built-in function
sorted:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">for val in sorted(hist.Values()):
    print(val, hist.Freq(val))</code></pre>
</div>
</div>
<div class="paragraph">
<p>Or you can use Items to iterate through value-frequency pairs:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">for val, freq in hist.Items():
     print(val, freq)</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_plotting_histograms"><a class="anchor" href="#_plotting_histograms"></a><a class="link" href="#_plotting_histograms">2.3. Plotting histograms</a></h3>
<div id="first_wgt_lb_hist" class="imageblock">
<div class="content">
<img src="figs/first_wgt_lb_hist.png" alt="first wgt lb hist" height="240">
</div>
<div class="title">Figure 1. Histogram of the pound part of birth weight.</div>
</div>
<div class="paragraph">
<p>For this book I wrote a module called thinkplot.py that provides
functions for plotting Hists and other objects defined in
thinkstats2.py. It is based on pyplot, which is part of the matplotlib
package. See Section <a href="#code">Using the code</a> for information about installing
matplotlib.</p>
</div>
<div class="paragraph">
<p>To plot hist with thinkplot, try this:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; import thinkplot
&gt;&gt;&gt; thinkplot.Hist(hist)
&gt;&gt;&gt; thinkplot.Show(xlabel='value', ylabel='frequency')</pre>
</div>
</div>
<div class="paragraph">
<p>You can read the documentation for thinkplot at
<a href="http://greenteapress.com/thinkstats2/thinkplot.html" class="bare">http://greenteapress.com/thinkstats2/thinkplot.html</a>.</p>
</div>
<div id="first_wgt_oz_hist" class="paragraph">
<p>image::figs/first_wgt_oz_hist.png[Histogram of the ounce part of birth
weight.,height=240]</p>
</div>
</div>
<div class="sect2">
<h3 id="_nsfg_variables"><a class="anchor" href="#_nsfg_variables"></a><a class="link" href="#_nsfg_variables">2.4. NSFG variables</a></h3>
<div class="paragraph">
<p>Now let’s get back to the data from the NSFG. The code in this chapter
is in first.py. For information about downloading and working with this
code, see Section <a href="#code">Using the code</a>.</p>
</div>
<div class="paragraph">
<p>When you start working with a new dataset, I suggest you explore the
variables you are planning to use one at a time, and a good way to start
is by looking at histograms.</p>
</div>
<div class="paragraph">
<p>In Section <a href="#cleaning">1.6</a> we transformed agepreg from centiyears to
years, and combined <code>birthwgt_lb</code> and <code>birthwgt_oz</code> into a single
quantity, <code>totalwgt_lb</code>. In this section I use these variables to
demonstrate some features of histograms.</p>
</div>
<div id="first_agepreg_hist" class="paragraph">
<p>image::figs/first_agepreg_hist.png[Histogram of mother’s age at end of
pregnancy.,height=240]</p>
</div>
<div class="paragraph">
<p>I’ll start by reading the data and selecting records for live births:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    preg = nsfg.ReadFemPreg()
    live = preg[preg.outcome == 1]</code></pre>
</div>
</div>
<div class="paragraph">
<p>The expression in brackets is a boolean Series that selects rows from
the DataFrame and returns a new DataFrame. Next I generate and plot the
histogram of <code>birthwgt_lb</code> for live births.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    hist = thinkstats2.Hist(live.birthwgt_lb, label='birthwgt_lb')
    thinkplot.Hist(hist)
    thinkplot.Show(xlabel='pounds', ylabel='frequency')</code></pre>
</div>
</div>
<div class="paragraph">
<p>When the argument passed to Hist is a pandas Series, any nan values are
dropped. label is a string that appears in the legend when the Hist is
plotted.</p>
</div>
<div id="first_prglngth_hist" class="imageblock">
<div class="content">
<img src="figs/first_prglngth_hist.png" alt="first prglngth hist" height="240">
</div>
<div class="title">Figure 2. Histogram of pregnancy length in weeks.</div>
</div>
<div class="paragraph">
<p>Figure <a href="#first_wgt_lb_hist">Figure 1</a> shows the result.
The most common value, called the <strong>mode</strong>, is 7 pounds. The distribution
is approximately bell-shaped, which is the shape of the <strong>normal</strong>
distribution, also called a <strong>Gaussian</strong> distribution. But unlike a true
normal distribution, this distribution is asymmetric; it has a <strong>tail</strong>
that extends farther to the left than to the right.</p>
</div>
<div class="paragraph">
<p>Figure <a href="#first_wgt_oz_hist">[first_wgt_oz_hist]</a> shows the histogram
of <code>birthwgt_oz</code>, which is the ounces part of birth weight. In theory
we expect this distribution to be <strong>uniform</strong>; that is, all values should
have the same frequency. In fact, 0 is more common than the other
values, and 1 and 15 are less common, probably because respondents round
off birth weights that are close to an integer value.</p>
</div>
<div class="paragraph">
<p>Figure <a href="#first_agepreg_hist">[first_agepreg_hist]</a> shows the
histogram of <code>agepreg</code>, the mother’s age at the end of pregnancy. The
mode is 21 years. The distribution is very roughly bell-shaped, but in
this case the tail extends farther to the right than left; most mothers
are in their 20s, fewer in their 30s.</p>
</div>
<div class="paragraph">
<p>Figure <a href="#first_prglngth_hist">Figure 2</a> shows the
histogram of <code>prglngth</code>, the length of the pregnancy in weeks. By far
the most common value is 39 weeks. The left tail is longer than the
right; early babies are common, but pregnancies seldom go past 43 weeks,
and doctors often intervene if they do.</p>
</div>
</div>
<div class="sect2">
<h3 id="_outliers"><a class="anchor" href="#_outliers"></a><a class="link" href="#_outliers">2.5. Outliers</a></h3>
<div class="paragraph">
<p>Looking at histograms, it is easy to identify the most common values and
the shape of the distribution, but rare values are not always visible.</p>
</div>
<div class="paragraph">
<p>Before going on, it is a good idea to check for <strong>outliers</strong>, which are
extreme values that might be errors in measurement and recording, or
might be accurate reports of rare events.</p>
</div>
<div class="paragraph">
<p>Hist provides methods Largest and Smallest, which take an integer n and
return the n largest or smallest values from the histogram:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    for weeks, freq in hist.Smallest(10):
        print(weeks, freq)</code></pre>
</div>
</div>
<div class="paragraph">
<p>In the list of pregnancy lengths for live births, the 10 lowest values
are . Values below 10 weeks are certainly errors; the most likely
explanation is that the outcome was not coded correctly. Values higher
than 30 weeks are probably legitimate. Between 10 and 30 weeks, it is
hard to be sure; some values are probably errors, but some represent
premature babies.</p>
</div>
<div class="paragraph">
<p>On the other end of the range, the highest values are:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>weeks  count
43     148
44     46
45     10
46     1
47     1
48     7
50     2</pre>
</div>
</div>
<div class="paragraph">
<p>Most doctors recommend induced labor if a pregnancy exceeds 42 weeks, so
some of the longer values are surprising. In particular, 50 weeks seems
medically unlikely.</p>
</div>
<div class="paragraph">
<p>The best way to handle outliers depends on &#8220;domain knowledge&#8221;; that
is, information about where the data come from and what they mean. And
it depends on what analysis you are planning to perform.</p>
</div>
<div class="paragraph">
<p>In this example, the motivating question is whether first babies tend to
be early (or late). When people ask this question, they are usually
interested in full-term pregnancies, so for this analysis I will focus
on pregnancies longer than 27 weeks.</p>
</div>
</div>
<div class="sect2">
<h3 id="_first_babies"><a class="anchor" href="#_first_babies"></a><a class="link" href="#_first_babies">2.6. First babies</a></h3>
<div class="paragraph">
<p>Now we can compare the distribution of pregnancy lengths for first
babies and others. I divided the DataFrame of live births using
birthord, and computed their histograms:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    firsts = live[live.birthord == 1]
    others = live[live.birthord != 1]

    first_hist = thinkstats2.Hist(firsts.prglngth, label='first')
    other_hist = thinkstats2.Hist(others.prglngth, label='other')</code></pre>
</div>
</div>
<div class="paragraph">
<p>Then I plotted their histograms on the same axis:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    width = 0.45
    thinkplot.PrePlot(2)
    thinkplot.Hist(first_hist, align='right', width=width)
    thinkplot.Hist(other_hist, align='left', width=width)
    thinkplot.Show(xlabel='weeks', ylabel='frequency',
                   xlim=[27, 46])</code></pre>
</div>
</div>
<div class="paragraph">
<p>thinkplot.PrePlot takes the number of histograms we are planning to
plot; it uses this information to choose an appropriate collection of
colors.</p>
</div>
<div id="first_nsfg_hist" class="paragraph">
<p>image::figs/first_nsfg_hist.png[Histogram of pregnancy
lengths.,height=240]</p>
</div>
<div class="paragraph">
<p>thinkplot.Hist normally uses align=’center’ so that each bar is centered
over its value. For this figure, I use align=’right’ and align=’left’ to
place corresponding bars on either side of the value.</p>
</div>
<div class="paragraph">
<p>With width=0.45, the total width of the two bars is 0.9, leaving some
space between each pair.</p>
</div>
<div class="paragraph">
<p>Finally, I adjust the axis to show only data between 27 and 46 weeks.
Figure <a href="#first_nsfg_hist">[first_nsfg_hist]</a> shows the result.</p>
</div>
<div class="paragraph">
<p>Histograms are useful because they make the most frequent values
immediately apparent. But they are not the best choice for comparing two
distributions. In this example, there are fewer &#8220;first babies&#8221; than
&#8220;others,&#8221; so some of the apparent differences in the histograms are
due to sample sizes. In the next chapter we address this problem using
probability mass functions.</p>
</div>
</div>
<div class="sect2">
<h3 id="mean"><a class="anchor" href="#mean"></a><a class="link" href="#mean">2.7. Summarizing distributions</a></h3>
<div class="paragraph">
<p>A histogram is a complete description of the distribution of a sample;
that is, given a histogram, we could reconstruct the values in the
sample (although not their order).</p>
</div>
<div class="paragraph">
<p>If the details of the distribution are important, it might be necessary
to present a histogram. But often we want to summarize the distribution
with a few descriptive statistics.</p>
</div>
<div class="paragraph">
<p>Some of the characteristics we might want to report are:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>central tendency: Do the values tend to cluster around a particular
point?</p>
</li>
<li>
<p>modes: Is there more than one cluster?</p>
</li>
<li>
<p>spread: How much variability is there in the values?</p>
</li>
<li>
<p>tails: How quickly do the probabilities drop off as we move away from
the modes?</p>
</li>
<li>
<p>outliers: Are there extreme values far from the modes?</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Statistics designed to answer these questions are called <strong>summary
statistics</strong>. By far the most common summary statistic is the <strong>mean</strong>,
which is meant to describe the central tendency of the distribution.</p>
</div>
<div class="paragraph">
<p>If you have a sample of n values, \(x_i\), the mean,
\(\bar{x}\), is the sum of the values divided by the number of
values; in other words</p>
</div>
<div class="stemblock">
<div class="content">
\[\bar{x}= \frac{1}{n} \sum_i x_i\]
</div>
</div>
<div class="paragraph">
<p>The words &#8220;mean&#8221; and &#8220;average&#8221; are sometimes used interchangeably,
but I make this distinction:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The &#8220;mean&#8221; of a sample is the summary statistic computed with the
previous formula.</p>
</li>
<li>
<p>An &#8220;average&#8221; is one of several summary statistics you might choose
to describe a central tendency.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Sometimes the mean is a good description of a set of values. For
example, apples are all pretty much the same size (at least the ones
sold in supermarkets). So if I buy 6 apples and the total weight is 3
pounds, it would be a reasonable summary to say they are about a half
pound each.</p>
</div>
<div class="paragraph">
<p>But pumpkins are more diverse. Suppose I grow several varieties in my
garden, and one day I harvest three decorative pumpkins that are 1 pound
each, two pie pumpkins that are 3 pounds each, and one Atlantic
Giant pumpkin that weighs 591 pounds. The mean of this sample is 100
pounds, but if I told you &#8220;The average pumpkin in my garden is 100
pounds,&#8221; that would be misleading. In this example, there is no
meaningful average because there is no typical pumpkin.</p>
</div>
</div>
<div class="sect2">
<h3 id="_variance"><a class="anchor" href="#_variance"></a><a class="link" href="#_variance">2.8. Variance</a></h3>
<div class="paragraph">
<p>If there is no single number that summarizes pumpkin weights, we can do
a little better with two numbers: mean and <strong>variance</strong>.</p>
</div>
<div class="paragraph">
<p>Variance is a summary statistic intended to describe the variability or
spread of a distribution. The variance of a set of values is</p>
</div>
<div class="stemblock">
<div class="content">
\[S^2 = \frac{1}{n} \sum_i (x_i - \bar{x})^2\]
</div>
</div>
<div class="paragraph">
<p>The term \(x_i - \bar{x}\) is called the &#8220;deviation from the
mean,&#8221; so variance is the mean squared deviation. The square root of
variance, \(S\), is the <strong>standard deviation</strong>.</p>
</div>
<div class="paragraph">
<p>If you have prior experience, you might have seen a formula for variance
with \(n-1\) in the denominator, rather than n. This statistic
is used to estimate the variance in a population using a sample. We will
come back to this in Chapter <a href="#estimation">8</a>.</p>
</div>
<div class="paragraph">
<p>Pandas data structures provides methods to compute mean, variance and
standard deviation:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    mean = live.prglngth.mean()
    var = live.prglngth.var()
    std = live.prglngth.std()</code></pre>
</div>
</div>
<div class="paragraph">
<p>For all live births, the mean pregnancy length is 38.6 weeks, the
standard deviation is 2.7 weeks, which means we should expect deviations
of 2-3 weeks to be common.</p>
</div>
<div class="paragraph">
<p>Variance of pregnancy length is 7.3, which is hard to interpret,
especially since the units are weeks\(^2\), or &#8220;square weeks.&#8221;
Variance is useful in some calculations, but it is not a good summary
statistic.</p>
</div>
</div>
<div class="sect2">
<h3 id="_effect_size"><a class="anchor" href="#_effect_size"></a><a class="link" href="#_effect_size">2.9. Effect size</a></h3>
<div class="paragraph">
<p>An <strong>effect size</strong> is a summary statistic intended to describe (wait for
it) the size of an effect. For example, to describe the difference
between two groups, one obvious choice is the difference in the means.</p>
</div>
<div class="paragraph">
<p>Mean pregnancy length for first babies is 38.601; for other babies it is
38.523. The difference is 0.078 weeks, which works out to 13 hours. As a
fraction of the typical pregnancy length, this difference is about 0.2%.</p>
</div>
<div class="paragraph">
<p>If we assume this estimate is accurate, such a difference would have no
practical consequences. In fact, without observing a large number of
pregnancies, it is unlikely that anyone would notice this difference at
all.</p>
</div>
<div class="paragraph">
<p>Another way to convey the size of the effect is to compare the
difference between groups to the variability within groups. Cohen’s
\(d\) is a statistic intended to do that; it is defined</p>
</div>
<div class="stemblock">
<div class="content">
\[d = \frac{\bar{x_1} - \bar{x_2}}{s}\]
</div>
</div>
<div class="paragraph">
<p>where \(\bar{x_1}\) and \(\bar{x_2}\) are the means of
the groups and \(s\) is the &#8220;pooled standard deviation&#8221;.
Here’s the Python code that computes Cohen’s \(d\):</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def CohenEffectSize(group1, group2):
    diff = group1.mean() - group2.mean()

    var1 = group1.var()
    var2 = group2.var()
    n1, n2 = len(group1), len(group2)

    pooled_var = (n1 * var1 + n2 * var2) / (n1 + n2)
    d = diff / math.sqrt(pooled_var)
    return d</code></pre>
</div>
</div>
<div class="paragraph">
<p>In this example, the difference in means is 0.029 standard deviations,
which is small. To put that in perspective, the difference in height
between men and women is about 1.7 standard deviations (see
<a href="https://en.wikipedia.org/wiki/Effect_size" class="bare">https://en.wikipedia.org/wiki/Effect_size</a>).</p>
</div>
</div>
<div class="sect2">
<h3 id="_reporting_results"><a class="anchor" href="#_reporting_results"></a><a class="link" href="#_reporting_results">2.10. Reporting results</a></h3>
<div class="paragraph">
<p>We have seen several ways to describe the difference in pregnancy length
(if there is one) between first babies and others. How should we report
these results?</p>
</div>
<div class="paragraph">
<p>The answer depends on who is asking the question. A scientist might be
interested in any (real) effect, no matter how small. A doctor might
only care about effects that are <strong>clinically significant</strong>; that is,
differences that affect treatment decisions. A pregnant woman might be
interested in results that are relevant to her, like the probability of
delivering early or late.</p>
</div>
<div class="paragraph">
<p>How you report results also depends on your goals. If you are trying to
demonstrate the importance of an effect, you might choose summary
statistics that emphasize differences. If you are trying to reassure a
patient, you might choose statistics that put the differences in
context.</p>
</div>
<div class="paragraph">
<p>Of course your decisions should also be guided by professional ethics.
It’s ok to be persuasive; you <em>should</em> design statistical reports and
visualizations that tell a story clearly. But you should also do your
best to make your reports honest, and to acknowledge uncertainty and
limitations.</p>
</div>
</div>
<div class="sect2">
<h3 id="_exercises_2"><a class="anchor" href="#_exercises_2"></a><a class="link" href="#_exercises_2">2.11. Exercises</a></h3>
<div class="paragraph">
<p>Based on the results in this chapter, suppose you were asked to
summarize what you learned about whether first babies arrive late.</p>
</div>
<div class="paragraph">
<p>Which summary statistics would you use if you wanted to get a story on
the evening news? Which ones would you use if you wanted to reassure an
anxious patient?</p>
</div>
<div class="paragraph">
<p>Finally, imagine that you are Cecil Adams, author of <em>The Straight Dope</em>
(<a href="http://straightdope.com" class="bare">http://straightdope.com</a>), and your job is to answer the question, &#8220;Do
first babies arrive late?&#8221; Write a paragraph that uses the results in
this chapter to answer the question clearly, precisely, and honestly.</p>
</div>
<div class="paragraph">
<p>In the repository you downloaded, you should find a file named
<code>chap02ex.ipynb</code>; open it. Some cells are already filled in, and you
should execute them. Other cells give you instructions for exercises.
Follow the instructions and fill in the answers.</p>
</div>
<div class="paragraph">
<p>A solution to this exercise is in <code>chap02soln.ipynb</code></p>
</div>
<div class="paragraph">
<p>In the repository you downloaded, you should find a file named
<code>chap02ex.py</code>; you can use this file as a starting place for the
following exercises. My solution is in <code>chap02soln.py</code>.</p>
</div>
<div class="paragraph">
<p>The mode of a distribution is the most frequent value; see
<a href="http://wikipedia.org/wiki/Mode_(statistics" class="bare">http://wikipedia.org/wiki/Mode_(statistics</a>). Write a function called
Mode that takes a Hist and returns the most frequent value.</p>
</div>
<div class="paragraph">
<p>As a more challenging exercise, write a function called AllModes that
returns a list of value-frequency pairs in descending order of
frequency.</p>
</div>
<div class="paragraph">
<p>Using the variable <code>totalwgt_lb</code>, investigate whether first babies are
lighter or heavier than others. Compute Cohen’s \(d\) to
quantify the difference between the groups. How does it compare to the
difference in pregnancy length?</p>
</div>
</div>
<div class="sect2">
<h3 id="_glossary_2"><a class="anchor" href="#_glossary_2"></a><a class="link" href="#_glossary_2">2.12. Glossary</a></h3>
<div class="ulist">
<ul>
<li>
<p>distribution: The values that appear in a sample and the frequency of
each.</p>
</li>
<li>
<p>histogram: A mapping from values to frequencies, or a graph that shows
this mapping.</p>
</li>
<li>
<p>frequency: The number of times a value appears in a sample.</p>
</li>
<li>
<p>mode: The most frequent value in a sample, or one of the most frequent
values.</p>
</li>
<li>
<p>normal distribution: An idealization of a bell-shaped distribution;
also known as a Gaussian distribution.</p>
</li>
<li>
<p>uniform distribution: A distribution in which all values have the same
frequency.</p>
</li>
<li>
<p>tail: The part of a distribution at the high and low extremes.</p>
</li>
<li>
<p>central tendency: A characteristic of a sample or population;
intuitively, it is an average or typical value.</p>
</li>
<li>
<p>outlier: A value far from the central tendency.</p>
</li>
<li>
<p>spread: A measure of how spread out the values in a distribution are.</p>
</li>
<li>
<p>summary statistic: A statistic that quantifies some aspect of a
distribution, like central tendency or spread.</p>
</li>
<li>
<p>variance: A summary statistic often used to quantify spread.</p>
</li>
<li>
<p>standard deviation: The square root of variance, also used as a
measure of spread.</p>
</li>
<li>
<p>effect size: A summary statistic intended to quantify the size of an
effect like a difference between groups.</p>
</li>
<li>
<p>clinically significant: A result, like a difference between groups,
that is relevant in practice.</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_probability_mass_functions"><a class="anchor" href="#_probability_mass_functions"></a><a class="link" href="#_probability_mass_functions">3. Probability mass functions</a></h2>
<div class="sectionbody">
<div class="paragraph">
<p>The code for this chapter is in probability.py. For information about
downloading and working with this code, see Section <a href="#code">Using the code</a>.</p>
</div>
<div class="sect2">
<h3 id="_pmfs"><a class="anchor" href="#_pmfs"></a><a class="link" href="#_pmfs">3.1. Pmfs</a></h3>
<div class="paragraph">
<p>Another way to represent a distribution is a <strong>probability mass function</strong>
(PMF), which maps from each value to its probability. A <strong>probability</strong> is
a frequency expressed as a fraction of the sample size, n. To get from
frequencies to probabilities, we divide through by n, which is called
<strong>normalization</strong>.</p>
</div>
<div class="paragraph">
<p>Given a Hist, we can make a dictionary that maps from each value to its
probability:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">n = hist.Total()
d = {}
for x, freq in hist.Items():
    d[x] = freq / n</code></pre>
</div>
</div>
<div class="paragraph">
<p>Or we can use the Pmf class provided by thinkstats2. Like Hist, the Pmf
constructor can take a list, pandas Series, dictionary, Hist, or another
Pmf object. Here’s an example with a simple list:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; import thinkstats2
&gt;&gt;&gt; pmf = thinkstats2.Pmf([1, 2, 2, 3, 5])
&gt;&gt;&gt; pmf
Pmf({1: 0.2, 2: 0.4, 3: 0.2, 5: 0.2})</pre>
</div>
</div>
<div class="paragraph">
<p>The Pmf is normalized so total probability is 1.</p>
</div>
<div class="paragraph">
<p>Pmf and Hist objects are similar in many ways; in fact, they inherit
many of their methods from a common parent class. For example, the
methods Values and Items work the same way for both. The biggest
difference is that a Hist maps from values to integer counters; a Pmf
maps from values to floating-point probabilities.</p>
</div>
<div class="paragraph">
<p>To look up the probability associated with a value, use Prob:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; pmf.Prob(2)
0.4</pre>
</div>
</div>
<div class="paragraph">
<p>The bracket operator is equivalent:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; pmf[2]
0.4</pre>
</div>
</div>
<div class="paragraph">
<p>You can modify an existing Pmf by incrementing the probability
associated with a value:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; pmf.Incr(2, 0.2)
&gt;&gt;&gt; pmf.Prob(2)
0.6</pre>
</div>
</div>
<div class="paragraph">
<p>Or you can multiply a probability by a factor:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; pmf.Mult(2, 0.5)
&gt;&gt;&gt; pmf.Prob(2)
0.3</pre>
</div>
</div>
<div class="paragraph">
<p>If you modify a Pmf, the result may not be normalized; that is, the
probabilities may no longer add up to 1. To check, you can call Total,
which returns the sum of the probabilities:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; pmf.Total()
0.9</pre>
</div>
</div>
<div class="paragraph">
<p>To renormalize, call Normalize:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; pmf.Normalize()
&gt;&gt;&gt; pmf.Total()
1.0</pre>
</div>
</div>
<div class="paragraph">
<p>Pmf objects provide a Copy method so you can make and modify a copy
without affecting the original.</p>
</div>
<div class="paragraph">
<p>My notation in this section might seem inconsistent, but there is a
system: I use Pmf for the name of the class, pmf for an instance of the
class, and PMF for the mathematical concept of a probability mass
function.</p>
</div>
</div>
<div class="sect2">
<h3 id="_plotting_pmfs"><a class="anchor" href="#_plotting_pmfs"></a><a class="link" href="#_plotting_pmfs">3.2. Plotting PMFs</a></h3>
<div class="paragraph">
<p>thinkplot provides two ways to plot Pmfs:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>To plot a Pmf as a bar graph, you can use thinkplot.Hist. Bar graphs
are most useful if the number of values in the Pmf is small.</p>
</li>
<li>
<p>To plot a Pmf as a step function, you can use thinkplot.Pmf. This
option is most useful if there are a large number of values and the Pmf
is smooth. This function also works with Hist objects.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>In addition, pyplot provides a function called hist that takes a
sequence of values, computes a histogram, and plots it. Since I use Hist
objects, I usually don’t use pyplot.hist.</p>
</div>
<div id="probability_nsfg_pmf" class="paragraph">
<p>image::figs/probability_nsfg_pmf.png[PMF of pregnancy lengths for first
babies and others, using bar graphs and step functions.,height=288]</p>
</div>
<div class="paragraph">
<p>Figure <a href="#probability_nsfg_pmf">[probability_nsfg_pmf]</a> shows PMFs of
pregnancy length for first babies and others using bar graphs (left) and
step functions (right).</p>
</div>
<div class="paragraph">
<p>By plotting the PMF instead of the histogram, we can compare the two
distributions without being mislead by the difference in sample size.
Based on this figure, first babies seem to be less likely than others to
arrive on time (week 39) and more likely to be a late (weeks 41 and 42).</p>
</div>
<div class="paragraph">
<p>Here’s the code that generates
Figure <a href="#probability_nsfg_pmf">[probability_nsfg_pmf]</a>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    thinkplot.PrePlot(2, cols=2)
    thinkplot.Hist(first_pmf, align='right', width=width)
    thinkplot.Hist(other_pmf, align='left', width=width)
    thinkplot.Config(xlabel='weeks',
                     ylabel='probability',
                     axis=[27, 46, 0, 0.6])

    thinkplot.PrePlot(2)
    thinkplot.SubPlot(2)
    thinkplot.Pmfs([first_pmf, other_pmf])
    thinkplot.Show(xlabel='weeks',
                   axis=[27, 46, 0, 0.6])</code></pre>
</div>
</div>
<div class="paragraph">
<p>PrePlot takes optional parameters rows and cols to make a grid of
figures, in this case one row of two figures. The first figure (on the
left) displays the Pmfs using thinkplot.Hist, as we have seen before.</p>
</div>
<div class="paragraph">
<p>The second call to PrePlot resets the color generator. Then SubPlot
switches to the second figure (on the right) and displays the Pmfs using
thinkplot.Pmfs. I used the axis option to ensure that the two figures
are on the same axes, which is generally a good idea if you intend to
compare two figures.</p>
</div>
</div>
<div class="sect2">
<h3 id="visualization"><a class="anchor" href="#visualization"></a><a class="link" href="#visualization">3.3. Other visualizations</a></h3>
<div class="paragraph">
<p>Histograms and PMFs are useful while you are exploring data and trying
to identify patterns and relationships. Once you have an idea what is
going on, a good next step is to design a visualization that makes the
patterns you have identified as clear as possible.</p>
</div>
<div class="paragraph">
<p>In the NSFG data, the biggest differences in the distributions are near
the mode. So it makes sense to zoom in on that part of the graph, and to
transform the data to emphasize differences:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    weeks = range(35, 46)
    diffs = []
    for week in weeks:
        p1 = first_pmf.Prob(week)
        p2 = other_pmf.Prob(week)
        diff = 100 * (p1 - p2)
        diffs.append(diff)

    thinkplot.Bar(weeks, diffs)</code></pre>
</div>
</div>
<div class="paragraph">
<p>In this code, weeks is the range of weeks; diffs is the difference
between the two PMFs in percentage points.
Figure <a href="#probability_nsfg_diffs">[probability_nsfg_diffs]</a> shows the
result as a bar chart. This figure makes the pattern clearer: first
babies are less likely to be born in week 39, and somewhat more likely
to be born in weeks 41 and 42.</p>
</div>
<div id="probability_nsfg_diffs" class="paragraph">
<p>image::figs/probability_nsfg_diffs.png[Difference, in percentage points,
by week.,height=240]</p>
</div>
<div class="paragraph">
<p>For now we should hold this conclusion only tentatively. We used the
same dataset to identify an apparent difference and then chose a
visualization that makes the difference apparent. We can’t be sure this
effect is real; it might be due to random variation. We’ll address this
concern later.</p>
</div>
</div>
<div class="sect2">
<h3 id="_the_class_size_paradox"><a class="anchor" href="#_the_class_size_paradox"></a><a class="link" href="#_the_class_size_paradox">3.4. The class size paradox</a></h3>
<div class="paragraph">
<p>Before we go on, I want to demonstrate one kind of computation you can
do with Pmf objects; I call this example the &#8220;class size paradox.&#8221;</p>
</div>
<div class="paragraph">
<p>At many American colleges and universities, the student-to-faculty ratio
is about 10:1. But students are often surprised to discover that their
average class size is bigger than 10. There are two reasons for the
discrepancy:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Students typically take 4–5 classes per semester, but professors often
teach 1 or 2.</p>
</li>
<li>
<p>The number of students who enjoy a small class is small, but the
number of students in a large class is (ahem!) large.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The first effect is obvious, at least once it is pointed out; the second
is more subtle. Let’s look at an example. Suppose that a college offers
65 classes in a given semester, with the following distribution of
sizes:</p>
</div>
<div class="literalblock">
<div class="content">
<pre> size      count
 5- 9          8
10-14          8
15-19         14
20-24          4
25-29          6
30-34         12
35-39          8
40-44          3
45-49          2</pre>
</div>
</div>
<div class="paragraph">
<p>If you ask the Dean for the average class size, he would construct a
PMF, compute the mean, and report that the average class size is 23.7.
Here’s the code:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    d = { 7: 8, 12: 8, 17: 14, 22: 4,
          27: 6, 32: 12, 37: 8, 42: 3, 47: 2 }

    pmf = thinkstats2.Pmf(d, label='actual')
    print('mean', pmf.Mean())</code></pre>
</div>
</div>
<div class="paragraph">
<p>But if you survey a group of students, ask them how many students are in
their classes, and compute the mean, you would think the average class
was bigger. Let’s see how much bigger.</p>
</div>
<div class="paragraph">
<p>First, I compute the distribution as observed by students, where the
probability associated with each class size is &#8220;biased&#8221; by the number
of students in the class.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def BiasPmf(pmf, label):
    new_pmf = pmf.Copy(label=label)

    for x, p in pmf.Items():
        new_pmf.Mult(x, x)

    new_pmf.Normalize()
    return new_pmf</code></pre>
</div>
</div>
<div class="paragraph">
<p>For each class size, x, we multiply the probability by x, the number of
students who observe that class size. The result is a new Pmf that
represents the biased distribution.</p>
</div>
<div class="paragraph">
<p>Now we can plot the actual and observed distributions:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    biased_pmf = BiasPmf(pmf, label='observed')
    thinkplot.PrePlot(2)
    thinkplot.Pmfs([pmf, biased_pmf])
    thinkplot.Show(xlabel='class size', ylabel='PMF')</code></pre>
</div>
</div>
<div id="class_size1" class="paragraph">
<p>image::figs/class_size1.png[Distribution of class sizes, actual and as
observed by students.,height=288]</p>
</div>
<div class="paragraph">
<p>Figure <a href="#class_size1">[class_size1]</a> shows the result. In the biased
distribution there are fewer small classes and more large ones. The mean
of the biased distribution is 29.1, almost 25% higher than the actual
mean.</p>
</div>
<div class="paragraph">
<p>It is also possible to invert this operation. Suppose you want to find
the distribution of class sizes at a college, but you can’t get reliable
data from the Dean. An alternative is to choose a random sample of
students and ask how many students are in their classes.</p>
</div>
<div class="paragraph">
<p>The result would be biased for the reasons we’ve just seen, but you can
use it to estimate the actual distribution. Here’s the function that
unbiases a Pmf:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def UnbiasPmf(pmf, label):
    new_pmf = pmf.Copy(label=label)

    for x, p in pmf.Items():
        new_pmf.Mult(x, 1.0/x)

    new_pmf.Normalize()
    return new_pmf</code></pre>
</div>
</div>
<div class="paragraph">
<p>It’s similar to BiasPmf; the only difference is that it divides each
probability by x instead of multiplying.</p>
</div>
</div>
<div class="sect2">
<h3 id="_dataframe_indexing"><a class="anchor" href="#_dataframe_indexing"></a><a class="link" href="#_dataframe_indexing">3.5. DataFrame indexing</a></h3>
<div class="paragraph">
<p>In Section <a href="#dataframe">1.4</a> we read a pandas DataFrame and used it
to select and modify data columns. Now let’s look at row selection. To
start, I create a NumPy array of random numbers and use it to initialize
a DataFrame:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; import pandas
&gt;&gt;&gt; array = np.random.randn(4, 2)
&gt;&gt;&gt; df = pandas.DataFrame(array)
&gt;&gt;&gt; df
          0         1
0 -0.143510  0.616050
1 -1.489647  0.300774
2 -0.074350  0.039621
3 -1.369968  0.545897</pre>
</div>
</div>
<div class="paragraph">
<p>By default, the rows and columns are numbered starting at zero, but you
can provide column names:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; columns = ['A', 'B']
&gt;&gt;&gt; df = pandas.DataFrame(array, columns=columns)
&gt;&gt;&gt; df
          A         B
0 -0.143510  0.616050
1 -1.489647  0.300774
2 -0.074350  0.039621
3 -1.369968  0.545897</pre>
</div>
</div>
<div class="paragraph">
<p>You can also provide row names. The set of row names is called the
<strong>index</strong>; the row names themselves are called <strong>labels</strong>.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; index = ['a', 'b', 'c', 'd']
&gt;&gt;&gt; df = pandas.DataFrame(array, columns=columns, index=index)
&gt;&gt;&gt; df
          A         B
a -0.143510  0.616050
b -1.489647  0.300774
c -0.074350  0.039621
d -1.369968  0.545897</pre>
</div>
</div>
<div class="paragraph">
<p>As we saw in the previous chapter, simple indexing selects a column,
returning a Series:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; df['A']
a   -0.143510
b   -1.489647
c   -0.074350
d   -1.369968
Name: A, dtype: float64</pre>
</div>
</div>
<div class="paragraph">
<p>To select a row by label, you can use the loc attribute, which returns a
Series:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; df.loc['a']
A   -0.14351
B    0.61605
Name: a, dtype: float64</pre>
</div>
</div>
<div class="paragraph">
<p>If you know the integer position of a row, rather than its label, you
can use the iloc attribute, which also returns a Series.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; df.iloc[0]
A   -0.14351
B    0.61605
Name: a, dtype: float64</pre>
</div>
</div>
<div class="paragraph">
<p>loc can also take a list of labels; in that case, the result is a
DataFrame.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; indices = ['a', 'c']
&gt;&gt;&gt; df.loc[indices]
         A         B
a -0.14351  0.616050
c -0.07435  0.039621</pre>
</div>
</div>
<div class="paragraph">
<p>Finally, you can use a slice to select a range of rows by label:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; df['a':'c']
          A         B
a -0.143510  0.616050
b -1.489647  0.300774
c -0.074350  0.039621</pre>
</div>
</div>
<div class="paragraph">
<p>Or by integer position:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; df[0:2]
          A         B
a -0.143510  0.616050
b -1.489647  0.300774</pre>
</div>
</div>
<div class="paragraph">
<p>The result in either case is a DataFrame, but notice that the first
result includes the end of the slice; the second doesn’t.</p>
</div>
<div class="paragraph">
<p>My advice: if your rows have labels that are not simple integers, use
the labels consistently and avoid using integer positions.</p>
</div>
</div>
<div class="sect2">
<h3 id="_exercises_3"><a class="anchor" href="#_exercises_3"></a><a class="link" href="#_exercises_3">3.6. Exercises</a></h3>
<div class="paragraph">
<p>Solutions to these exercises are in <code>chap03soln.ipynb</code> and
<code>chap03soln.py</code></p>
</div>
<div class="paragraph">
<p>Something like the class size paradox appears if you survey children and
ask how many children are in their family. Families with many children
are more likely to appear in your sample, and families with no children
have no chance to be in the sample.</p>
</div>
<div class="paragraph">
<p>Use the NSFG respondent variable <code>NUMKDHH</code> to construct the actual
distribution for the number of children under 18 in the household.</p>
</div>
<div class="paragraph">
<p>Now compute the biased distribution we would see if we surveyed the
children and asked them how many children under 18 (including
themselves) are in their household.</p>
</div>
<div class="paragraph">
<p>Plot the actual and biased distributions, and compute their means. As a
starting place, you can use <code>chap03ex.ipynb</code>.</p>
</div>
<div class="paragraph">
<p>In Section <a href="#mean">2.7</a> we computed the mean of a sample by adding up
the elements and dividing by n. If you are given a PMF, you can still
compute the mean, but the process is slightly different:</p>
</div>
<div class="stemblock">
<div class="content">
\[\bar{x}= \sum_i p_i~x_i\]
</div>
</div>
<div class="paragraph">
<p>where the \(x_i\) are the unique values in the PMF and
\(p_i=PMF(x_i)\). Similarly, you can compute variance like this:</p>
</div>
<div class="stemblock">
<div class="content">
\[S^2 = \sum_i p_i~(x_i - \bar{x})^2\]
</div>
</div>
<div class="paragraph">
<p>Write functions called PmfMean and PmfVar that take a Pmf object and
compute the mean and variance. To test these methods, check that they
are consistent with the methods Mean and Var provided by Pmf.</p>
</div>
<div class="paragraph">
<p>I started with the question, &#8220;Are first babies more likely to be
late?&#8221; To address it, I computed the difference in means between groups
of babies, but I ignored the possibility that there might be a
difference between first babies and others <em>for the same woman</em>.</p>
</div>
<div class="paragraph">
<p>To address this version of the question, select respondents who have at
least two babies and compute pairwise differences. Does this formulation
of the question yield a different result?</p>
</div>
<div class="paragraph">
<p>Hint: use nsfg.MakePregMap.</p>
</div>
<div id="relay" class="paragraph">
<p>In most foot races, everyone starts at the same time. If you are a fast
runner, you usually pass a lot of people at the beginning of the race,
but after a few miles everyone around you is going at the same speed.</p>
</div>
<div class="paragraph">
<p>When I ran a long-distance (209 miles) relay race for the first time, I
noticed an odd phenomenon: when I overtook another runner, I was usually
much faster, and when another runner overtook me, he was usually much
faster.</p>
</div>
<div class="paragraph">
<p>At first I thought that the distribution of speeds might be bimodal;
that is, there were many slow runners and many fast runners, but few at
my speed.</p>
</div>
<div class="paragraph">
<p>Then I realized that I was the victim of a bias similar to the effect of
class size. The race was unusual in two ways: it used a staggered start,
so teams started at different times; also, many teams included runners
at different levels of ability.</p>
</div>
<div class="paragraph">
<p>As a result, runners were spread out along the course with little
relationship between speed and location. When I joined the race, the
runners near me were (pretty much) a random sample of the runners in the
race.</p>
</div>
<div class="paragraph">
<p>So where does the bias come from? During my time on the course, the
chance of overtaking a runner, or being overtaken, is proportional to
the difference in our speeds. I am more likely to catch a slow runner,
and more likely to be caught by a fast runner. But runners at the same
speed are unlikely to see each other.</p>
</div>
<div class="paragraph">
<p>Write a function called ObservedPmf that takes a Pmf representing the
actual distribution of runners’ speeds, and the speed of a running
observer, and returns a new Pmf representing the distribution of
runners’ speeds as seen by the observer.</p>
</div>
<div class="paragraph">
<p>To test your function, you can use relay.py, which reads the results
from the James Joyce Ramble 10K in Dedham MA and converts the pace of
each runner to mph.</p>
</div>
<div class="paragraph">
<p>Compute the distribution of speeds you would observe if you ran a relay
race at 7.5 mph with this group of runners. A solution to this exercise
is in <code>relay_soln.py</code>.</p>
</div>
</div>
<div class="sect2">
<h3 id="_glossary_3"><a class="anchor" href="#_glossary_3"></a><a class="link" href="#_glossary_3">3.7. Glossary</a></h3>
<div class="ulist">
<ul>
<li>
<p>Probability mass function (PMF): a representation of a distribution as
a function that maps from values to probabilities.</p>
</li>
<li>
<p>probability: A frequency expressed as a fraction of the sample size.</p>
</li>
<li>
<p>normalization: The process of dividing a frequency by a sample size to
get a probability.</p>
</li>
<li>
<p>index: In a pandas DataFrame, the index is a special column that
contains the row labels.</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="cumulative"><a class="anchor" href="#cumulative"></a><a class="link" href="#cumulative">4. Cumulative distribution functions</a></h2>
<div class="sectionbody">
<div class="paragraph">
<p>The code for this chapter is in cumulative.py. For information about
downloading and working with this code, see Section <a href="#code">Using the code</a>.</p>
</div>
<div class="sect2">
<h3 id="_the_limits_of_pmfs"><a class="anchor" href="#_the_limits_of_pmfs"></a><a class="link" href="#_the_limits_of_pmfs">4.1. The limits of PMFs</a></h3>
<div class="paragraph">
<p>PMFs work well if the number of values is small. But as the number of
values increases, the probability associated with each value gets
smaller and the effect of random noise increases.</p>
</div>
<div class="paragraph">
<p>For example, we might be interested in the distribution of birth
weights. In the NSFG data, the variable <code>totalwgt_lb</code> records weight
at birth in pounds. Figure <a href="#nsfg_birthwgt_pmf">[nsfg_birthwgt_pmf]</a>
shows the PMF of these values for first babies and others.</p>
</div>
<div id="nsfg_birthwgt_pmf" class="paragraph">
<p>image::figs/nsfg_birthwgt_pmf.png[PMF of birth weights. This figure shows
a limitation of PMFs: they are hard to compare visually.,height=240]</p>
</div>
<div class="paragraph">
<p>Overall, these distributions resemble the bell shape of a normal
distribution, with many values near the mean and a few values much
higher and lower.</p>
</div>
<div class="paragraph">
<p>But parts of this figure are hard to interpret. There are many spikes
and valleys, and some apparent differences between the distributions. It
is hard to tell which of these features are meaningful. Also, it is hard
to see overall patterns; for example, which distribution do you think
has the higher mean?</p>
</div>
<div class="paragraph">
<p>These problems can be mitigated by binning the data; that is, dividing
the range of values into non-overlapping intervals and counting the
number of values in each bin. Binning can be useful, but it is tricky to
get the size of the bins right. If they are big enough to smooth out
noise, they might also smooth out useful information.</p>
</div>
<div class="paragraph">
<p>An alternative that avoids these problems is the cumulative distribution
function (CDF), which is the subject of this chapter. But before I can
explain CDFs, I have to explain percentiles.</p>
</div>
</div>
<div class="sect2">
<h3 id="_percentiles"><a class="anchor" href="#_percentiles"></a><a class="link" href="#_percentiles">4.2. Percentiles</a></h3>
<div class="paragraph">
<p>If you have taken a standardized test, you probably got your results in
the form of a raw score and a <strong>percentile rank</strong>. In this context, the
percentile rank is the fraction of people who scored lower than you (or
the same). So if you are &#8220;in the 90th percentile,&#8221; you did as well as
or better than 90% of the people who took the exam.</p>
</div>
<div class="paragraph">
<p>Here’s how you could compute the percentile rank of a value,
<code>your_score</code>, relative to the values in the sequence scores:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def PercentileRank(scores, your_score):
    count = 0
    for score in scores:
        if score &lt;= your_score:
            count += 1

    percentile_rank = 100.0 * count / len(scores)
    return percentile_rank</code></pre>
</div>
</div>
<div class="paragraph">
<p>As an example, if the scores in the sequence were 55, 66, 77, 88 and 99,
and you got the 88, then your percentile rank would be 100 * 4 / 5 which
is 80.</p>
</div>
<div class="paragraph">
<p>If you are given a value, it is easy to find its percentile rank; going
the other way is slightly harder. If you are given a percentile rank and
you want to find the corresponding value, one option is to sort the
values and search for the one you want:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def Percentile(scores, percentile_rank):
    scores.sort()
    for score in scores:
        if PercentileRank(scores, score) &gt;= percentile_rank:
            return score</code></pre>
</div>
</div>
<div class="paragraph">
<p>The result of this calculation is a <strong>percentile</strong>. For example, the 50th
percentile is the value with percentile rank 50. In the distribution of
exam scores, the 50th percentile is 77.</p>
</div>
<div class="paragraph">
<p>This implementation of Percentile is not efficient. A better approach is
to use the percentile rank to compute the index of the corresponding
percentile:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def Percentile2(scores, percentile_rank):
    scores.sort()
    index = percentile_rank * (len(scores)-1) // 100
    return scores[index]</code></pre>
</div>
</div>
<div class="paragraph">
<p>The difference between &#8220;percentile&#8221; and &#8220;percentile rank&#8221; can be
confusing, and people do not always use the terms precisely. To
summarize, PercentileRank takes a value and computes its percentile rank
in a set of values; Percentile takes a percentile rank and computes the
corresponding value.</p>
</div>
</div>
<div class="sect2">
<h3 id="_cdfs"><a class="anchor" href="#_cdfs"></a><a class="link" href="#_cdfs">4.3. CDFs</a></h3>
<div class="paragraph">
<p>Now that we understand percentiles and percentile ranks, we are ready to
tackle the <strong>cumulative distribution function</strong> (CDF). The CDF is the
function that maps from a value to its percentile rank.</p>
</div>
<div class="paragraph">
<p>The CDF is a function of \(x\), where \(x\) is any value
that might appear in the distribution. To evaluate
\(\mathrm{CDF}(x)\) for a particular value of \(x\), we
compute the fraction of values in the distribution less than or equal to
\(x\).</p>
</div>
<div class="paragraph">
<p>Here’s what that looks like as a function that takes a sequence, sample,
and a value, x:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def EvalCdf(sample, x):
    count = 0.0
    for value in sample:
        if value &lt;= x:
            count += 1

    prob = count / len(sample)
    return prob</code></pre>
</div>
</div>
<div class="paragraph">
<p>This function is almost identical to PercentileRank, except that the
result is a probability in the range 0–1 rather than a percentile rank
in the range 0–100.</p>
</div>
<div class="paragraph">
<p>As an example, suppose we collect a sample with the values . Here are
some values from its CDF:</p>
</div>
<div class="stemblock">
<div class="content">
\[CDF(0) = 0\]
</div>
</div>
<div class="stemblock">
<div class="content">
\[CDF(1) = 0.2\]
</div>
</div>
<div class="stemblock">
<div class="content">
\[CDF(2) = 0.6\]
</div>
</div>
<div class="stemblock">
<div class="content">
\[CDF(3) = 0.8\]
</div>
</div>
<div class="stemblock">
<div class="content">
\[CDF(4) = 0.8\]
</div>
</div>
<div class="stemblock">
<div class="content">
\[CDF(5) = 1\]
</div>
</div>
<div class="paragraph">
<p>We can evaluate the CDF for any value of \(x\), not just values
that appear in the sample. If \(x\) is less than the smallest
value in the sample, \(\mathrm{CDF}(x)\) is 0. If \(x\)
is greater than the largest value, \(\mathrm{CDF}(x)\) is 1.</p>
</div>
<div id="example_cdf" class="imageblock">
<div class="content">
<img src="figs/cumulative_example_cdf.png" alt="Example of a CDF." height="240">
</div>
</div>
<div class="paragraph">
<p>Figure <a href="#example_cdf">[example_cdf]</a> is a graphical representation of
this CDF. The CDF of a sample is a step function.</p>
</div>
</div>
<div class="sect2">
<h3 id="_representing_cdfs"><a class="anchor" href="#_representing_cdfs"></a><a class="link" href="#_representing_cdfs">4.4. Representing CDFs</a></h3>
<div class="paragraph">
<p>thinkstats2 provides a class named Cdf that represents CDFs. The
fundamental methods Cdf provides are:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Prob(x): Given a value x, computes the probability
\(p = \mathrm{CDF}(x)\). The bracket operator is equivalent to
Prob.</p>
</li>
<li>
<p>Value(p): Given a probability p, computes the corresponding value, x;
that is, the <strong>inverse CDF</strong> of p.</p>
</li>
</ul>
</div>
<div id="cumulative_prglngth_cdf" class="imageblock">
<div class="content">
<img src="figs/cumulative_prglngth_cdf.png" alt="CDF of pregnancy length." height="240">
</div>
</div>
<div class="paragraph">
<p>The Cdf constructor can take as an argument a list of values, a pandas
Series, a Hist, Pmf, or another Cdf. The following code makes a Cdf for
the distribution of pregnancy lengths in the NSFG:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    live, firsts, others = first.MakeFrames()
    cdf = thinkstats2.Cdf(live.prglngth, label='prglngth')</code></pre>
</div>
</div>
<div class="paragraph">
<p>thinkplot provides a function named Cdf that plots Cdfs as lines:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    thinkplot.Cdf(cdf)
    thinkplot.Show(xlabel='weeks', ylabel='CDF')</code></pre>
</div>
</div>
<div class="paragraph">
<p>Figure <a href="#cumulative_prglngth_cdf">[cumulative_prglngth_cdf]</a> shows
the result. One way to read a CDF is to look up percentiles. For
example, it looks like about 10% of pregnancies are shorter than 36
weeks, and about 90% are shorter than 41 weeks. The CDF also provides a
visual representation of the shape of the distribution. Common values
appear as steep or vertical sections of the CDF; in this example, the
mode at 39 weeks is apparent. There are few values below 30 weeks, so
the CDF in this range is flat.</p>
</div>
<div class="paragraph">
<p>It takes some time to get used to CDFs, but once you do, I think you
will find that they show more information, more clearly, than PMFs.</p>
</div>
</div>
<div class="sect2">
<h3 id="birth_weights"><a class="anchor" href="#birth_weights"></a><a class="link" href="#birth_weights">4.5. Comparing CDFs</a></h3>
<div class="paragraph">
<p>CDFs are especially useful for comparing distributions. For example,
here is the code that plots the CDF of birth weight for first babies and
others.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    first_cdf = thinkstats2.Cdf(firsts.totalwgt_lb, label='first')
    other_cdf = thinkstats2.Cdf(others.totalwgt_lb, label='other')

    thinkplot.PrePlot(2)
    thinkplot.Cdfs([first_cdf, other_cdf])
    thinkplot.Show(xlabel='weight (pounds)', ylabel='CDF')</code></pre>
</div>
</div>
<div id="cumulative_birthwgt_cdf" class="paragraph">
<p>image::figs/cumulative_birthwgt_cdf.png[CDF of birth weights for first
babies and others.,height=240]</p>
</div>
<div class="paragraph">
<p>Figure <a href="#cumulative_birthwgt_cdf">[cumulative_birthwgt_cdf]</a> shows
the result. Compared to
Figure <a href="#nsfg_birthwgt_pmf">[nsfg_birthwgt_pmf]</a>, this figure makes
the shape of the distributions, and the differences between them, much
clearer. We can see that first babies are slightly lighter throughout
the distribution, with a larger discrepancy above the mean.</p>
</div>
</div>
<div class="sect2">
<h3 id="_percentile_based_statistics"><a class="anchor" href="#_percentile_based_statistics"></a><a class="link" href="#_percentile_based_statistics">4.6. Percentile-based statistics</a></h3>
<div class="paragraph">
<p>Once you have computed a CDF, it is easy to compute percentiles and
percentile ranks. The Cdf class provides these two methods:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>PercentileRank(x): Given a value x, computes its percentile rank,
\(100 \cdot \mathrm{CDF}(x)\).</p>
</li>
<li>
<p>Percentile(p): Given a percentile rank p, computes the corresponding
value, x. Equivalent to Value(p/100).</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Percentile can be used to compute percentile-based summary statistics.
For example, the 50th percentile is the value that divides the
distribution in half, also known as the <strong>median</strong>. Like the mean, the
median is a measure of the central tendency of a distribution.</p>
</div>
<div class="paragraph">
<p>Actually, there are several definitions of &#8220;median,&#8221; each with
different properties. But Percentile(50) is simple and efficient to
compute.</p>
</div>
<div class="paragraph">
<p>Another percentile-based statistic is the <strong>interquartile range</strong> (IQR),
which is a measure of the spread of a distribution. The IQR is the
difference between the 75th and 25th percentiles.</p>
</div>
<div class="paragraph">
<p>More generally, percentiles are often used to summarize the shape of a
distribution. For example, the distribution of income is often reported
in &#8220;quintiles&#8221;; that is, it is split at the 20th, 40th, 60th and 80th
percentiles. Other distributions are divided into ten &#8220;deciles&#8221;.
Statistics like these that represent equally-spaced points in a CDF are
called <strong>quantiles</strong>. For more, see
<a href="https://en.wikipedia.org/wiki/Quantile" class="bare">https://en.wikipedia.org/wiki/Quantile</a>.</p>
</div>
</div>
<div class="sect2">
<h3 id="random"><a class="anchor" href="#random"></a><a class="link" href="#random">4.7. Random numbers</a></h3>
<div class="paragraph">
<p>Suppose we choose a random sample from the population of live births and
look up the percentile rank of their birth weights. Now suppose we
compute the CDF of the percentile ranks. What do you think the
distribution will look like?</p>
</div>
<div class="paragraph">
<p>Here’s how we can compute it. First, we make the Cdf of birth weights:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    weights = live.totalwgt_lb
    cdf = thinkstats2.Cdf(weights, label='totalwgt_lb')</code></pre>
</div>
</div>
<div class="paragraph">
<p>Then we generate a sample and compute the percentile rank of each value
in the sample.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    sample = np.random.choice(weights, 100, replace=True)
    ranks = [cdf.PercentileRank(x) for x in sample]</code></pre>
</div>
</div>
<div class="paragraph">
<p>sample is a random sample of 100 birth weights, chosen with
<strong>replacement</strong>; that is, the same value could be chosen more than once.
ranks is a list of percentile ranks.</p>
</div>
<div class="paragraph">
<p>Finally we make and plot the Cdf of the percentile ranks.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    rank_cdf = thinkstats2.Cdf(ranks)
    thinkplot.Cdf(rank_cdf)
    thinkplot.Show(xlabel='percentile rank', ylabel='CDF')</code></pre>
</div>
</div>
<div id="cumulative_random" class="paragraph">
<p>image::figs/cumulative_random.png[CDF of percentile ranks for a random
sample of birth weights.,height=240]</p>
</div>
<div class="paragraph">
<p>Figure <a href="#cumulative_random">[cumulative_random]</a> shows the result.
The CDF is approximately a straight line, which means that the
distribution is uniform.</p>
</div>
<div class="paragraph">
<p>That outcome might be non-obvious, but it is a consequence of the way
the CDF is defined. What this figure shows is that 10% of the sample is
below the 10th percentile, 20% is below the 20th percentile, and so on,
exactly as we should expect.</p>
</div>
<div class="paragraph">
<p>So, regardless of the shape of the CDF, the distribution of percentile
ranks is uniform. This property is useful, because it is the basis of a
simple and efficient algorithm for generating random numbers with a
given CDF. Here’s how:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Choose a percentile rank uniformly from the range 0–100.</p>
</li>
<li>
<p>Use Cdf.Percentile to find the value in the distribution that
corresponds to the percentile rank you chose.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Cdf provides an implementation of this algorithm, called Random:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python"># class Cdf:
    def Random(self):
        return self.Percentile(random.uniform(0, 100))</code></pre>
</div>
</div>
<div class="paragraph">
<p>Cdf also provides Sample, which takes an integer, n, and returns a list
of n values chosen at random from the Cdf.</p>
</div>
</div>
<div class="sect2">
<h3 id="_comparing_percentile_ranks"><a class="anchor" href="#_comparing_percentile_ranks"></a><a class="link" href="#_comparing_percentile_ranks">4.8. Comparing percentile ranks</a></h3>
<div class="paragraph">
<p>Percentile ranks are useful for comparing measurements across different
groups. For example, people who compete in foot races are usually
grouped by age and gender. To compare people in different age groups,
you can convert race times to percentile ranks.</p>
</div>
<div class="paragraph">
<p>A few years ago I ran the James Joyce Ramble 10K in Dedham MA; I
finished in 42:44, which was 97th in a field of 1633. I beat or tied
1537 runners out of 1633, so my percentile rank in the field is 94%.</p>
</div>
<div class="paragraph">
<p>More generally, given position and field size, we can compute percentile
rank:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def PositionToPercentile(position, field_size):
    beat = field_size - position + 1
    percentile = 100.0 * beat / field_size
    return percentile</code></pre>
</div>
</div>
<div class="paragraph">
<p>In my age group, denoted M4049 for &#8220;male between 40 and 49 years of
age&#8221;, I came in 26th out of 256. So my percentile rank in my age group
was 90%.</p>
</div>
<div class="paragraph">
<p>If I am still running in 10 years (and I hope I am), I will be in the
M5059 division. Assuming that my percentile rank in my division is the
same, how much slower should I expect to be?</p>
</div>
<div class="paragraph">
<p>I can answer that question by converting my percentile rank in M4049 to
a position in M5059. Here’s the code:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def PercentileToPosition(percentile, field_size):
    beat = percentile * field_size / 100.0
    position = field_size - beat + 1
    return position</code></pre>
</div>
</div>
<div class="paragraph">
<p>There were 171 people in M5059, so I would have to come in between 17th
and 18th place to have the same percentile rank. The finishing time of
the 17th runner in M5059 was 46:05, so that’s the time I will have to
beat to maintain my percentile rank.</p>
</div>
</div>
<div class="sect2">
<h3 id="_exercises_4"><a class="anchor" href="#_exercises_4"></a><a class="link" href="#_exercises_4">4.9. Exercises</a></h3>
<div class="paragraph">
<p>For the following exercises, you can start with <code>chap04ex.ipynb</code>. My
solution is in <code>chap04soln.ipynb</code>.</p>
</div>
<div class="paragraph">
<p>How much did you weigh at birth? If you don’t know, call your mother or
someone else who knows. Using the NSFG data (all live births), compute
the distribution of birth weights and use it to find your percentile
rank. If you were a first baby, find your percentile rank in the
distribution for first babies. Otherwise use the distribution for
others. If you are in the 90th percentile or higher, call your mother
back and apologize.</p>
</div>
<div class="paragraph">
<p>The numbers generated by random.random are supposed to be uniform
between 0 and 1; that is, every value in the range should have the same
probability.</p>
</div>
<div class="paragraph">
<p>Generate 1000 numbers from random.random and plot their PMF and CDF. Is
the distribution uniform?</p>
</div>
</div>
<div class="sect2">
<h3 id="_glossary_4"><a class="anchor" href="#_glossary_4"></a><a class="link" href="#_glossary_4">4.10. Glossary</a></h3>
<div class="ulist">
<ul>
<li>
<p>percentile rank: The percentage of values in a distribution that are
less than or equal to a given value.</p>
</li>
<li>
<p>percentile: The value associated with a given percentile rank.</p>
</li>
<li>
<p>cumulative distribution function (CDF): A function that maps from
values to their cumulative probabilities. \(\mathrm{CDF}(x)\) is
the fraction of the sample less than or equal to \(x\).</p>
</li>
<li>
<p>inverse CDF: A function that maps from a cumulative probability,
\(p\), to the corresponding value.</p>
</li>
<li>
<p>median: The 50th percentile, often used as a measure of central
tendency.</p>
</li>
<li>
<p>interquartile range: The difference between the 75th and 25th
percentiles, used as a measure of spread.</p>
</li>
<li>
<p>quantile: A sequence of values that correspond to equally spaced
percentile ranks; for example, the quartiles of a distribution are the
25th, 50th and 75th percentiles.</p>
</li>
<li>
<p>replacement: A property of a sampling process. &#8220;With replacement&#8221;
means that the same value can be chosen more than once; &#8220;without
replacement&#8221; means that once a value is chosen, it is removed from the
population.</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="modeling"><a class="anchor" href="#modeling"></a><a class="link" href="#modeling">5. Modeling distributions</a></h2>
<div class="sectionbody">
<div class="paragraph">
<p>The distributions we have used so far are called <strong>empirical
distributions</strong> because they are based on empirical observations, which
are necessarily finite samples.</p>
</div>
<div class="paragraph">
<p>The alternative is an <strong>analytic distribution</strong>, which is characterized by
a CDF that is a mathematical function. Analytic distributions can be
used to model empirical distributions. In this context, a <strong>model</strong> is a
simplification that leaves out unneeded details. This chapter presents
common analytic distributions and uses them to model data from a variety
of sources.</p>
</div>
<div class="paragraph">
<p>The code for this chapter is in analytic.py. For information about
downloading and working with this code, see Section <a href="#code">Using the code</a>.</p>
</div>
<div class="sect2">
<h3 id="exponential"><a class="anchor" href="#exponential"></a><a class="link" href="#exponential">5.1. The exponential distribution</a></h3>
<div id="analytic_expo_cdf" class="paragraph">
<p>image::figs/analytic_expo_cdf.png[CDFs of exponential distributions with
various parameters.,height=240]</p>
</div>
<div class="paragraph">
<p>I’ll start with the <strong>exponential distribution</strong> because it is relatively
simple. The CDF of the exponential distribution is</p>
</div>
<div class="stemblock">
<div class="content">
\[\mathrm{CDF}(x) = 1 - e^{-\lambda x}\]
</div>
</div>
<div class="paragraph">
<p>The parameter, \(\lambda\), determines the shape of the
distribution. Figure <a href="#analytic_expo_cdf">[analytic_expo_cdf]</a> shows
what this CDF looks like with \(\lambda =\) 0.5, 1, and 2.</p>
</div>
<div class="paragraph">
<p>In the real world, exponential distributions come up when we look at a
series of events and measure the times between events, called
<strong>interarrival times</strong>. If the events are equally likely to occur at any
time, the distribution of interarrival times tends to look like an
exponential distribution.</p>
</div>
<div class="paragraph">
<p>As an example, we will look at the interarrival time of births. On
December 18, 1997, 44 babies were born in a hospital in Brisbane,
Australia.<sup class="footnote">[<a id="_footnoteref_1" class="footnote" href="#_footnote_1" title="View footnote.">1</a>]</sup> The time of birth for
all 44 babies was reported in the local paper; the complete dataset is
in a file called babyboom.dat, in the ThinkStats2 repository.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    df = ReadBabyBoom()
    diffs = df.minutes.diff()
    cdf = thinkstats2.Cdf(diffs, label='actual')

    thinkplot.Cdf(cdf)
    thinkplot.Show(xlabel='minutes', ylabel='CDF')</code></pre>
</div>
</div>
<div class="paragraph">
<p>ReadBabyBoom reads the data file and returns a DataFrame with columns
time, sex, <code>weight_g</code>, and minutes, where minutes is time of birth
converted to minutes since midnight.</p>
</div>
<div id="analytic_interarrival_cdf" class="paragraph">
<p>image::figs/analytic_interarrivals.png[CDF of interarrival times (left)
and CCDF on a log-y scale (right).,height=240]</p>
</div>
<div class="paragraph">
<p>diffs is the difference between consecutive birth times, and cdf is the
distribution of these interarrival times.
Figure <a href="#analytic_interarrival_cdf">[analytic_interarrival_cdf]</a>
(left) shows the CDF. It seems to have the general shape of an
exponential distribution, but how can we tell?</p>
</div>
<div class="paragraph">
<p>One way is to plot the <strong>complementary CDF</strong>, which is
\(1 - \mathrm{CDF}(x)\), on a log-y scale. For data from an
exponential distribution, the result is a straight line. Let’s see why
that works.</p>
</div>
<div class="paragraph">
<p>If you plot the complementary CDF (CCDF) of a dataset that you think is
exponential, you expect to see a function like:</p>
</div>
<div class="stemblock">
<div class="content">
\[y \approx e^{-\lambda x}\]
</div>
</div>
<div class="paragraph">
<p>Taking the log of both sides yields:</p>
</div>
<div class="stemblock">
<div class="content">
\[\log y \approx -\lambda x\]
</div>
</div>
<div class="paragraph">
<p>So on a log-y scale the CCDF is a straight line with slope
\(-\lambda\). Here’s how we can generate a plot like that:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    thinkplot.Cdf(cdf, complement=True)
    thinkplot.Show(xlabel='minutes',
                   ylabel='CCDF',
                   yscale='log')</code></pre>
</div>
</div>
<div class="paragraph">
<p>With the argument complement=True, thinkplot.Cdf computes the
complementary CDF before plotting. And with yscale=’log’, thinkplot.Show
sets the y axis to a logarithmic scale.</p>
</div>
<div class="paragraph">
<p>Figure <a href="#analytic_interarrival_cdf">[analytic_interarrival_cdf]</a>
(right) shows the result. It is not exactly straight, which indicates
that the exponential distribution is not a perfect model for this data.
Most likely the underlying assumption—that a birth is equally likely at
any time of day—is not exactly true. Nevertheless, it might be
reasonable to model this dataset with an exponential distribution. With
that simplification, we can summarize the distribution with a single
parameter.</p>
</div>
<div class="paragraph">
<p>The parameter, \(\lambda\), can be interpreted as a rate; that
is, the number of events that occur, on average, in a unit of time. In
this example, 44 babies are born in 24 hours, so the rate is
\(\lambda =
0.0306\) births per minute. The mean of an exponential distribution is
\(1/\lambda\), so the mean time between births is 32.7 minutes.</p>
</div>
</div>
<div class="sect2">
<h3 id="normal"><a class="anchor" href="#normal"></a><a class="link" href="#normal">5.2. The normal distribution</a></h3>
<div class="paragraph">
<p>The <strong>normal distribution</strong>, also called Gaussian, is commonly used
because it describes many phenomena, at least approximately. It turns
out that there is a good reason for its ubiquity, which we will get to
in Section <a href="#CLT">14.4</a>.</p>
</div>
<div id="analytic_gaussian_cdf" class="paragraph">
<p>image::figs/analytic_gaussian_cdf.png[CDF of normal distributions with a
range of parameters.,height=240]</p>
</div>
<div class="paragraph">
<p>The normal distribution is characterized by two parameters: the mean,
\(\mu\), and standard deviation \(\sigma\). The normal
distribution with \(\mu=0\) and \(\sigma=1\) is called
the <strong>standard normal distribution</strong>. Its CDF is defined by an integral
that does not have a closed form solution, but there are algorithms that
evaluate it efficiently. One of them is provided by SciPy:
scipy.stats.norm is an object that represents a normal distribution; it
provides a method, cdf, that evaluates the standard normal CDF:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; import scipy.stats
&gt;&gt;&gt; scipy.stats.norm.cdf(0)
0.5</pre>
</div>
</div>
<div class="paragraph">
<p>This result is correct: the median of the standard normal distribution
is 0 (the same as the mean), and half of the values fall below the
median, so \(\mathrm{CDF}(0)\) is 0.5.</p>
</div>
<div class="paragraph">
<p>norm.cdf takes optional parameters: loc, which specifies the mean, and
scale, which specifies the standard deviation.</p>
</div>
<div class="paragraph">
<p>thinkstats2 makes this function a little easier to use by providing
EvalNormalCdf, which takes parameters mu and sigma and evaluates the CDF
at x:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def EvalNormalCdf(x, mu=0, sigma=1):
    return scipy.stats.norm.cdf(x, loc=mu, scale=sigma)</code></pre>
</div>
</div>
<div class="paragraph">
<p>Figure <a href="#analytic_gaussian_cdf">[analytic_gaussian_cdf]</a> shows CDFs
for normal distributions with a range of parameters. The sigmoid shape
of these curves is a recognizable characteristic of a normal
distribution.</p>
</div>
<div class="paragraph">
<p>In the previous chapter we looked at the distribution of birth weights
in the NSFG.
Figure <a href="#analytic_birthwgt_model">[analytic_birthwgt_model]</a> shows
the empirical CDF of weights for all live births and the CDF of a normal
distribution with the same mean and variance.</p>
</div>
<div id="analytic_birthwgt_model" class="paragraph">
<p>image::figs/analytic_birthwgt_model.png[CDF of birth weights with a
normal model.,height=240]</p>
</div>
<div class="paragraph">
<p>The normal distribution is a good model for this dataset, so if we
summarize the distribution with the parameters \(\mu = 7.28\)
and \(\sigma = 1.24\), the resulting error (difference between
the model and the data) is small.</p>
</div>
<div class="paragraph">
<p>Below the 10th percentile there is a discrepancy between the data and
the model; there are more light babies than we would expect in a normal
distribution. If we are specifically interested in preterm babies, it
would be important to get this part of the distribution right, so it
might not be appropriate to use the normal model.</p>
</div>
</div>
<div class="sect2">
<h3 id="_normal_probability_plot"><a class="anchor" href="#_normal_probability_plot"></a><a class="link" href="#_normal_probability_plot">5.3. Normal probability plot</a></h3>
<div class="paragraph">
<p>For the exponential distribution, and a few others, there are simple
transformations we can use to test whether an analytic distribution is a
good model for a dataset.</p>
</div>
<div class="paragraph">
<p>For the normal distribution there is no such transformation, but there
is an alternative called a <strong>normal probability plot</strong>. There are two ways
to generate a normal probability plot: the hard way and the easy way. If
you are interested in the hard way, you can read about it at
<a href="https://en.wikipedia.org/wiki/Normal_probability_plot" class="bare">https://en.wikipedia.org/wiki/Normal_probability_plot</a>. Here’s the easy
way:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Sort the values in the sample.</p>
</li>
<li>
<p>From a standard normal distribution (\(\mu=0\) and
\(\sigma=1\)), generate a random sample with the same size as
the sample, and sort it.</p>
</li>
<li>
<p>Plot the sorted values from the sample versus the random values.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>If the distribution of the sample is approximately normal, the result is
a straight line with intercept mu and slope sigma. thinkstats2 provides
NormalProbability, which takes a sample and returns two NumPy arrays:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">xs, ys = thinkstats2.NormalProbability(sample)</code></pre>
</div>
</div>
<div id="analytic_normal_prob_example" class="paragraph">
<p>image::figs/analytic_normal_prob_example.png[Normal probability plot for
random samples from normal distributions.,height=240]</p>
</div>
<div class="paragraph">
<p>ys contains the sorted values from sample; xs contains the random values
from the standard normal distribution.</p>
</div>
<div class="paragraph">
<p>To test NormalProbability I generated some fake samples that were
actually drawn from normal distributions with various parameters.
Figure <a href="#analytic_normal_prob_example">[analytic_normal_prob_example]</a>
shows the results. The lines are approximately straight, with values in
the tails deviating more than values near the mean.</p>
</div>
<div class="paragraph">
<p>Now let’s try it with real data. Here’s code to generate a normal
probability plot for the birth weight data from the previous section. It
plots a gray line that represents the model and a blue line that
represents the data.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def MakeNormalPlot(weights):
    mean = weights.mean()
    std = weights.std()

    xs = [-4, 4]
    fxs, fys = thinkstats2.FitLine(xs, inter=mean, slope=std)
    thinkplot.Plot(fxs, fys, color='gray', label='model')

    xs, ys = thinkstats2.NormalProbability(weights)
    thinkplot.Plot(xs, ys, label='birth weights')</code></pre>
</div>
</div>
<div class="paragraph">
<p>weights is a pandas Series of birth weights; mean and std are the mean
and standard deviation.</p>
</div>
<div class="paragraph">
<p>FitLine takes a sequence of xs, an intercept, and a slope; it returns xs
and ys that represent a line with the given parameters, evaluated at the
values in xs.</p>
</div>
<div class="paragraph">
<p>NormalProbability returns xs and ys that contain values from the
standard normal distribution and values from weights. If the
distribution of weights is normal, the data should match the model.</p>
</div>
<div id="analytic_birthwgt_normal" class="paragraph">
<p>image::figs/analytic_birthwgt_normal.png[Normal probability plot of birth
weights.,height=240]</p>
</div>
<div class="paragraph">
<p>Figure <a href="#analytic_birthwgt_normal">[analytic_birthwgt_normal]</a> shows
the results for all live births, and also for full term births
(pregnancy length greater than 36 weeks). Both curves match the model
near the mean and deviate in the tails. The heaviest babies are heavier
than what the model expects, and the lightest babies are lighter.</p>
</div>
<div class="paragraph">
<p>When we select only full term births, we remove some of the lightest
weights, which reduces the discrepancy in the lower tail of the
distribution.</p>
</div>
<div class="paragraph">
<p>This plot suggests that the normal model describes the distribution well
within a few standard deviations from the mean, but not in the tails.
Whether it is good enough for practical purposes depends on the
purposes.</p>
</div>
</div>
<div class="sect2">
<h3 id="lognormal"><a class="anchor" href="#lognormal"></a><a class="link" href="#lognormal">5.4. The lognormal distribution</a></h3>
<div class="paragraph">
<p>If the logarithms of a set of values have a normal distribution, the
values have a <strong>lognormal distribution</strong>. The CDF of the lognormal
distribution is the same as the CDF of the normal distribution, with
\(\log x\) substituted for \(x\).</p>
</div>
<div class="stemblock">
<div class="content">
\[CDF_{lognormal}(x) = CDF_{normal}(\log x)\]
</div>
</div>
<div class="paragraph">
<p>The parameters of the lognormal distribution are usually denoted
\(\mu\) and \(\sigma\). But remember that these
parameters are <em>not</em> the mean and standard deviation; the mean of a
lognormal distribution is \(\exp(\mu +\sigma^2/2)\) and the
standard deviation is ugly (see
<a href="http://wikipedia.org/wiki/Log-normal_distribution" class="bare">http://wikipedia.org/wiki/Log-normal_distribution</a>).</p>
</div>
<div id="brfss_weight" class="paragraph">
<p>image::figs/brfss_weight.png[CDF of adult weights on a linear scale
(left) and log scale (right).,height=240]</p>
</div>
<div class="paragraph">
<p>If a sample is approximately lognormal and you plot its CDF on a log-x
scale, it will have the characteristic shape of a normal distribution.
To test how well the sample fits a lognormal model, you can make a
normal probability plot using the log of the values in the sample.</p>
</div>
<div class="paragraph">
<p>As an example, let’s look at the distribution of adult weights, which is
approximately lognormal.<sup class="footnote">[<a id="_footnoteref_2" class="footnote" href="#_footnote_2" title="View footnote.">2</a>]</sup></p>
</div>
<div class="paragraph">
<p>The National Center for Chronic Disease Prevention and Health Promotion
conducts an annual survey as part of the Behavioral Risk Factor
Surveillance System (BRFSS).<sup class="footnote">[<a id="_footnoteref_3" class="footnote" href="#_footnote_3" title="View footnote.">3</a>]</sup> In 2008, they
interviewed 414,509 respondents and asked about their demographics,
health, and health risks. Among the data they collected are the weights
in kilograms of 398,484 respondents.</p>
</div>
<div class="paragraph">
<p>The repository for this book contains CDBRFS08.ASC.gz, a fixed-width
ASCII file that contains data from the BRFSS, and brfss.py, which reads
the file and analyzes the data.</p>
</div>
<div id="brfss_weight_normal" class="paragraph">
<p>image::figs/brfss_weight_normal.png[Normal probability plots for adult
weight on a linear scale (left) and log scale (right).,height=240]</p>
</div>
<div class="paragraph">
<p>Figure <a href="#brfss_weight">[brfss_weight]</a> (left) shows the distribution
of adult weights on a linear scale with a normal model.
Figure <a href="#brfss_weight">[brfss_weight]</a> (right) shows the same
distribution on a log scale with a lognormal model. The lognormal model
is a better fit, but this representation of the data does not make the
difference particularly dramatic.</p>
</div>
<div class="paragraph">
<p>Figure <a href="#brfss_weight_normal">[brfss_weight_normal]</a> shows normal
probability plots for adult weights, \(w\), and for their
logarithms, \(\log_{10} w\). Now it is apparent that the data
deviate substantially from the normal model. On the other hand, the
lognormal model is a good match for the data.</p>
</div>
</div>
<div class="sect2">
<h3 id="_the_pareto_distribution"><a class="anchor" href="#_the_pareto_distribution"></a><a class="link" href="#_the_pareto_distribution">5.5. The Pareto distribution</a></h3>
<div class="paragraph">
<p>The <strong>Pareto distribution</strong> is named after the economist Vilfredo Pareto,
who used it to describe the distribution of wealth (see
<a href="http://wikipedia.org/wiki/Pareto_distribution" class="bare">http://wikipedia.org/wiki/Pareto_distribution</a>). Since then, it has been
used to describe phenomena in the natural and social sciences including
sizes of cities and towns, sand particles and meteorites, forest fires
and earthquakes.</p>
</div>
<div class="paragraph">
<p>The CDF of the Pareto distribution is:</p>
</div>
<div class="stemblock">
<div class="content">
\[CDF(x) = 1 - \left( \frac{x}{x_m} \right) ^{-\alpha}\]
</div>
</div>
<div class="paragraph">
<p>The parameters \(x_{m}\) and \(\alpha\) determine the
location and shape of the distribution. \(x_{m}\) is the minimum
possible value. Figure <a href="#analytic_pareto_cdf">[analytic_pareto_cdf]</a>
shows CDFs of Pareto distributions with \(x_{m} = 0.5\) and
different values of \(\alpha\).</p>
</div>
<div id="analytic_pareto_cdf" class="paragraph">
<p>image::figs/analytic_pareto_cdf.png[CDFs of Pareto distributions with
different parameters.,height=240]</p>
</div>
<div class="paragraph">
<p>There is a simple visual test that indicates whether an empirical
distribution fits a Pareto distribution: on a log-log scale, the CCDF
looks like a straight line. Let’s see why that works.</p>
</div>
<div class="paragraph">
<p>If you plot the CCDF of a sample from a Pareto distribution on a linear
scale, you expect to see a function like:</p>
</div>
<div class="stemblock">
<div class="content">
\[y \approx \left( \frac{x}{x_m} \right) ^{-\alpha}\]
</div>
</div>
<div class="paragraph">
<p>Taking the log of both sides yields:</p>
</div>
<div class="stemblock">
<div class="content">
\[\log y \approx -\alpha (\log x - \log x_{m})\]
</div>
</div>
<div class="paragraph">
<p>So if you plot \(\log y\) versus \(\log x\), it should
look like a straight line with slope \(-\alpha\) and intercept
\(\alpha \log x_{m}\).</p>
</div>
<div class="paragraph">
<p>As an example, let’s look at the sizes of cities and towns. The
U.S. Census Bureau publishes the population of every incorporated city
and town in the United States.</p>
</div>
<div id="populations_pareto" class="paragraph">
<p>image::figs/populations_pareto.png[CCDFs of city and town populations, on
a log-log scale.,height=240]</p>
</div>
<div class="paragraph">
<p>I downloaded their data from
<a href="http://www.census.gov/popest/data/cities/totals/2012/SUB-EST2012-3.html" class="bare">http://www.census.gov/popest/data/cities/totals/2012/SUB-EST2012-3.html</a>;
it is in the repository for this book in a file named
<code>PEP_2012_PEPANNRES_with_ann.csv</code>. The repository also contains
populations.py, which reads the file and plots the distribution of
populations.</p>
</div>
<div class="paragraph">
<p>Figure <a href="#populations_pareto">[populations_pareto]</a> shows the CCDF of
populations on a log-log scale. The largest 1% of cities and towns,
below \(10^{-2}\), fall along a straight line. So we could
conclude, as some researchers have, that the tail of this distribution
fits a Pareto model.</p>
</div>
<div class="paragraph">
<p>On the other hand, a lognormal distribution also models the data well.
Figure <a href="#populations_normal">[populations_normal]</a> shows the CDF of
populations and a lognormal model (left), and a normal probability plot
(right). Both plots show good agreement between the data and the model.</p>
</div>
<div class="paragraph">
<p>Neither model is perfect. The Pareto model only applies to the largest
1% of cities, but it is a better fit for that part of the distribution.
The lognormal model is a better fit for the other 99%. Which model is
appropriate depends on which part of the distribution is relevant.</p>
</div>
<div id="populations_normal" class="paragraph">
<p>image::figs/populations_normal.png[CDF of city and town populations on a
log-x scale (left), and normal probability plot of log-transformed
populations (right).,height=240]</p>
</div>
</div>
<div class="sect2">
<h3 id="_generating_random_numbers"><a class="anchor" href="#_generating_random_numbers"></a><a class="link" href="#_generating_random_numbers">5.6. Generating random numbers</a></h3>
<div class="paragraph">
<p>Analytic CDFs can be used to generate random numbers with a given
distribution function, \(p = \mathrm{CDF}(x)\). If there is an
efficient way to compute the inverse CDF, we can generate random values
with the appropriate distribution by choosing \(p\) from a
uniform distribution between 0 and 1, then choosing
\(x = ICDF(p)\).</p>
</div>
<div class="paragraph">
<p>For example, the CDF of the exponential distribution is</p>
</div>
<div class="stemblock">
<div class="content">
\[p = 1 - e^{-\lambda x}\]
</div>
</div>
<div class="paragraph">
<p>Solving for \(x\) yields:</p>
</div>
<div class="stemblock">
<div class="content">
\[x = -\log (1 - p) / \lambda\]
</div>
</div>
<div class="paragraph">
<p>So in Python we can write</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def expovariate(lam):
    p = random.random()
    x = -math.log(1-p) / lam
    return x</code></pre>
</div>
</div>
<div class="paragraph">
<p>expovariate takes lam and returns a random value chosen from the
exponential distribution with parameter lam.</p>
</div>
<div class="paragraph">
<p>Two notes about this implementation: I called the parameter <code>lam</code>
because <code>lambda</code> is a Python keyword. Also, since \(\log 0\)
is undefined, we have to be a little careful. The implementation of
random.random can return 0 but not 1, so \(1 - p\) can be 1 but
not 0, so log(1-p) is always defined.</p>
</div>
</div>
<div class="sect2">
<h3 id="_why_model"><a class="anchor" href="#_why_model"></a><a class="link" href="#_why_model">5.7. Why model?</a></h3>
<div class="paragraph">
<p>At the beginning of this chapter, I said that many real world phenomena
can be modeled with analytic distributions. &#8220;So,&#8221; you might ask,
&#8220;what?&#8221;</p>
</div>
<div class="paragraph">
<p>Like all models, analytic distributions are abstractions, which means
they leave out details that are considered irrelevant. For example, an
observed distribution might have measurement errors or quirks that are
specific to the sample; analytic models smooth out these idiosyncrasies.</p>
</div>
<div class="paragraph">
<p>Analytic models are also a form of data compression. When a model fits a
dataset well, a small set of parameters can summarize a large amount of
data.</p>
</div>
<div class="paragraph">
<p>It is sometimes surprising when data from a natural phenomenon fit an
analytic distribution, but these observations can provide insight into
physical systems. Sometimes we can explain why an observed distribution
has a particular form. For example, Pareto distributions are often the
result of generative processes with positive feedback (so-called
preferential attachment processes: see
<a href="http://wikipedia.org/wiki/Preferential_attachment." class="bare">http://wikipedia.org/wiki/Preferential_attachment.</a>).</p>
</div>
<div class="paragraph">
<p>Also, analytic distributions lend themselves to mathematical analysis,
as we will see in Chapter <a href="#analysis">14</a>.</p>
</div>
<div class="paragraph">
<p>But it is important to remember that all models are imperfect. Data from
the real world never fit an analytic distribution perfectly. People
sometimes talk as if data are generated by models; for example, they
might say that the distribution of human heights is normal, or the
distribution of income is lognormal. Taken literally, these claims
cannot be true; there are always differences between the real world and
mathematical models.</p>
</div>
<div class="paragraph">
<p>Models are useful if they capture the relevant aspects of the real world
and leave out unneeded details. But what is &#8220;relevant&#8221; or &#8220;unneeded&#8221;
depends on what you are planning to use the model for.</p>
</div>
</div>
<div class="sect2">
<h3 id="_exercises_5"><a class="anchor" href="#_exercises_5"></a><a class="link" href="#_exercises_5">5.8. Exercises</a></h3>
<div class="paragraph">
<p>For the following exercises, you can start with <code>chap05ex.ipynb</code>. My
solution is in <code>chap05soln.ipynb</code>.</p>
</div>
<div class="paragraph">
<p>In the BRFSS (see Section <a href="#lognormal">5.4</a>), the
distribution of heights is roughly normal with parameters
\(\mu = 178\) cm and \(\sigma = 7.7\) cm for men, and
\(\mu = 163\) cm and \(\sigma = 7.3\) cm for women.</p>
</div>
<div class="paragraph">
<p>In order to join Blue Man Group, you have to be male between 5’10” and
6’1” (see <a href="http://bluemancasting.com" class="bare">http://bluemancasting.com</a>). What percentage of the U.S. male
population is in this range? Hint: use scipy.stats.norm.cdf.</p>
</div>
<div class="paragraph">
<p>To get a feel for the Pareto distribution, let’s see how different the
world would be if the distribution of human height were Pareto. With the
parameters \(x_{m} = 1\) m and \(\alpha = 1.7\), we get
a distribution with a reasonable minimum, 1 m, and median, 1.5 m.</p>
</div>
<div class="paragraph">
<p>Plot this distribution. What is the mean human height in Pareto world?
What fraction of the population is shorter than the mean? If there are 7
billion people in Pareto world, how many do we expect to be taller than
1 km? How tall do we expect the tallest person to be?</p>
</div>
<div id="weibull" class="paragraph">
<p>The Weibull distribution is a generalization of the exponential
distribution that comes up in failure analysis (see
<a href="http://wikipedia.org/wiki/Weibull_distribution" class="bare">http://wikipedia.org/wiki/Weibull_distribution</a>). Its CDF is</p>
</div>
<div class="stemblock">
<div class="content">
\[CDF(x) = 1 - e^{-(x / \lambda)^k}\]
</div>
</div>
<div class="paragraph">
<p>Can you find a transformation that makes a Weibull distribution look
like a straight line? What do the slope and intercept of the line
indicate?</p>
</div>
<div class="paragraph">
<p>Use random.weibullvariate to generate a sample from a Weibull
distribution and use it to test your transformation.</p>
</div>
<div class="paragraph">
<p>For small values of \(n\), we don’t expect an empirical
distribution to fit an analytic distribution exactly. One way to
evaluate the quality of fit is to generate a sample from an analytic
distribution and see how well it matches the data.</p>
</div>
<div class="paragraph">
<p>For example, in Section <a href="#exponential">5.1</a> we plotted the
distribution of time between births and saw that it is approximately
exponential. But the distribution is based on only 44 data points. To
see whether the data might have come from an exponential distribution,
generate 44 values from an exponential distribution with the same mean
as the data, about 33 minutes between births.</p>
</div>
<div class="paragraph">
<p>Plot the distribution of the random values and compare it to the actual
distribution. You can use random.expovariate to generate the values.</p>
</div>
<div class="paragraph">
<p>In the repository for this book, you’ll find a set of data files called
mystery0.dat, mystery1.dat, and so on. Each contains a sequence of
random numbers generated from an analytic distribution.</p>
</div>
<div class="paragraph">
<p>You will also find <code>test_models.py</code>, a script that reads data from a
file and plots the CDF under a variety of transforms. You can run it
like this:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>$ python test_models.py mystery0.dat</pre>
</div>
</div>
<div class="paragraph">
<p>Based on these plots, you should be able to infer what kind of
distribution generated each file. If you are stumped, you can look in
mystery.py, which contains the code that generated the files.</p>
</div>
<div id="income" class="paragraph">
<p>The distributions of wealth and income are sometimes modeled using
lognormal and Pareto distributions. To see which is better, let’s look
at some data.</p>
</div>
<div class="paragraph">
<p>The Current Population Survey (CPS) is a joint effort of the Bureau of
Labor Statistics and the Census Bureau to study income and related
variables. Data collected in 2013 is available from
<a href="http://www.census.gov/hhes/www/cpstables/032013/hhinc/toc.htm" class="bare">http://www.census.gov/hhes/www/cpstables/032013/hhinc/toc.htm</a>. I
downloaded hinc06.xls, which is an Excel spreadsheet with information
about household income, and converted it to hinc06.csv, a CSV file you
will find in the repository for this book. You will also find hinc.py,
which reads this file.</p>
</div>
<div class="paragraph">
<p>Extract the distribution of incomes from this dataset. Are any of the
analytic distributions in this chapter a good model of the data? A
solution to this exercise is in hinc_soln.py.</p>
</div>
</div>
<div class="sect2">
<h3 id="_glossary_5"><a class="anchor" href="#_glossary_5"></a><a class="link" href="#_glossary_5">5.9. Glossary</a></h3>
<div class="ulist">
<ul>
<li>
<p>empirical distribution: The distribution of values in a sample.</p>
</li>
<li>
<p>analytic distribution: A distribution whose CDF is an analytic
function.</p>
</li>
<li>
<p>model: A useful simplification. Analytic distributions are often good
models of more complex empirical distributions.</p>
</li>
<li>
<p>interarrival time: The elapsed time between two events.</p>
</li>
<li>
<p>complementary CDF: A function that maps from a value, \(x\),
to the fraction of values that exceed \(x\), which is
\(1 - \mathrm{CDF}(x)\).</p>
</li>
<li>
<p>standard normal distribution: The normal distribution with mean 0 and
standard deviation 1.</p>
</li>
<li>
<p>normal probability plot: A plot of the values in a sample versus
random values from a standard normal distribution.</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="density"><a class="anchor" href="#density"></a><a class="link" href="#density">6. Probability density functions</a></h2>
<div class="sectionbody">
<div class="paragraph">
<p>The code for this chapter is in density.py. For information about
downloading and working with this code, see Section <a href="#code">Using the code</a>.</p>
</div>
<div class="sect2">
<h3 id="_pdfs"><a class="anchor" href="#_pdfs"></a><a class="link" href="#_pdfs">6.1. PDFs</a></h3>
<div class="paragraph">
<p>The derivative of a CDF is called a <strong>probability density function</strong>, or
PDF. For example, the PDF of an exponential distribution is</p>
</div>
<div class="stemblock">
<div class="content">
\[\mathrm{PDF}_{expo}(x) = \lambda e^{-\lambda x}\]
</div>
</div>
<div class="paragraph">
<p>The PDF of a normal distribution is</p>
</div>
<div class="stemblock">
<div class="content">
\[\mathrm{PDF}_{normal}(x) = \frac{1}{\sigma \sqrt{2 \pi}}
                 \exp \left[ -\frac{1}{2}
                 \left( \frac{x - \mu}{\sigma} \right)^2 \right]\]
</div>
</div>
<div class="paragraph">
<p>Evaluating a PDF for a particular value of \(x\) is usually not
useful. The result is not a probability; it is a probability <em>density</em>.</p>
</div>
<div class="paragraph">
<p>In physics, density is mass per unit of volume; in order to get a mass,
you have to multiply by volume or, if the density is not constant, you
have to integrate over volume.</p>
</div>
<div class="paragraph">
<p>Similarly, <strong>probability density</strong> measures probability per unit of
\(x\). In order to get a probability mass, you have to integrate
over \(x\).</p>
</div>
<div class="paragraph">
<p>thinkstats2 provides a class called Pdf that represents a probability
density function. Every Pdf object provides the following methods:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Density, which takes a value, x, and returns the density of the
distribution at x.</p>
</li>
<li>
<p>Render, which evaluates the density at a discrete set of values and
returns a pair of sequences: the sorted values, xs, and their
probability densities, ds.</p>
</li>
<li>
<p>MakePmf, which evaluates Density at a discrete set of values and
returns a normalized Pmf that approximates the Pdf.</p>
</li>
<li>
<p>GetLinspace, which returns the default set of points used by Render
and MakePmf.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Pdf is an abstract parent class, which means you should not instantiate
it; that is, you cannot create a Pdf object. Instead, you should define
a child class that inherits from Pdf and provides definitions of Density
and GetLinspace. Pdf provides Render and MakePmf.</p>
</div>
<div class="paragraph">
<p>For example, thinkstats2 provides a class named NormalPdf that evaluates
the normal density function.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">class NormalPdf(Pdf):

    def __init__(self, mu=0, sigma=1, label=''):
        self.mu = mu
        self.sigma = sigma
        self.label = label

    def Density(self, xs):
        return scipy.stats.norm.pdf(xs, self.mu, self.sigma)

    def GetLinspace(self):
        low, high = self.mu-3*self.sigma, self.mu+3*self.sigma
        return np.linspace(low, high, 101)</code></pre>
</div>
</div>
<div class="paragraph">
<p>The NormalPdf object contains the parameters mu and sigma. Density uses
scipy.stats.norm, which is an object that represents a normal
distribution and provides cdf and pdf, among other methods (see
Section <a href="#normal">5.2</a>).</p>
</div>
<div class="paragraph">
<p>The following example creates a NormalPdf with the mean and variance of
adult female heights, in cm, from the BRFSS (see
Section <a href="#lognormal">5.4</a>). Then it computes the density of the
distribution at a location one standard deviation from the mean.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; mean, var = 163, 52.8
&gt;&gt;&gt; std = math.sqrt(var)
&gt;&gt;&gt; pdf = thinkstats2.NormalPdf(mean, std)
&gt;&gt;&gt; pdf.Density(mean + std)
0.0333001</pre>
</div>
</div>
<div class="paragraph">
<p>The result is about 0.03, in units of probability mass per cm. Again, a
probability density doesn’t mean much by itself. But if we plot the Pdf,
we can see the shape of the distribution:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; thinkplot.Pdf(pdf, label='normal')
&gt;&gt;&gt; thinkplot.Show()</pre>
</div>
</div>
<div class="paragraph">
<p>thinkplot.Pdf plots the Pdf as a smooth function, as contrasted with
thinkplot.Pmf, which renders a Pmf as a step function.
Figure <a href="#pdf_example">[pdf_example]</a> shows the result, as well as a
PDF estimated from a sample, which we’ll compute in the next section.</p>
</div>
<div class="paragraph">
<p>You can use MakePmf to approximate the Pdf:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; pmf = pdf.MakePmf()</pre>
</div>
</div>
<div class="paragraph">
<p>By default, the resulting Pmf contains 101 points equally spaced from mu
- 3*sigma to mu + 3*sigma. Optionally, MakePmf and Render can take
keyword arguments low, high, and n.</p>
</div>
<div id="pdf_example" class="paragraph">
<p>image::figs/pdf_example.png[A normal PDF that models adult female height
in the U.S., and the kernel density estimate of a sample with
\(n=500\).,height=211]</p>
</div>
</div>
<div class="sect2">
<h3 id="_kernel_density_estimation"><a class="anchor" href="#_kernel_density_estimation"></a><a class="link" href="#_kernel_density_estimation">6.2. Kernel density estimation</a></h3>
<div class="paragraph">
<p><strong>Kernel density estimation</strong> (KDE) is an algorithm that takes a sample
and finds an appropriately smooth PDF that fits the data. You can read
details at <a href="http://en.wikipedia.org/wiki/Kernel_density_estimation" class="bare">http://en.wikipedia.org/wiki/Kernel_density_estimation</a>.</p>
</div>
<div class="paragraph">
<p>scipy provides an implementation of KDE and thinkstats2 provides a class
called EstimatedPdf that uses it:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">class EstimatedPdf(Pdf):

    def __init__(self, sample):
        self.kde = scipy.stats.gaussian_kde(sample)

    def Density(self, xs):
        return self.kde.evaluate(xs)</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>__init__</code> takes a sample and computes a kernel density estimate. The
result is a <code>gaussian_kde</code> object that provides an evaluate method.</p>
</div>
<div class="paragraph">
<p>Density takes a value or sequence, calls <code>gaussian_kde.evaluate</code>, and
returns the resulting density. The word &#8220;Gaussian&#8221; appears in the name
because it uses a filter based on a Gaussian distribution to smooth the
KDE.</p>
</div>
<div class="paragraph">
<p>Here’s an example that generates a sample from a normal distribution and
then makes an EstimatedPdf to fit it:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; sample = [random.gauss(mean, std) for i in range(500)]
&gt;&gt;&gt; sample_pdf = thinkstats2.EstimatedPdf(sample)
&gt;&gt;&gt; thinkplot.Pdf(sample_pdf, label='sample KDE')</pre>
</div>
</div>
<div class="paragraph">
<p><code>sample</code> is a list of 500 random heights. <code>sample_pdf</code> is a Pdf
object that contains the estimated KDE of the sample.</p>
</div>
<div class="paragraph">
<p>Figure <a href="#pdf_example">[pdf_example]</a> shows the normal density
function and a KDE based on a sample of 500 random heights. The estimate
is a good match for the original distribution.</p>
</div>
<div class="paragraph">
<p>Estimating a density function with KDE is useful for several purposes:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><em>Visualization:</em> During the exploration phase of a project, CDFs are
usually the best visualization of a distribution. After you look at a
CDF, you can decide whether an estimated PDF is an appropriate model of
the distribution. If so, it can be a better choice for presenting the
distribution to an audience that is unfamiliar with CDFs.</p>
</li>
<li>
<p><em>Interpolation:</em> An estimated PDF is a way to get from a sample to a
model of the population. If you have reason to believe that the
population distribution is smooth, you can use KDE to interpolate the
density for values that don’t appear in the sample.</p>
</li>
<li>
<p><em>Simulation:</em> Simulations are often based on the distribution of a
sample. If the sample size is small, it might be appropriate to smooth
the sample distribution using KDE, which allows the simulation to
explore more possible outcomes, rather than replicating the observed
data.</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_the_distribution_framework"><a class="anchor" href="#_the_distribution_framework"></a><a class="link" href="#_the_distribution_framework">6.3. The distribution framework</a></h3>
<div id="dist_framework" class="paragraph">
<p>image::figs/distribution_functions.png[A framework that relates
representations of distribution functions.,height=211]</p>
</div>
<div class="paragraph">
<p>At this point we have seen PMFs, CDFs and PDFs; let’s take a minute to
review. Figure <a href="#dist_framework">[dist_framework]</a> shows how these
functions relate to each other.</p>
</div>
<div class="paragraph">
<p>We started with PMFs, which represent the probabilities for a discrete
set of values. To get from a PMF to a CDF, you add up the probability
masses to get cumulative probabilities. To get from a CDF back to a PMF,
you compute differences in cumulative probabilities. We’ll see the
implementation of these operations in the next few sections.</p>
</div>
<div class="paragraph">
<p>A PDF is the derivative of a continuous CDF; or, equivalently, a CDF is
the integral of a PDF. Remember that a PDF maps from values to
probability densities; to get a probability, you have to integrate.</p>
</div>
<div class="paragraph">
<p>To get from a discrete to a continuous distribution, you can perform
various kinds of smoothing. One form of smoothing is to assume that the
data come from an analytic continuous distribution (like exponential or
normal) and to estimate the parameters of that distribution. Another
option is kernel density estimation.</p>
</div>
<div class="paragraph">
<p>The opposite of smoothing is <strong>discretizing</strong>, or quantizing. If you
evaluate a PDF at discrete points, you can generate a PMF that is an
approximation of the PDF. You can get a better approximation using
numerical integration.</p>
</div>
<div class="paragraph">
<p>To distinguish between continuous and discrete CDFs, it might be better
for a discrete CDF to be a &#8220;cumulative mass function,&#8221; but as far as I
can tell no one uses that term.</p>
</div>
</div>
<div class="sect2">
<h3 id="_hist_implementation"><a class="anchor" href="#_hist_implementation"></a><a class="link" href="#_hist_implementation">6.4. Hist implementation</a></h3>
<div class="paragraph">
<p>At this point you should know how to use the basic types provided by
thinkstats2: Hist, Pmf, Cdf, and Pdf. The next few sections provide
details about how they are implemented. This material might help you use
these classes more effectively, but it is not strictly necessary.</p>
</div>
<div class="paragraph">
<p>Hist and Pmf inherit from a parent class called <code>_DictWrapper</code>. The
leading underscore indicates that this class is &#8220;internal;&#8221; that is,
it should not be used by code in other modules. The name indicates what
it is: a dictionary wrapper. Its primary attribute is d, the dictionary
that maps from values to their frequencies.</p>
</div>
<div class="paragraph">
<p>The values can be any hashable type. The frequencies should be integers,
but can be any numeric type.</p>
</div>
<div class="paragraph">
<p><code>_DictWrapper</code> contains methods appropriate for both Hist and Pmf,
including <code>__init__</code>, Values, Items and Render. It also provides
modifier methods Set, Incr, Mult, and Remove. These methods are all
implemented with dictionary operations. For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python"># class _DictWrapper

    def Incr(self, x, term=1):
        self.d[x] = self.d.get(x, 0) + term

    def Mult(self, x, factor):
        self.d[x] = self.d.get(x, 0) * factor

    def Remove(self, x):
        del self.d[x]</code></pre>
</div>
</div>
<div class="paragraph">
<p>Hist also provides Freq, which looks up the frequency of a given value.</p>
</div>
<div class="paragraph">
<p>Because Hist operators and methods are based on dictionaries, these
methods are constant time operations; that is, their run time does not
increase as the Hist gets bigger.</p>
</div>
</div>
<div class="sect2">
<h3 id="_pmf_implementation"><a class="anchor" href="#_pmf_implementation"></a><a class="link" href="#_pmf_implementation">6.5. Pmf implementation</a></h3>
<div class="paragraph">
<p>Pmf and Hist are almost the same thing, except that a Pmf maps values to
floating-point probabilities, rather than integer frequencies. If the
sum of the probabilities is 1, the Pmf is normalized.</p>
</div>
<div class="paragraph">
<p>Pmf provides Normalize, which computes the sum of the probabilities and
divides through by a factor:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python"># class Pmf

    def Normalize(self, fraction=1.0):
        total = self.Total()
        if total == 0.0:
            raise ValueError('Total probability is zero.')

        factor = float(fraction) / total
        for x in self.d:
            self.d[x] *= factor

        return total</code></pre>
</div>
</div>
<div class="paragraph">
<p>fraction determines the sum of the probabilities after normalizing; the
default value is 1. If the total probability is 0, the Pmf cannot be
normalized, so Normalize raises ValueError.</p>
</div>
<div class="paragraph">
<p>Hist and Pmf have the same constructor. It can take as an argument a
dict, Hist, Pmf or Cdf, a pandas Series, a list of (value, frequency)
pairs, or a sequence of values.</p>
</div>
<div class="paragraph">
<p>If you instantiate a Pmf, the result is normalized. If you instantiate a
Hist, it is not. To construct an unnormalized Pmf, you can create an
empty Pmf and modify it. The Pmf modifiers do not renormalize the Pmf.</p>
</div>
</div>
<div class="sect2">
<h3 id="_cdf_implementation"><a class="anchor" href="#_cdf_implementation"></a><a class="link" href="#_cdf_implementation">6.6. Cdf implementation</a></h3>
<div class="paragraph">
<p>A CDF maps from values to cumulative probabilities, so I could have
implemented Cdf as a <code>_DictWrapper</code>. But the values in a CDF are
ordered and the values in a <code>_DictWrapper</code> are not. Also, it is often
useful to compute the inverse CDF; that is, the map from cumulative
probability to value. So the implementaion I chose is two sorted lists.
That way I can use binary search to do a forward or inverse lookup in
logarithmic time.</p>
</div>
<div class="paragraph">
<p>The Cdf constructor can take as a parameter a sequence of values or a
pandas Series, a dictionary that maps from values to probabilities, a
sequence of (value, probability) pairs, a Hist, Pmf, or Cdf. Or if it is
given two parameters, it treats them as a sorted sequence of values and
the sequence of corresponding cumulative probabilities.</p>
</div>
<div class="paragraph">
<p>Given a sequence, pandas Series, or dictionary, the constructor makes a
Hist. Then it uses the Hist to initialize the attributes:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">        self.xs, freqs = zip(*sorted(dw.Items()))
        self.ps = np.cumsum(freqs, dtype=np.float)
        self.ps /= self.ps[-1]</code></pre>
</div>
</div>
<div class="paragraph">
<p>xs is the sorted list of values; freqs is the list of corresponding
frequencies. np.cumsum computes the cumulative sum of the frequencies.
Dividing through by the total frequency yields cumulative probabilities.
For n values, the time to construct the Cdf is proportional to
\(n \log n\).</p>
</div>
<div class="paragraph">
<p>Here is the implementation of Prob, which takes a value and returns its
cumulative probability:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python"># class Cdf
    def Prob(self, x):
        if x &lt; self.xs[0]:
            return 0.0
        index = bisect.bisect(self.xs, x)
        p = self.ps[index - 1]
        return p</code></pre>
</div>
</div>
<div class="paragraph">
<p>The bisect module provides an implementation of binary search. And here
is the implementation of Value, which takes a cumulative probability and
returns the corresponding value:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python"># class Cdf
    def Value(self, p):
        if p &lt; 0 or p &gt; 1:
            raise ValueError('p must be in range [0, 1]')

        index = bisect.bisect_left(self.ps, p)
        return self.xs[index]</code></pre>
</div>
</div>
<div class="paragraph">
<p>Given a Cdf, we can compute the Pmf by computing differences between
consecutive cumulative probabilities. If you call the Cdf constructor
and pass a Pmf, it computes differences by calling Cdf.Items:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python"># class Cdf
    def Items(self):
        a = self.ps
        b = np.roll(a, 1)
        b[0] = 0
        return zip(self.xs, a-b)</code></pre>
</div>
</div>
<div class="paragraph">
<p>np.roll shifts the elements of a to the right, and &#8220;rolls&#8221; the last
one back to the beginning. We replace the first element of b with 0 and
then compute the difference a-b. The result is a NumPy array of
probabilities.</p>
</div>
<div class="paragraph">
<p>Cdf provides Shift and Scale, which modify the values in the Cdf, but
the probabilities should be treated as immutable.</p>
</div>
</div>
<div class="sect2">
<h3 id="_moments"><a class="anchor" href="#_moments"></a><a class="link" href="#_moments">6.7. Moments</a></h3>
<div class="paragraph">
<p>Any time you take a sample and reduce it to a single number, that number
is a statistic. The statistics we have seen so far include mean,
variance, median, and interquartile range.</p>
</div>
<div class="paragraph">
<p>A <strong>raw moment</strong> is a kind of statistic. If you have a sample of values,
\(x_i\), the \(k\)th raw moment is:</p>
</div>
<div class="stemblock">
<div class="content">
\[m'_k = \frac{1}{n} \sum_i x_i^k\]
</div>
</div>
<div class="paragraph">
<p>Or if you prefer Python notation:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def RawMoment(xs, k):
    return sum(x**k for x in xs) / len(xs)</code></pre>
</div>
</div>
<div class="paragraph">
<p>When \(k=1\) the result is the sample mean, \(\bar{x}\).
The other raw moments don’t mean much by themselves, but they are used
in some computations.</p>
</div>
<div class="paragraph">
<p>The <strong>central moments</strong> are more useful. The \(k\)th central
moment is:</p>
</div>
<div class="stemblock">
<div class="content">
\[m_k = \frac{1}{n} \sum_i (x_i - \bar{x})^k\]
</div>
</div>
<div class="paragraph">
<p>Or in Python:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def CentralMoment(xs, k):
    mean = RawMoment(xs, 1)
    return sum((x - mean)**k for x in xs) / len(xs)</code></pre>
</div>
</div>
<div class="paragraph">
<p>When \(k=2\) the result is the second central moment, which you
might recognize as variance. The definition of variance gives a hint
about why these statistics are called moments. If we attach a weight
along a ruler at each location, \(x_i\), and then spin the ruler
around the mean, the moment of inertia of the spinning weights is the
variance of the values. If you are not familiar with moment of inertia,
see <a href="http://en.wikipedia.org/wiki/Moment_of_inertia" class="bare">http://en.wikipedia.org/wiki/Moment_of_inertia</a>.</p>
</div>
<div class="paragraph">
<p>When you report moment-based statistics, it is important to think about
the units. For example, if the values \(x_i\) are in cm, the
first raw moment is also in cm. But the second moment is in
cm\(^2\), the third moment is in cm\(^3\), and so on.</p>
</div>
<div class="paragraph">
<p>Because of these units, moments are hard to interpret by themselves.
That’s why, for the second moment, it is common to report standard
deviation, which is the square root of variance, so it is in the same
units as \(x_i\).</p>
</div>
</div>
<div class="sect2">
<h3 id="_skewness"><a class="anchor" href="#_skewness"></a><a class="link" href="#_skewness">6.8. Skewness</a></h3>
<div class="paragraph">
<p><strong>Skewness</strong> is a property that describes the shape of a distribution. If
the distribution is symmetric around its central tendency, it is
unskewed. If the values extend farther to the right, it is &#8220;right
skewed&#8221; and if the values extend left, it is &#8220;left skewed.&#8221;</p>
</div>
<div class="paragraph">
<p>This use of &#8220;skewed&#8221; does not have the usual connotation of
&#8220;biased.&#8221; Skewness only describes the shape of the distribution; it
says nothing about whether the sampling process might have been biased.</p>
</div>
<div class="paragraph">
<p>Several statistics are commonly used to quantify the skewness of a
distribution. Given a sequence of values, \(x_i\), the <strong>sample
skewness</strong>, \(g_1\), can be computed like this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def StandardizedMoment(xs, k):
    var = CentralMoment(xs, 2)
    std = math.sqrt(var)
    return CentralMoment(xs, k) / std**k

def Skewness(xs):
    return StandardizedMoment(xs, 3)</code></pre>
</div>
</div>
<div class="paragraph">
<p>\(g_1\) is the third <strong>standardized moment</strong>, which means that it
has been normalized so it has no units.</p>
</div>
<div class="paragraph">
<p>Negative skewness indicates that a distribution skews left; positive
skewness indicates that a distribution skews right. The magnitude of
\(g_1\) indicates the strength of the skewness, but by itself it
is not easy to interpret.</p>
</div>
<div class="paragraph">
<p>In practice, computing sample skewness is usually not a good idea. If
there are any outliers, they have a disproportionate effect on
\(g_1\).</p>
</div>
<div class="paragraph">
<p>Another way to evaluate the asymmetry of a distribution is to look at
the relationship between the mean and median. Extreme values have more
effect on the mean than the median, so in a distribution that skews
left, the mean is less than the median. In a distribution that skews
right, the mean is greater.</p>
</div>
<div class="paragraph">
<p><strong>Pearson’s median skewness coefficient</strong> is a measure of skewness based
on the difference between the sample mean and median:</p>
</div>
<div class="stemblock">
<div class="content">
\[g_p = 3 (\bar{x}- m) / S\]
</div>
</div>
<div class="paragraph">
<p>Where \(\bar{x}\) is the sample mean, \(m\) is the
median, and \(S\) is the standard deviation. Or in Python:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def Median(xs):
    cdf = thinkstats2.Cdf(xs)
    return cdf.Value(0.5)

def PearsonMedianSkewness(xs):
    median = Median(xs)
    mean = RawMoment(xs, 1)
    var = CentralMoment(xs, 2)
    std = math.sqrt(var)
    gp = 3 * (mean - median) / std
    return gp</code></pre>
</div>
</div>
<div class="paragraph">
<p>This statistic is <strong>robust</strong>, which means that it is less vulnerable to
the effect of outliers.</p>
</div>
<div id="density_totalwgt_kde" class="paragraph">
<p>image::figs/density_totalwgt_kde.png[Estimated PDF of birthweight data
from the NSFG.,height=211]</p>
</div>
<div class="paragraph">
<p>As an example, let’s look at the skewness of birth weights in the NSFG
pregnancy data. Here’s the code to estimate and plot the PDF:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    live, firsts, others = first.MakeFrames()
    data = live.totalwgt_lb.dropna()
    pdf = thinkstats2.EstimatedPdf(data)
    thinkplot.Pdf(pdf, label='birth weight')</code></pre>
</div>
</div>
<div class="paragraph">
<p>Figure <a href="#density_totalwgt_kde">[density_totalwgt_kde]</a> shows the
result. The left tail appears longer than the right, so we suspect the
distribution is skewed left. The mean, 7.27 lbs, is a bit less than the
median, 7.38 lbs, so that is consistent with left skew. And both
skewness coefficients are negative: sample skewness is -0.59; Pearson’s
median skewness is -0.23.</p>
</div>
<div id="density_wtkg2_kde" class="paragraph">
<p>image::figs/density_wtkg2_kde.png[Estimated PDF of adult weight data from
the BRFSS.,height=211]</p>
</div>
<div class="paragraph">
<p>Now let’s compare this distribution to the distribution of adult weight
in the BRFSS. Again, here’s the code:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    df = brfss.ReadBrfss(nrows=None)
    data = df.wtkg2.dropna()
    pdf = thinkstats2.EstimatedPdf(data)
    thinkplot.Pdf(pdf, label='adult weight')</code></pre>
</div>
</div>
<div class="paragraph">
<p>Figure <a href="#density_wtkg2_kde">[density_wtkg2_kde]</a> shows the result.
The distribution appears skewed to the right. Sure enough, the mean,
79.0, is bigger than the median, 77.3. The sample skewness is 1.1 and
Pearson’s median skewness is 0.26.</p>
</div>
<div class="paragraph">
<p>The sign of the skewness coefficient indicates whether the distribution
skews left or right, but other than that, they are hard to interpret.
Sample skewness is less robust; that is, it is more susceptible to
outliers. As a result it is less reliable when applied to skewed
distributions, exactly when it would be most relevant.</p>
</div>
<div class="paragraph">
<p>Pearson’s median skewness is based on a computed mean and variance, so
it is also susceptible to outliers, but since it does not depend on a
third moment, it is somewhat more robust.</p>
</div>
</div>
<div class="sect2">
<h3 id="_exercises_6"><a class="anchor" href="#_exercises_6"></a><a class="link" href="#_exercises_6">6.9. Exercises</a></h3>
<div class="paragraph">
<p>A solution to this exercise is in <code>chap06soln.py</code>.</p>
</div>
<div class="paragraph">
<p>The distribution of income is famously skewed to the right. In this
exercise, we’ll measure how strong that skew is.</p>
</div>
<div class="paragraph">
<p>The Current Population Survey (CPS) is a joint effort of the Bureau of
Labor Statistics and the Census Bureau to study income and related
variables. Data collected in 2013 is available from
<a href="http://www.census.gov/hhes/www/cpstables/032013/hhinc/toc.htm" class="bare">http://www.census.gov/hhes/www/cpstables/032013/hhinc/toc.htm</a>. I
downloaded hinc06.xls, which is an Excel spreadsheet with information
about household income, and converted it to hinc06.csv, a CSV file you
will find in the repository for this book. You will also find hinc2.py,
which reads this file and transforms the data.</p>
</div>
<div class="paragraph">
<p>The dataset is in the form of a series of income ranges and the number
of respondents who fell in each range. The lowest range includes
respondents who reported annual household income &#8220;Under $5000.&#8221; The
highest range includes respondents who made &#8220;$250,000 or more.&#8221;</p>
</div>
<div class="paragraph">
<p>To estimate mean and other statistics from these data, we have to make
some assumptions about the lower and upper bounds, and how the values
are distributed in each range. hinc2.py provides InterpolateSample,
which shows one way to model this data. It takes a DataFrame with a
column, income, that contains the upper bound of each range, and freq,
which contains the number of respondents in each frame.</p>
</div>
<div class="paragraph">
<p>It also takes <code>log_upper</code>, which is an assumed upper bound on the
highest range, expressed in log10 dollars. The default value,
<code>log_upper=6.0</code> represents the assumption that the largest income
among the respondents is \(10^6\), or one million dollars.</p>
</div>
<div class="paragraph">
<p>InterpolateSample generates a pseudo-sample; that is, a sample of
household incomes that yields the same number of respondents in each
range as the actual data. It assumes that incomes in each range are
equally spaced on a log10 scale.</p>
</div>
<div class="paragraph">
<p>Compute the median, mean, skewness and Pearson’s skewness of the
resulting sample. What fraction of households reports a taxable income
below the mean? How do the results depend on the assumed upper bound?</p>
</div>
</div>
<div class="sect2">
<h3 id="_glossary_6"><a class="anchor" href="#_glossary_6"></a><a class="link" href="#_glossary_6">6.10. Glossary</a></h3>
<div class="ulist">
<ul>
<li>
<p>Probability density function (PDF): The derivative of a continuous
CDF, a function that maps a value to its probability density.</p>
</li>
<li>
<p>Probability density: A quantity that can be integrated over a range of
values to yield a probability. If the values are in units of cm, for
example, probability density is in units of probability per cm.</p>
</li>
<li>
<p>Kernel density estimation (KDE): An algorithm that estimates a PDF
based on a sample.</p>
</li>
<li>
<p>discretize: To approximate a continuous function or distribution with
a discrete function. The opposite of smoothing.</p>
</li>
<li>
<p>raw moment: A statistic based on the sum of data raised to a power.</p>
</li>
<li>
<p>central moment: A statistic based on deviation from the mean, raised
to a power.</p>
</li>
<li>
<p>standardized moment: A ratio of moments that has no units.</p>
</li>
<li>
<p>skewness: A measure of how asymmetric a distribution is.</p>
</li>
<li>
<p>sample skewness: A moment-based statistic intended to quantify the
skewness of a distribution.</p>
</li>
<li>
<p>Pearson’s median skewness coefficient: A statistic intended to
quantify the skewness of a distribution based on the median, mean, and
standard deviation.</p>
</li>
<li>
<p>robust: A statistic is robust if it is relatively immune to the effect
of outliers.</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_relationships_between_variables"><a class="anchor" href="#_relationships_between_variables"></a><a class="link" href="#_relationships_between_variables">7. Relationships between variables</a></h2>
<div class="sectionbody">
<div class="paragraph">
<p>So far we have only looked at one variable at a time. In this chapter we
look at relationships between variables. Two variables are related if
knowing one gives you information about the other. For example, height
and weight are related; people who are taller tend to be heavier. Of
course, it is not a perfect relationship: there are short heavy people
and tall light ones. But if you are trying to guess someone’s weight,
you will be more accurate if you know their height than if you don’t.</p>
</div>
<div class="paragraph">
<p>The code for this chapter is in scatter.py. For information about
downloading and working with this code, see Section <a href="#code">Using the code</a>.</p>
</div>
<div class="sect2">
<h3 id="_scatter_plots"><a class="anchor" href="#_scatter_plots"></a><a class="link" href="#_scatter_plots">7.1. Scatter plots</a></h3>
<div class="paragraph">
<p>The simplest way to check for a relationship between two variables is a
<strong>scatter plot</strong>, but making a good scatter plot is not always easy. As an
example, I’ll plot weight versus height for the respondents in the BRFSS
(see Section <a href="#lognormal">5.4</a>).</p>
</div>
<div class="paragraph">
<p>Here’s the code that reads the data file and extracts height and weight:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    df = brfss.ReadBrfss(nrows=None)
    sample = thinkstats2.SampleRows(df, 5000)
    heights, weights = sample.htm3, sample.wtkg2</code></pre>
</div>
</div>
<div class="paragraph">
<p>SampleRows chooses a random subset of the data:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def SampleRows(df, nrows, replace=False):
    indices = np.random.choice(df.index, nrows, replace=replace)
    sample = df.loc[indices]
    return sample</code></pre>
</div>
</div>
<div class="paragraph">
<p>df is the DataFrame, nrows is the number of rows to choose, and replace
is a boolean indicating whether sampling should be done with
replacement; in other words, whether the same row could be chosen more
than once.</p>
</div>
<div class="paragraph">
<p>thinkplot provides Scatter, which makes scatter plots:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    thinkplot.Scatter(heights, weights)
    thinkplot.Show(xlabel='Height (cm)',
                   ylabel='Weight (kg)',
                   axis=[140, 210, 20, 200])</code></pre>
</div>
</div>
<div class="paragraph">
<p>The result, in Figure <a href="#scatter1">[scatter1]</a> (left), shows the shape of the
relationship. As we expected, taller people tend to be heavier.</p>
</div>
<div class="paragraph">
<p>image::figs/scatter1.png[Scatter plots of weight versus height for the
respondents in the BRFSS, unjittered (left), jittered
(right).,height=288]</p>
</div>
<div class="paragraph">
<p>But this is not the best representation of the data, because the data
are packed into columns. The problem is that the heights are rounded to
the nearest inch, converted to centimeters, and then rounded again. Some
information is lost in translation.</p>
</div>
<div class="paragraph">
<p>We can’t get that information back, but we can minimize the effect on
the scatter plot by <strong>jittering</strong> the data, which means adding random
noise to reverse the effect of rounding off. Since these measurements
were rounded to the nearest inch, they might be off by up to 0.5 inches
or 1.3 cm. Similarly, the weights might be off by 0.5 kg.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    heights = thinkstats2.Jitter(heights, 1.3)
    weights = thinkstats2.Jitter(weights, 0.5)</code></pre>
</div>
</div>
<div class="paragraph">
<p>Here’s the implementation of Jitter:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def Jitter(values, jitter=0.5):
    n = len(values)
    return np.random.uniform(-jitter, +jitter, n) + values</code></pre>
</div>
</div>
<div class="paragraph">
<p>The values can be any sequence; the result is a NumPy array.</p>
</div>
<div class="paragraph">
<p>Figure <a href="#scatter1">[scatter1]</a> (right) shows the result. Jittering reduces
the visual effect of rounding and makes the shape of the relationship
clearer. But in general you should only jitter data for purposes of
visualization and avoid using jittered data for analysis.</p>
</div>
<div class="paragraph">
<p>Even with jittering, this is not the best way to represent the data.
There are many overlapping points, which hides data in the dense parts
of the figure and gives disproportionate emphasis to outliers. This
effect is called <strong>saturation</strong>.</p>
</div>
<div class="paragraph">
<p>image::figs/scatter2.png[Scatter plot with jittering and transparency
(left), hexbin plot (right).,height=288]</p>
</div>
<div class="paragraph">
<p>We can solve this problem with the alpha parameter, which makes the
points partly transparent:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    thinkplot.Scatter(heights, weights, alpha=0.2)</code></pre>
</div>
</div>
<div class="paragraph">
<p>Figure <a href="#scatter2">[scatter2]</a> (left) shows the result. Overlapping data
points look darker, so darkness is proportional to density. In this
version of the plot we can see two details that were not apparent
before: vertical clusters at several heights and a horizontal line near
90 kg or 200 pounds. Since this data is based on self-reports in pounds,
the most likely explanation is that some respondents reported rounded
values.</p>
</div>
<div class="paragraph">
<p>Using transparency works well for moderate-sized datasets, but this
figure only shows the first 5000 records in the BRFSS, out of a total of
414 509.</p>
</div>
<div class="paragraph">
<p>To handle larger datasets, another option is a hexbin plot, which
divides the graph into hexagonal bins and colors each bin according to
how many data points fall in it. thinkplot provides HexBin:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    thinkplot.HexBin(heights, weights)</code></pre>
</div>
</div>
<div class="paragraph">
<p>Figure <a href="#scatter2">[scatter2]</a> (right) shows the result. An advantage of a
hexbin is that it shows the shape of the relationship well, and it is
efficient for large datasets, both in time and in the size of the file
it generates. A drawback is that it makes the outliers invisible.</p>
</div>
<div class="paragraph">
<p>The point of this example is that it is not easy to make a scatter plot
that shows relationships clearly without introducing misleading
artifacts.</p>
</div>
</div>
<div class="sect2">
<h3 id="characterizing"><a class="anchor" href="#characterizing"></a><a class="link" href="#characterizing">7.2. Characterizing relationships</a></h3>
<div class="paragraph">
<p>Scatter plots provide a general impression of the relationship between
variables, but there are other visualizations that provide more insight
into the nature of the relationship. One option is to bin one variable
and plot percentiles of the other.</p>
</div>
<div class="paragraph">
<p>NumPy and pandas provide functions for binning data:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    df = df.dropna(subset=['htm3', 'wtkg2'])
    bins = np.arange(135, 210, 5)
    indices = np.digitize(df.htm3, bins)
    groups = df.groupby(indices)</code></pre>
</div>
</div>
<div class="paragraph">
<p>dropna drops rows with nan in any of the listed columns. arange makes a
NumPy array of bins from 135 to, but not including, 210, in increments
of 5.</p>
</div>
<div class="paragraph">
<p>digitize computes the index of the bin that contains each value in
df.htm3. The result is a NumPy array of integer indices. Values that
fall below the lowest bin are mapped to index 0. Values above the
highest bin are mapped to len(bins).</p>
</div>
<div class="paragraph">
<p>image::figs/scatter3.png[Percentiles of weight for a range of height
bins.,height=240]</p>
</div>
<div class="paragraph">
<p>groupby is a DataFrame method that returns a GroupBy object; used in a
for loop, groups iterates the names of the groups and the DataFrames
that represent them. So, for example, we can print the number of rows in
each group like this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">for i, group in groups:
    print(i, len(group))</code></pre>
</div>
</div>
<div class="paragraph">
<p>Now for each group we can compute the mean height and the CDF of weight:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    heights = [group.htm3.mean() for i, group in groups]
    cdfs = [thinkstats2.Cdf(group.wtkg2) for i, group in groups]</code></pre>
</div>
</div>
<div class="paragraph">
<p>Finally, we can plot percentiles of weight versus height:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    for percent in [75, 50, 25]:
        weights = [cdf.Percentile(percent) for cdf in cdfs]
        label = '%dth' % percent
        thinkplot.Plot(heights, weights, label=label)</code></pre>
</div>
</div>
<div class="paragraph">
<p>Figure <a href="#scatter3">[scatter3]</a> shows the result. Between 140 and 200 cm the
relationship between these variables is roughly linear. This range
includes more than 99% of the data, so we don’t have to worry too much
about the extremes.</p>
</div>
</div>
<div class="sect2">
<h3 id="_correlation"><a class="anchor" href="#_correlation"></a><a class="link" href="#_correlation">7.3. Correlation</a></h3>
<div class="paragraph">
<p>A <strong>correlation</strong> is a statistic intended to quantify the strength of the
relationship between two variables.</p>
</div>
<div class="paragraph">
<p>A challenge in measuring correlation is that the variables we want to
compare are often not expressed in the same units. And even if they are
in the same units, they come from different distributions.</p>
</div>
<div class="paragraph">
<p>There are two common solutions to these problems:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Transform each value to a <strong>standard score</strong>, which is the number of
standard deviations from the mean. This transform leads to the &#8220;Pearson
product-moment correlation coefficient.&#8221;</p>
</li>
<li>
<p>Transform each value to its <strong>rank</strong>, which is its index in the sorted
list of values. This transform leads to the &#8220;Spearman rank correlation
coefficient.&#8221;</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>If \(X\) is a series of \(n\) values, \(x_i\),
we can convert to standard scores by subtracting the mean and dividing
by the standard deviation: \(z_i = (x_i - \mu) / \sigma\).</p>
</div>
<div class="paragraph">
<p>The numerator is a deviation: the distance from the mean. Dividing by
\(\sigma\) <strong>standardizes</strong> the deviation, so the values of
\(Z\) are dimensionless (no units) and their distribution has
mean 0 and variance 1.</p>
</div>
<div class="paragraph">
<p>If \(X\) is normally distributed, so is \(Z\). But if
\(X\) is skewed or has outliers, so does \(Z\); in those
cases, it is more robust to use percentile ranks. If we compute a new
variable, \(R\), so that \(r_i\) is the rank of
\(x_i\), the distribution of \(R\) is uniform from 1 to
\(n\), regardless of the distribution of \(X\).</p>
</div>
</div>
<div class="sect2">
<h3 id="_covariance"><a class="anchor" href="#_covariance"></a><a class="link" href="#_covariance">7.4. Covariance</a></h3>
<div class="paragraph">
<p><strong>Covariance</strong> is a measure of the tendency of two variables to vary
together. If we have two series, \(X\) and \(Y\), their
deviations from the mean are</p>
</div>
<div class="stemblock">
<div class="content">
\[dx_i = x_i - \bar{x}\]
</div>
</div>
<div class="stemblock">
<div class="content">
\[dy_i = y_i - \bar{y}\]
</div>
</div>
<div class="paragraph">
<p>where \(\bar{x}\) is the sample mean of \(X\) and
\(\bar{y}\) is the sample mean of \(Y\). If
\(X\) and \(Y\) vary together, their deviations tend to
have the same sign.</p>
</div>
<div class="paragraph">
<p>If we multiply them together, the product is positive when the
deviations have the same sign and negative when they have the opposite
sign. So adding up the products gives a measure of the tendency to vary
together.</p>
</div>
<div class="paragraph">
<p>Covariance is the mean of these products:</p>
</div>
<div class="stemblock">
<div class="content">
\[Cov(X,Y) = \frac{1}{n} \sum dx_i~dy_i\]
</div>
</div>
<div class="paragraph">
<p>where \(n\) is the length of the two series (they have to be the
same length).</p>
</div>
<div class="paragraph">
<p>If you have studied linear algebra, you might recognize that Cov is the
dot product of the deviations, divided by their length. So the
covariance is maximized if the two vectors are identical, 0 if they are
orthogonal, and negative if they point in opposite directions.
thinkstats2 uses np.dot to implement Cov efficiently:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def Cov(xs, ys, meanx=None, meany=None):
    xs = np.asarray(xs)
    ys = np.asarray(ys)

    if meanx is None:
        meanx = np.mean(xs)
    if meany is None:
        meany = np.mean(ys)

    cov = np.dot(xs-meanx, ys-meany) / len(xs)
    return cov</code></pre>
</div>
</div>
<div class="paragraph">
<p>By default Cov computes deviations from the sample means, or you can
provide known means. If xs and ys are Python sequences, np.asarray
converts them to NumPy arrays. If they are already NumPy arrays,
np.asarray does nothing.</p>
</div>
<div class="paragraph">
<p>This implementation of covariance is meant to be simple for purposes of
explanation. NumPy and pandas also provide implementations of
covariance, but both of them apply a correction for small sample sizes
that we have not covered yet, and np.cov returns a covariance matrix,
which is more than we need for now.</p>
</div>
</div>
<div class="sect2">
<h3 id="_pearson_s_correlation"><a class="anchor" href="#_pearson_s_correlation"></a><a class="link" href="#_pearson_s_correlation">7.5. Pearson’s correlation</a></h3>
<div class="paragraph">
<p>Covariance is useful in some computations, but it is seldom reported as
a summary statistic because it is hard to interpret. Among other
problems, its units are the product of the units of \(X\) and
\(Y\). For example, the covariance of weight and height in the
BRFSS dataset is 113 kilogram-centimeters, whatever that means.</p>
</div>
<div class="paragraph">
<p>One solution to this problem is to divide the deviations by the standard
deviation, which yields standard scores, and compute the product of
standard scores:</p>
</div>
<div class="stemblock">
<div class="content">
\[p_i = \frac{(x_i - \bar{x})}{S_X} \frac{(y_i - \bar{y})}{S_Y}\]
</div>
</div>
<div class="paragraph">
<p>Where \(S_X\) and \(S_Y\) are the standard deviations of
\(X\) and \(Y\). The mean of these products is</p>
</div>
<div class="stemblock">
<div class="content">
\[\rho = \frac{1}{n} \sum p_i\]
</div>
</div>
<div class="paragraph">
<p>Or we can rewrite \(\rho\) by factoring out \(S_X\) and
\(S_Y\):</p>
</div>
<div class="stemblock">
<div class="content">
\[\rho = \frac{Cov(X,Y)}{S_X S_Y}\]
</div>
</div>
<div class="paragraph">
<p>This value is called <strong>Pearson’s correlation</strong> after Karl Pearson, an
influential early statistician. It is easy to compute and easy to
interpret. Because standard scores are dimensionless, so is
\(\rho\).</p>
</div>
<div class="paragraph">
<p>Here is the implementation in thinkstats2:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def Corr(xs, ys):
    xs = np.asarray(xs)
    ys = np.asarray(ys)

    meanx, varx = MeanVar(xs)
    meany, vary = MeanVar(ys)

    corr = Cov(xs, ys, meanx, meany) / math.sqrt(varx * vary)
    return corr</code></pre>
</div>
</div>
<div class="paragraph">
<p>MeanVar computes mean and variance slightly more efficiently than
separate calls to np.mean and np.var.</p>
</div>
<div class="paragraph">
<p>Pearson’s correlation is always between -1 and +1 (including both). If
\(\rho\) is positive, we say that the correlation is positive,
which means that when one variable is high, the other tends to be high.
If \(\rho\) is negative, the correlation is negative, so when
one variable is high, the other is low.</p>
</div>
<div class="paragraph">
<p>The magnitude of \(\rho\) indicates the strength of the
correlation. If \(\rho\) is 1 or -1, the variables are perfectly
correlated, which means that if you know one, you can make a perfect
prediction about the other.</p>
</div>
<div class="paragraph">
<p>Most correlation in the real world is not perfect, but it is still
useful. The correlation of height and weight is 0.51, which is a strong
correlation compared to similar human-related variables.</p>
</div>
</div>
<div class="sect2">
<h3 id="_nonlinear_relationships"><a class="anchor" href="#_nonlinear_relationships"></a><a class="link" href="#_nonlinear_relationships">7.6. Nonlinear relationships</a></h3>
<div class="paragraph">
<p>If Pearson’s correlation is near 0, it is tempting to conclude that
there is no relationship between the variables, but that conclusion is
not valid. Pearson’s correlation only measures <em>linear</em> relationships.
If there’s a nonlinear relationship, \(\rho\) understates its
strength.</p>
</div>
<div id="corr_examples" class="paragraph">
<p>image::figs/Correlation_examples.png[Examples of datasets with a range of
correlations.,height=240]</p>
</div>
<div class="paragraph">
<p>Figure <a href="#corr_examples">[corr_examples]</a> is from
<a href="http://wikipedia.org/wiki/Correlation_and_dependence" class="bare">http://wikipedia.org/wiki/Correlation_and_dependence</a>. It shows scatter
plots and correlation coefficients for several carefully constructed
datasets.</p>
</div>
<div class="paragraph">
<p>The top row shows linear relationships with a range of correlations; you
can use this row to get a sense of what different values of
\(\rho\) look like. The second row shows perfect correlations
with a range of slopes, which demonstrates that correlation is unrelated
to slope (we’ll talk about estimating slope soon). The third row shows
variables that are clearly related, but because the relationship is
nonlinear, the correlation coefficient is 0.</p>
</div>
<div class="paragraph">
<p>The moral of this story is that you should always look at a scatter plot
of your data before blindly computing a correlation coefficient.</p>
</div>
</div>
<div class="sect2">
<h3 id="_spearman_s_rank_correlation"><a class="anchor" href="#_spearman_s_rank_correlation"></a><a class="link" href="#_spearman_s_rank_correlation">7.7. Spearman’s rank correlation</a></h3>
<div class="paragraph">
<p>Pearson’s correlation works well if the relationship between variables
is linear and if the variables are roughly normal. But it is not robust
in the presence of outliers. Spearman’s rank correlation is an
alternative that mitigates the effect of outliers and skewed
distributions. To compute Spearman’s correlation, we have to compute the
<strong>rank</strong> of each value, which is its index in the sorted sample. For
example, in the sample the rank of the value 5 is 3, because it appears
third in the sorted list. Then we compute Pearson’s correlation for the
ranks.</p>
</div>
<div class="paragraph">
<p>thinkstats2 provides a function that computes Spearman’s rank
correlation:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def SpearmanCorr(xs, ys):
    xranks = pandas.Series(xs).rank()
    yranks = pandas.Series(ys).rank()
    return Corr(xranks, yranks)</code></pre>
</div>
</div>
<div class="paragraph">
<p>I convert the arguments to pandas Series objects so I can use rank,
which computes the rank for each value and returns a Series. Then I use
Corr to compute the correlation of the ranks.</p>
</div>
<div class="paragraph">
<p>I could also use Series.corr directly and specify Spearman’s method:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def SpearmanCorr(xs, ys):
    xs = pandas.Series(xs)
    ys = pandas.Series(ys)
    return xs.corr(ys, method='spearman')</code></pre>
</div>
</div>
<div class="paragraph">
<p>The Spearman rank correlation for the BRFSS data is 0.54, a little
higher than the Pearson correlation, 0.51. There are several possible
reasons for the difference, including:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>If the relationship is nonlinear, Pearson’s correlation tends to
underestimate the strength of the relationship, and</p>
</li>
<li>
<p>Pearson’s correlation can be affected (in either direction) if one of
the distributions is skewed or contains outliers. Spearman’s rank
correlation is more robust.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>In the BRFSS example, we know that the distribution of weights is
roughly lognormal; under a log transform it approximates a normal
distribution, so it has no skew. So another way to eliminate the effect
of skewness is to compute Pearson’s correlation with log-weight and
height:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    thinkstats2.Corr(df.htm3, np.log(df.wtkg2)))</code></pre>
</div>
</div>
<div class="paragraph">
<p>The result is 0.53, close to the rank correlation, 0.54. So that
suggests that skewness in the distribution of weight explains most of
the difference between Pearson’s and Spearman’s correlation.</p>
</div>
</div>
<div class="sect2">
<h3 id="_correlation_and_causation"><a class="anchor" href="#_correlation_and_causation"></a><a class="link" href="#_correlation_and_causation">7.8. Correlation and causation</a></h3>
<div class="paragraph">
<p>If variables A and B are correlated, there are three possible
explanations: A causes B, or B causes A, or some other set of factors
causes both A and B. These explanations are called &#8220;causal
relationships&#8221;.</p>
</div>
<div class="paragraph">
<p>Correlation alone does not distinguish between these explanations, so it
does not tell you which ones are true. This rule is often summarized
with the phrase &#8220;Correlation does not imply causation,&#8221; which is so
pithy it has its own Wikipedia page:
<a href="http://wikipedia.org/wiki/Correlation_does_not_imply_causation" class="bare">http://wikipedia.org/wiki/Correlation_does_not_imply_causation</a>.</p>
</div>
<div class="paragraph">
<p>So what can you do to provide evidence of causation?</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Use time. If A comes before B, then A can cause B but not the other
way around (at least according to our common understanding of
causation). The order of events can help us infer the direction of
causation, but it does not preclude the possibility that something else
causes both A and B.</p>
</li>
<li>
<p>Use randomness. If you divide a large sample into two groups at random
and compute the means of almost any variable, you expect the difference
to be small. If the groups are nearly identical in all variables but
one, you can eliminate spurious relationships.</p>
<div class="paragraph">
<p>This works even if you don’t know what the relevant variables are, but
it works even better if you do, because you can check that the groups
are identical.</p>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p>These ideas are the motivation for the <strong>randomized controlled trial</strong>, in
which subjects are assigned randomly to two (or more) groups: a
<strong>treatment group</strong> that receives some kind of intervention, like a new
medicine, and a <strong>control group</strong> that receives no intervention, or
another treatment whose effects are known.</p>
</div>
<div class="paragraph">
<p>A randomized controlled trial is the most reliable way to demonstrate a
causal relationship, and the foundation of science-based medicine (see
<a href="http://wikipedia.org/wiki/Randomized_controlled_trial" class="bare">http://wikipedia.org/wiki/Randomized_controlled_trial</a>).</p>
</div>
<div class="paragraph">
<p>Unfortunately, controlled trials are only possible in the laboratory
sciences, medicine, and a few other disciplines. In the social sciences,
controlled experiments are rare, usually because they are impossible or
unethical.</p>
</div>
<div class="paragraph">
<p>An alternative is to look for a <strong>natural experiment</strong>, where different
&#8220;treatments&#8221; are applied to groups that are otherwise similar. One
danger of natural experiments is that the groups might differ in ways
that are not apparent. You can read more about this topic at
<a href="http://wikipedia.org/wiki/Natural_experiment" class="bare">http://wikipedia.org/wiki/Natural_experiment</a>.</p>
</div>
<div class="paragraph">
<p>In some cases it is possible to infer causal relationships using
<strong>regression analysis</strong>, which is the topic of
Chapter <a href="#regression">[regression]</a>.</p>
</div>
</div>
<div class="sect2">
<h3 id="_exercises_7"><a class="anchor" href="#_exercises_7"></a><a class="link" href="#_exercises_7">7.9. Exercises</a></h3>
<div class="paragraph">
<p>A solution to this exercise is in <code>chap07soln.py</code>.</p>
</div>
<div class="paragraph">
<p>Using data from the NSFG, make a scatter plot of birth weight versus
mother’s age. Plot percentiles of birth weight versus mother’s age.
Compute Pearson’s and Spearman’s correlations. How would you
characterize the relationship between these variables?</p>
</div>
</div>
<div class="sect2">
<h3 id="_glossary_7"><a class="anchor" href="#_glossary_7"></a><a class="link" href="#_glossary_7">7.10. Glossary</a></h3>
<div class="ulist">
<ul>
<li>
<p>scatter plot: A visualization of the relationship between two
variables, showing one point for each row of data.</p>
</li>
<li>
<p>jitter: Random noise added to data for purposes of visualization.</p>
</li>
<li>
<p>saturation: Loss of information when multiple points are plotted on
top of each other.</p>
</li>
<li>
<p>correlation: A statistic that measures the strength of the
relationship between two variables.</p>
</li>
<li>
<p>standardize: To transform a set of values so that their mean is 0 and
their variance is 1.</p>
</li>
<li>
<p>standard score: A value that has been standardized so that it is
expressed in standard deviations from the mean.</p>
</li>
<li>
<p>covariance: A measure of the tendency of two variables to vary
together.</p>
</li>
<li>
<p>rank: The index where an element appears in a sorted list.</p>
</li>
<li>
<p>randomized controlled trial: An experimental design in which subjects
are divided into groups at random, and different groups are given
different treatments.</p>
</li>
<li>
<p>treatment group: A group in a controlled trial that receives some kind
of intervention.</p>
</li>
<li>
<p>control group: A group in a controlled trial that receives no
treatment, or a treatment whose effect is known.</p>
</li>
<li>
<p>natural experiment: An experimental design that takes advantage of a
natural division of subjects into groups in ways that are at least
approximately random.</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="estimation"><a class="anchor" href="#estimation"></a><a class="link" href="#estimation">8. Estimation</a></h2>
<div class="sectionbody">
<div class="paragraph">
<p>The code for this chapter is in estimation.py. For information about
downloading and working with this code, see Section <a href="#code">Using the code</a>.</p>
</div>
<div class="sect2">
<h3 id="_the_estimation_game"><a class="anchor" href="#_the_estimation_game"></a><a class="link" href="#_the_estimation_game">8.1. The estimation game</a></h3>
<div class="paragraph">
<p>Let’s play a game. I think of a distribution, and you have to guess what
it is. I’ll give you two hints: it’s a normal distribution, and here’s a
random sample drawn from it:</p>
</div>
<div class="paragraph">
<p>What do you think is the mean parameter, \(\mu\), of this
distribution?</p>
</div>
<div class="paragraph">
<p>One choice is to use the sample mean, \(\bar{x}\), as an
estimate of \(\mu\). In this example, \(\bar{x}\) is
0.155, so it would be reasonable to guess \(\mu\) = 0.155. This
process is called <strong>estimation</strong>, and the statistic we used (the sample
mean) is called an <strong>estimator</strong>.</p>
</div>
<div class="paragraph">
<p>Using the sample mean to estimate \(\mu\) is so obvious that it
is hard to imagine a reasonable alternative. But suppose we change the
game by introducing outliers.</p>
</div>
<div class="paragraph">
<p><em>I’m thinking of a distribution.</em> It’s a normal distribution, and here’s
a sample that was collected by an unreliable surveyor who occasionally
puts the decimal point in the wrong place.</p>
</div>
<div class="paragraph">
<p>Now what’s your estimate of \(\mu\)? If you use the sample mean,
your guess is -35.12. Is that the best choice? What are the
alternatives?</p>
</div>
<div class="paragraph">
<p>One option is to identify and discard outliers, then compute the sample
mean of the rest. Another option is to use the median as an estimator.</p>
</div>
<div class="paragraph">
<p>Which estimator is best depends on the circumstances (for example,
whether there are outliers) and on what the goal is. Are you trying to
minimize errors, or maximize your chance of getting the right answer?</p>
</div>
<div class="paragraph">
<p>If there are no outliers, the sample mean minimizes the <strong>mean squared
error</strong> (MSE). That is, if we play the game many times, and each time
compute the error \(\bar{x}- \mu\), the sample mean minimizes</p>
</div>
<div class="stemblock">
<div class="content">
\[MSE = \frac{1}{m} \sum (\bar{x}- \mu)^2\]
</div>
</div>
<div class="paragraph">
<p>Where \(m\) is the number of times you play the estimation game,
not to be confused with \(n\), which is the size of the sample
used to compute \(\bar{x}\).</p>
</div>
<div class="paragraph">
<p>Here is a function that simulates the estimation game and computes the
root mean squared error (RMSE), which is the square root of MSE:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def Estimate1(n=7, m=1000):
    mu = 0
    sigma = 1

    means = []
    medians = []
    for _ in range(m):
        xs = [random.gauss(mu, sigma) for i in range(n)]
        xbar = np.mean(xs)
        median = np.median(xs)
        means.append(xbar)
        medians.append(median)

    print('rmse xbar', RMSE(means, mu))
    print('rmse median', RMSE(medians, mu))</code></pre>
</div>
</div>
<div class="paragraph">
<p>Again, n is the size of the sample, and m is the number of times we play
the game. means is the list of estimates based on \(\bar{x}\).
medians is the list of medians.</p>
</div>
<div class="paragraph">
<p>Here’s the function that computes RMSE:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def RMSE(estimates, actual):
    e2 = [(estimate-actual)**2 for estimate in estimates]
    mse = np.mean(e2)
    return math.sqrt(mse)</code></pre>
</div>
</div>
<div class="paragraph">
<p>estimates is a list of estimates; actual is the actual value being
estimated. In practice, of course, we don’t know actual; if we did, we
wouldn’t have to estimate it. The purpose of this experiment is to
compare the performance of the two estimators.</p>
</div>
<div class="paragraph">
<p>When I ran this code, the RMSE of the sample mean was 0.41, which means
that if we use \(\bar{x}\) to estimate the mean of this
distribution, based on a sample with \(n=7\), we should expect
to be off by 0.41 on average. Using the median to estimate the mean
yields RMSE 0.53, which confirms that \(\bar{x}\) yields lower
RMSE, at least for this example.</p>
</div>
<div class="paragraph">
<p>Minimizing MSE is a nice property, but it’s not always the best
strategy. For example, suppose we are estimating the distribution of
wind speeds at a building site. If the estimate is too high, we might
overbuild the structure, increasing its cost. But if it’s too low, the
building might collapse. Because cost as a function of error is not
symmetric, minimizing MSE is not the best strategy.</p>
</div>
<div class="paragraph">
<p>As another example, suppose I roll three six-sided dice and ask you to
predict the total. If you get it exactly right, you get a prize;
otherwise you get nothing. In this case the value that minimizes MSE is
10.5, but that would be a bad guess, because the total of three dice is
never 10.5. For this game, you want an estimator that has the highest
chance of being right, which is a <strong>maximum likelihood estimator</strong> (MLE).
If you pick 10 or 11, your chance of winning is 1 in 8, and that’s the
best you can do.</p>
</div>
</div>
<div class="sect2">
<h3 id="_guess_the_variance"><a class="anchor" href="#_guess_the_variance"></a><a class="link" href="#_guess_the_variance">8.2. Guess the variance</a></h3>
<div class="paragraph">
<p><em>I’m thinking of a distribution.</em> It’s a normal distribution, and here’s
a (familiar) sample:</p>
</div>
<div class="paragraph">
<p>What do you think is the variance, \(\sigma^2\), of my
distribution? Again, the obvious choice is to use the sample variance,
\(S^2\), as an estimator.</p>
</div>
<div class="stemblock">
<div class="content">
\[S^2 = \frac{1}{n} \sum (x_i - \bar{x})^2\]
</div>
</div>
<div class="paragraph">
<p>For large samples, \(S^2\) is an adequate estimator, but for
small samples it tends to be too low. Because of this unfortunate
property, it is called a <strong>biased</strong> estimator. An estimator is <strong>unbiased</strong>
if the expected total (or mean) error, after many iterations of the
estimation game, is 0.</p>
</div>
<div class="paragraph">
<p>Fortunately, there is another simple statistic that is an unbiased
estimator of \(\sigma^2\):</p>
</div>
<div class="stemblock">
<div class="content">
\[S_{n-1}^2 = \frac{1}{n-1} \sum (x_i - \bar{x})^2\]
</div>
</div>
<div class="paragraph">
<p>For an explanation of why \(S^2\) is biased, and a proof that
\(S_{n-1}^2\) is unbiased, see
<a href="http://wikipedia.org/wiki/Bias_of_an_estimator" class="bare">http://wikipedia.org/wiki/Bias_of_an_estimator</a>.</p>
</div>
<div class="paragraph">
<p>The biggest problem with this estimator is that its name and symbol are
used inconsistently. The name &#8220;sample variance&#8221; can refer to either
\(S^2\) or \(S_{n-1}^2\), and the symbol \(S^2\)
is used for either or both.</p>
</div>
<div class="paragraph">
<p>Here is a function that simulates the estimation game and tests the
performance of \(S^2\) and \(S_{n-1}^2\):</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def Estimate2(n=7, m=1000):
    mu = 0
    sigma = 1

    estimates1 = []
    estimates2 = []
    for _ in range(m):
        xs = [random.gauss(mu, sigma) for i in range(n)]
        biased = np.var(xs)
        unbiased = np.var(xs, ddof=1)
        estimates1.append(biased)
        estimates2.append(unbiased)

    print('mean error biased', MeanError(estimates1, sigma**2))
    print('mean error unbiased', MeanError(estimates2, sigma**2))</code></pre>
</div>
</div>
<div class="paragraph">
<p>Again, n is the sample size and m is the number of times we play the
game. np.var computes \(S^2\) by default and
\(S_{n-1}^2\) if you provide the argument ddof=1, which stands
for &#8220;delta degrees of freedom.&#8221; I won’t explain that term, but you can
read about it at
<a href="http://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics" class="bare">http://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics</a>).</p>
</div>
<div class="paragraph">
<p>MeanError computes the mean difference between the estimates and the
actual value:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def MeanError(estimates, actual):
    errors = [estimate-actual for estimate in estimates]
    return np.mean(errors)</code></pre>
</div>
</div>
<div class="paragraph">
<p>When I ran this code, the mean error for \(S^2\) was -0.13. As
expected, this biased estimator tends to be too low. For
\(S_{n-1}^2\), the mean error was 0.014, about 10 times smaller.
As m increases, we expect the mean error for \(S_{n-1}^2\) to
approach 0.</p>
</div>
<div class="paragraph">
<p>Properties like MSE and bias are long-term expectations based on many
iterations of the estimation game. By running simulations like the ones
in this chapter, we can compare estimators and check whether they have
desired properties.</p>
</div>
<div class="paragraph">
<p>But when you apply an estimator to real data, you just get one estimate.
It would not be meaningful to say that the estimate is unbiased; being
unbiased is a property of the estimator, not the estimate.</p>
</div>
<div class="paragraph">
<p>After you choose an estimator with appropriate properties, and use it to
generate an estimate, the next step is to characterize the uncertainty
of the estimate, which is the topic of the next section.</p>
</div>
</div>
<div class="sect2">
<h3 id="gorilla"><a class="anchor" href="#gorilla"></a><a class="link" href="#gorilla">8.3. Sampling distributions</a></h3>
<div class="paragraph">
<p>Suppose you are a scientist studying gorillas in a wildlife preserve.
You want to know the average weight of the adult female gorillas in the
preserve. To weigh them, you have to tranquilize them, which is
dangerous, expensive, and possibly harmful to the gorillas. But if it is
important to obtain this information, it might be acceptable to weigh a
sample of 9 gorillas. Let’s assume that the population of the preserve
is well known, so we can choose a representative sample of adult
females. We could use the sample mean, \(\bar{x}\), to estimate
the unknown population mean, \(\mu\).</p>
</div>
<div class="paragraph">
<p>Having weighed 9 female gorillas, you might find \(\bar{x}=90\)
kg and sample standard deviation, \(S=7.5\) kg. The sample mean
is an unbiased estimator of \(\mu\), and in the long run it
minimizes MSE. So if you report a single estimate that summarizes the
results, you would report 90 kg.</p>
</div>
<div class="paragraph">
<p>But how confident should you be in this estimate? If you only weigh
\(n=9\) gorillas out of a much larger population, you might be
unlucky and choose the 9 heaviest gorillas (or the 9 lightest ones) just
by chance. Variation in the estimate caused by random selection is
called <strong>sampling error</strong>.</p>
</div>
<div class="paragraph">
<p>To quantify sampling error, we can simulate the sampling process with
hypothetical values of \(\mu\) and \(\sigma\), and see
how much \(\bar{x}\) varies.</p>
</div>
<div class="paragraph">
<p>Since we don’t know the actual values of \(\mu\) and
\(\sigma\) in the population, we’ll use the estimates
\(\bar{x}\) and \(S\). So the question we answer is:
&#8220;If the actual values of \(\mu\) and \(\sigma\) were 90
kg and 7.5 kg, and we ran the same experiment many times, how much would
the estimated mean, \(\bar{x}\), vary?&#8221;</p>
</div>
<div class="paragraph">
<p>The following function answers that question:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def SimulateSample(mu=90, sigma=7.5, n=9, m=1000):
    means = []
    for j in range(m):
        xs = np.random.normal(mu, sigma, n)
        xbar = np.mean(xs)
        means.append(xbar)

    cdf = thinkstats2.Cdf(means)
    ci = cdf.Percentile(5), cdf.Percentile(95)
    stderr = RMSE(means, mu)</code></pre>
</div>
</div>
<div class="paragraph">
<p>mu and sigma are the <em>hypothetical</em> values of the parameters. n is the
sample size, the number of gorillas we measured. m is the number of
times we run the simulation.</p>
</div>
<div class="paragraph">
<p>image::figs/estimation1.png[Sampling distribution of \(\bar{x}\),
with confidence interval.,height=240]</p>
</div>
<div class="paragraph">
<p>In each iteration, we choose n values from a normal distribution with
the given parameters, and compute the sample mean, xbar. We run 1000
simulations and then compute the distribution, cdf, of the estimates.
The result is shown in Figure <a href="#estimation1">[estimation1]</a>. This distribution
is called the <strong>sampling distribution</strong> of the estimator. It shows how
much the estimates would vary if we ran the experiment over and over.</p>
</div>
<div class="paragraph">
<p>The mean of the sampling distribution is pretty close to the
hypothetical value of \(\mu\), which means that the experiment
yields the right answer, on average. After 1000 tries, the lowest result
is 82 kg, and the highest is 98 kg. This range suggests that the
estimate might be off by as much as 8 kg.</p>
</div>
<div class="paragraph">
<p>There are two common ways to summarize the sampling distribution:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Standard error</strong> (SE) is a measure of how far we expect the estimate
to be off, on average. For each simulated experiment, we compute the
error, \(\bar{x}- \mu\), and then compute the root mean squared
error (RMSE). In this example, it is roughly 2.5 kg.</p>
</li>
<li>
<p>A <strong>confidence interval</strong> (CI) is a range that includes a given fraction
of the sampling distribution. For example, the 90% confidence interval
is the range from the 5th to the 95th percentile. In this example, the
90% CI is \((86, 94)\) kg.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Standard errors and confidence intervals are the source of much
confusion:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>People often confuse standard error and standard deviation. Remember
that standard deviation describes variability in a measured quantity; in
this example, the standard deviation of gorilla weight is 7.5 kg.
Standard error describes variability in an estimate. In this example,
the standard error of the mean, based on a sample of 9 measurements, is
2.5 kg.</p>
<div class="paragraph">
<p>One way to remember the difference is that, as sample size increases,
standard error gets smaller; standard deviation does not.</p>
</div>
</li>
<li>
<p>People often think that there is a 90% probability that the actual
parameter, \(\mu\), falls in the 90% confidence interval. Sadly,
that is not true. If you want to make a claim like that, you have to use
Bayesian methods (see my book, <em>Think Bayes</em>).</p>
<div class="paragraph">
<p>The sampling distribution answers a different question: it gives you a
sense of how reliable an estimate is by telling you how much it would
vary if you ran the experiment again.</p>
</div>
</li>
</ul>
</div>
<div class="paragraph">
<p>It is important to remember that confidence intervals and standard
errors only quantify sampling error; that is, error due to measuring
only part of the population. The sampling distribution does not account
for other sources of error, notably sampling bias and measurement error,
which are the topics of the next section.</p>
</div>
</div>
<div class="sect2">
<h3 id="_sampling_bias"><a class="anchor" href="#_sampling_bias"></a><a class="link" href="#_sampling_bias">8.4. Sampling bias</a></h3>
<div class="paragraph">
<p>Suppose that instead of the weight of gorillas in a nature preserve, you
want to know the average weight of women in the city where you live. It
is unlikely that you would be allowed to choose a representative sample
of women and weigh them.</p>
</div>
<div class="paragraph">
<p>A simple alternative would be &#8220;telephone sampling;&#8221; that is, you could
choose random numbers from the phone book, call and ask to speak to an
adult woman, and ask how much she weighs.</p>
</div>
<div class="paragraph">
<p>Telephone sampling has obvious limitations. For example, the sample is
limited to people whose telephone numbers are listed, so it eliminates
people without phones (who might be poorer than average) and people with
unlisted numbers (who might be richer). Also, if you call home
telephones during the day, you are less likely to sample people with
jobs. And if you only sample the person who answers the phone, you are
less likely to sample people who share a phone line.</p>
</div>
<div class="paragraph">
<p>If factors like income, employment, and household size are related to
weight—and it is plausible that they are—the results of your survey
would be affected one way or another. This problem is called <strong>sampling
bias</strong> because it is a property of the sampling process.</p>
</div>
<div class="paragraph">
<p>This sampling process is also vulnerable to self-selection, which is a
kind of sampling bias. Some people will refuse to answer the question,
and if the tendency to refuse is related to weight, that would affect
the results.</p>
</div>
<div class="paragraph">
<p>Finally, if you ask people how much they weigh, rather than weighing
them, the results might not be accurate. Even helpful respondents might
round up or down if they are uncomfortable with their actual weight. And
not all respondents are helpful. These inaccuracies are examples of
<strong>measurement error</strong>.</p>
</div>
<div class="paragraph">
<p>When you report an estimated quantity, it is useful to report standard
error, or a confidence interval, or both, in order to quantify sampling
error. But it is also important to remember that sampling error is only
one source of error, and often it is not the biggest.</p>
</div>
</div>
<div class="sect2">
<h3 id="_exponential_distributions"><a class="anchor" href="#_exponential_distributions"></a><a class="link" href="#_exponential_distributions">8.5. Exponential distributions</a></h3>
<div class="paragraph">
<p>Let’s play one more round of the estimation game. <em>I’m thinking of a
distribution.</em> It’s an exponential distribution, and here’s a sample:</p>
</div>
<div class="paragraph">
<p>What do you think is the parameter, \(\lambda\), of this
distribution?</p>
</div>
<div class="paragraph">
<p>In general, the mean of an exponential distribution is
\(1/\lambda\), so working backwards, we might choose</p>
</div>
<div class="stemblock">
<div class="content">
\[L= 1 / \bar{x}\]
</div>
</div>
<div class="paragraph">
<p>\(L\) is an estimator of \(\lambda\). And not just any
estimator; it is also the maximum likelihood estimator (see
<a href="http://wikipedia.org/wiki/Exponential_distribution#Maximum_likelihood" class="bare">http://wikipedia.org/wiki/Exponential_distribution#Maximum_likelihood</a>).
So if you want to maximize your chance of guessing \(\lambda\)
exactly, \(L\) is the way to go.</p>
</div>
<div class="paragraph">
<p>But we know that \(\bar{x}\) is not robust in the presence of
outliers, so we expect \(L\) to have the same problem.</p>
</div>
<div class="paragraph">
<p>We can choose an alternative based on the sample median. The median of
an exponential distribution is \(\ln(2) / \lambda\), so working
backwards again, we can define an estimator</p>
</div>
<div class="stemblock">
<div class="content">
\[L_m= \ln(2) / m\]
</div>
</div>
<div class="paragraph">
<p>where \(m\) is the sample median.</p>
</div>
<div class="paragraph">
<p>To test the performance of these estimators, we can simulate the
sampling process:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def Estimate3(n=7, m=1000):
    lam = 2

    means = []
    medians = []
    for _ in range(m):
        xs = np.random.exponential(1.0/lam, n)
        L = 1 / np.mean(xs)
        Lm = math.log(2) / thinkstats2.Median(xs)
        means.append(L)
        medians.append(Lm)

    print('rmse L', RMSE(means, lam))
    print('rmse Lm', RMSE(medians, lam))
    print('mean error L', MeanError(means, lam))
    print('mean error Lm', MeanError(medians, lam))</code></pre>
</div>
</div>
<div class="paragraph">
<p>When I run this experiment with \(\lambda=2\), the RMSE of
\(L\) is 1.1. For the median-based estimator \(L_m\),
RMSE is 1.8. We can’t tell from this experiment whether \(L\)
minimizes MSE, but at least it seems better than \(L_m\).</p>
</div>
<div class="paragraph">
<p>Sadly, it seems that both estimators are biased. For \(L\) the
mean error is 0.33; for \(L_m\) it is 0.45. And neither
converges to 0 as m increases.</p>
</div>
<div class="paragraph">
<p>It turns out that \(\bar{x}\) is an unbiased estimator of the
mean of the distribution, \(1 / \lambda\), but \(L\) is
not an unbiased estimator of \(\lambda\).</p>
</div>
</div>
<div class="sect2">
<h3 id="_exercises_8"><a class="anchor" href="#_exercises_8"></a><a class="link" href="#_exercises_8">8.6. Exercises</a></h3>
<div class="paragraph">
<p>For the following exercises, you might want to start with a copy of
estimation.py. Solutions are in <code>chap08soln.py</code></p>
</div>
<div class="paragraph">
<p>In this chapter we used \(\bar{x}\) and median to estimate
\(\mu\), and found that \(\bar{x}\) yields lower MSE.
Also, we used \(S^2\) and \(S_{n-1}^2\) to estimate
\(\sigma\), and found that \(S^2\) is biased and
\(S_{n-1}^2\) unbiased.</p>
</div>
<div class="paragraph">
<p>Run similar experiments to see if \(\bar{x}\) and median are
biased estimates of \(\mu\). Also check whether \(S^2\)
or \(S_{n-1}^2\) yields a lower MSE.</p>
</div>
<div class="paragraph">
<p>Suppose you draw a sample with size \(n=10\) from an exponential
distribution with \(\lambda=2\). Simulate this experiment 1000
times and plot the sampling distribution of the estimate \(L\).
Compute the standard error of the estimate and the 90% confidence
interval.</p>
</div>
<div class="paragraph">
<p>Repeat the experiment with a few different values of \(n\) and
make a plot of standard error versus \(n\).</p>
</div>
<div class="paragraph">
<p>In games like hockey and soccer, the time between goals is roughly
exponential. So you could estimate a team’s goal-scoring rate by
observing the number of goals they score in a game. This estimation
process is a little different from sampling the time between goals, so
let’s see how it works.</p>
</div>
<div class="paragraph">
<p>Write a function that takes a goal-scoring rate, lam, in goals per game,
and simulates a game by generating the time between goals until the
total time exceeds 1 game, then returns the number of goals scored.</p>
</div>
<div class="paragraph">
<p>Write another function that simulates many games, stores the estimates
of lam, then computes their mean error and RMSE.</p>
</div>
<div class="paragraph">
<p>Is this way of making an estimate biased? Plot the sampling distribution
of the estimates and the 90% confidence interval. What is the standard
error? What happens to sampling error for increasing values of lam?</p>
</div>
</div>
<div class="sect2">
<h3 id="_glossary_8"><a class="anchor" href="#_glossary_8"></a><a class="link" href="#_glossary_8">8.7. Glossary</a></h3>
<div class="ulist">
<ul>
<li>
<p>estimation: The process of inferring the parameters of a distribution
from a sample.</p>
</li>
<li>
<p>estimator: A statistic used to estimate a parameter.</p>
</li>
<li>
<p>mean squared error (MSE): A measure of estimation error.</p>
</li>
<li>
<p>root mean squared error (RMSE): The square root of MSE, a more
meaningful representation of typical error magnitude.</p>
</li>
<li>
<p>maximum likelihood estimator (MLE): An estimator that computes the
point estimate most likely to be correct.</p>
</li>
<li>
<p>bias (of an estimator): The tendency of an estimator to be above or
below the actual value of the parameter, when averaged over repeated
experiments.</p>
</li>
<li>
<p>sampling error: Error in an estimate due to the limited size of the
sample and variation due to chance.</p>
</li>
<li>
<p>sampling bias: Error in an estimate due to a sampling process that is
not representative of the population.</p>
</li>
<li>
<p>measurement error: Error in an estimate due to inaccuracy collecting
or recording data.</p>
</li>
<li>
<p>sampling distribution: The distribution of a statistic if an
experiment is repeated many times.</p>
</li>
<li>
<p>standard error: The RMSE of an estimate, which quantifies variability
due to sampling error (but not other sources of error).</p>
</li>
<li>
<p>confidence interval: An interval that represents the expected range of
an estimator if an experiment is repeated many times.</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="testing"><a class="anchor" href="#testing"></a><a class="link" href="#testing">9. Hypothesis testing</a></h2>
<div class="sectionbody">
<div class="paragraph">
<p>The code for this chapter is in hypothesis.py. For information about
downloading and working with this code, see Section <a href="#code">Using the code</a>.</p>
</div>
<div class="sect2">
<h3 id="_classical_hypothesis_testing"><a class="anchor" href="#_classical_hypothesis_testing"></a><a class="link" href="#_classical_hypothesis_testing">9.1. Classical hypothesis testing</a></h3>
<div class="paragraph">
<p>Exploring the data from the NSFG, we saw several &#8220;apparent effects,&#8221;
including differences between first babies and others. So far we have
taken these effects at face value; in this chapter, we put them to the
test.</p>
</div>
<div class="paragraph">
<p>The fundamental question we want to address is whether the effects we
see in a sample are likely to appear in the larger population. For
example, in the NSFG sample we see a difference in mean pregnancy length
for first babies and others. We would like to know if that effect
reflects a real difference for women in the U.S., or if it might appear
in the sample by chance.</p>
</div>
<div class="paragraph">
<p>There are several ways we could formulate this question, including
Fisher null hypothesis testing, Neyman-Pearson decision theory, and
Bayesian inference<sup class="footnote">[<a id="_footnoteref_4" class="footnote" href="#_footnote_4" title="View footnote.">4</a>]</sup>. What I present here is a subset of
all three that makes up most of what people use in practice, which I
will call <strong>classical hypothesis testing</strong>.</p>
</div>
<div class="paragraph">
<p>The goal of classical hypothesis testing is to answer the question,
&#8220;Given a sample and an apparent effect, what is the probability of
seeing such an effect by chance?&#8221; Here’s how we answer that question:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The first step is to quantify the size of the apparent effect by
choosing a <strong>test statistic</strong>. In the NSFG example, the apparent effect is
a difference in pregnancy length between first babies and others, so a
natural choice for the test statistic is the difference in means between
the two groups.</p>
</li>
<li>
<p>The second step is to define a <strong>null hypothesis</strong>, which is a model of
the system based on the assumption that the apparent effect is not real.
In the NSFG example the null hypothesis is that there is no difference
between first babies and others; that is, that pregnancy lengths for
both groups have the same distribution.</p>
</li>
<li>
<p>The third step is to compute a <strong>p-value</strong>, which is the probability of
seeing the apparent effect if the null hypothesis is true. In the NSFG
example, we would compute the actual difference in means, then compute
the probability of seeing a difference as big, or bigger, under the null
hypothesis.</p>
</li>
<li>
<p>The last step is to interpret the result. If the p-value is low, the
effect is said to be <strong>statistically significant</strong>, which means that it is
unlikely to have occurred by chance. In that case we infer that the
effect is more likely to appear in the larger population.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The logic of this process is similar to a proof by contradiction. To
prove a mathematical statement, A, you assume temporarily that A is
false. If that assumption leads to a contradiction, you conclude that A
must actually be true.</p>
</div>
<div class="paragraph">
<p>Similarly, to test a hypothesis like, &#8220;This effect is real,&#8221; we
assume, temporarily, that it is not. That’s the null hypothesis. Based
on that assumption, we compute the probability of the apparent effect.
That’s the p-value. If the p-value is low, we conclude that the null
hypothesis is unlikely to be true.</p>
</div>
</div>
<div class="sect2">
<h3 id="hypotest"><a class="anchor" href="#hypotest"></a><a class="link" href="#hypotest">9.2. HypothesisTest</a></h3>
<div class="paragraph">
<p>thinkstats2 provides HypothesisTest, a class that represents the
structure of a classical hypothesis test. Here is the definition:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">class HypothesisTest(object):

    def __init__(self, data):
        self.data = data
        self.MakeModel()
        self.actual = self.TestStatistic(data)

    def PValue(self, iters=1000):
        self.test_stats = [self.TestStatistic(self.RunModel())
                           for _ in range(iters)]

        count = sum(1 for x in self.test_stats if x &gt;= self.actual)
        return count / iters

    def TestStatistic(self, data):
        raise UnimplementedMethodException()

    def MakeModel(self):
        pass

    def RunModel(self):
        raise UnimplementedMethodException()</code></pre>
</div>
</div>
<div class="paragraph">
<p>HypothesisTest is an abstract parent class that provides complete
definitions for some methods and place-keepers for others. Child classes
based on HypothesisTest inherit <code>__init__</code> and PValue and provide
TestStatistic, RunModel, and optionally MakeModel.</p>
</div>
<div class="paragraph">
<p><code>__init__</code> takes the data in whatever form is appropriate. It calls
MakeModel, which builds a representation of the null hypothesis, then
passes the data to TestStatistic, which computes the size of the effect
in the sample.</p>
</div>
<div class="paragraph">
<p>PValue computes the probability of the apparent effect under the null
hypothesis. It takes as a parameter iters, which is the number of
simulations to run. The first line generates simulated data, computes
test statistics, and stores them in <code>test_stats</code>. The result is the
fraction of elements in <code>test_stats</code> that exceed or equal the observed
test statistic, self.actual.</p>
</div>
<div class="paragraph">
<p>As a simple example<sup class="footnote">[<a id="_footnoteref_5" class="footnote" href="#_footnote_5" title="View footnote.">5</a>]</sup>, suppose we toss a coin 250
times and see 140 heads and 110 tails. Based on this result, we might
suspect that the coin is biased; that is, more likely to land heads. To
test this hypothesis, we compute the probability of seeing such a
difference if the coin is actually fair:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">class CoinTest(thinkstats2.HypothesisTest):

    def TestStatistic(self, data):
        heads, tails = data
        test_stat = abs(heads - tails)
        return test_stat

    def RunModel(self):
        heads, tails = self.data
        n = heads + tails
        sample = [random.choice('HT') for _ in range(n)]
        hist = thinkstats2.Hist(sample)
        data = hist['H'], hist['T']
        return data</code></pre>
</div>
</div>
<div class="paragraph">
<p>The parameter, data, is a pair of integers: the number of heads and
tails. The test statistic is the absolute difference between them, so
self.actual is 30.</p>
</div>
<div class="paragraph">
<p>RunModel simulates coin tosses assuming that the coin is actually fair.
It generates a sample of 250 tosses, uses Hist to count the number of
heads and tails, and returns a pair of integers.</p>
</div>
<div class="paragraph">
<p>Now all we have to do is instantiate CoinTest and call PValue:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    ct = CoinTest((140, 110))
    pvalue = ct.PValue()</code></pre>
</div>
</div>
<div class="paragraph">
<p>The result is about 0.07, which means that if the coin is fair, we
expect to see a difference as big as 30 about 7% of the time.</p>
</div>
<div class="paragraph">
<p>How should we interpret this result? By convention, 5% is the threshold
of statistical significance. If the p-value is less than 5%, the effect
is considered significant; otherwise it is not.</p>
</div>
<div class="paragraph">
<p>But the choice of 5% is arbitrary, and (as we will see later) the
p-value depends on the choice of the test statistics and the model of
the null hypothesis. So p-values should not be considered precise
measurements.</p>
</div>
<div class="paragraph">
<p>I recommend interpreting p-values according to their order of magnitude:
if the p-value is less than 1%, the effect is unlikely to be due to
chance; if it is greater than 10%, the effect can plausibly be explained
by chance. P-values between 1% and 10% should be considered borderline.
So in this example I conclude that the data do not provide strong
evidence that the coin is biased or not.</p>
</div>
</div>
<div class="sect2">
<h3 id="testdiff"><a class="anchor" href="#testdiff"></a><a class="link" href="#testdiff">9.3. Testing a difference in means</a></h3>
<div class="paragraph">
<p>One of the most common effects to test is a difference in mean between
two groups. In the NSFG data, we saw that the mean pregnancy length for
first babies is slightly longer, and the mean birth weight is slightly
smaller. Now we will see if those effects are statistically significant.</p>
</div>
<div class="paragraph">
<p>For these examples, the null hypothesis is that the distributions for
the two groups are the same. One way to model the null hypothesis is by
<strong>permutation</strong>; that is, we can take values for first babies and others
and shuffle them, treating the two groups as one big group:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">class DiffMeansPermute(thinkstats2.HypothesisTest):

    def TestStatistic(self, data):
        group1, group2 = data
        test_stat = abs(group1.mean() - group2.mean())
        return test_stat

    def MakeModel(self):
        group1, group2 = self.data
        self.n, self.m = len(group1), len(group2)
        self.pool = np.hstack((group1, group2))

    def RunModel(self):
        np.random.shuffle(self.pool)
        data = self.pool[:self.n], self.pool[self.n:]
        return data</code></pre>
</div>
</div>
<div class="paragraph">
<p>data is a pair of sequences, one for each group. The test statistic is
the absolute difference in the means.</p>
</div>
<div class="paragraph">
<p>MakeModel records the sizes of the groups, n and m, and combines the
groups into one NumPy array, self.pool.</p>
</div>
<div class="paragraph">
<p>RunModel simulates the null hypothesis by shuffling the pooled values
and splitting them into two groups with sizes n and m. As always, the
return value from RunModel has the same format as the observed data.</p>
</div>
<div class="paragraph">
<p>To test the difference in pregnancy length, we run:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    live, firsts, others = first.MakeFrames()
    data = firsts.prglngth.values, others.prglngth.values
    ht = DiffMeansPermute(data)
    pvalue = ht.PValue()</code></pre>
</div>
</div>
<div class="paragraph">
<p>MakeFrames reads the NSFG data and returns DataFrames representing all
live births, first babies, and others. We extract pregnancy lengths as
NumPy arrays, pass them as data to DiffMeansPermute, and compute the
p-value. The result is about 0.17, which means that we expect to see a
difference as big as the observed effect about 17% of the time. So this
effect is not statistically significant.</p>
</div>
<div class="paragraph">
<p>image::figs/hypothesis1.png[CDF of difference in mean pregnancy length
under the null hypothesis.,height=240]</p>
</div>
<div class="paragraph">
<p>HypothesisTest provides PlotCdf, which plots the distribution of the
test statistic and a gray line indicating the observed effect size:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    ht.PlotCdf()
    thinkplot.Show(xlabel='test statistic',
                   ylabel='CDF')</code></pre>
</div>
</div>
<div class="paragraph">
<p>Figure <a href="#hypothesis1">[hypothesis1]</a> shows the result. The CDF intersects the
observed difference at 0.83, which is the complement of the p-value,
0.17.</p>
</div>
<div class="paragraph">
<p>If we run the same analysis with birth weight, the computed p-value is
0; after 1000 attempts, the simulation never yields an effect as big as
the observed difference, 0.12 lbs. So we would report
\(p &lt; 0.001\), and conclude that the difference in birth weight
is statistically significant.</p>
</div>
</div>
<div class="sect2">
<h3 id="_other_test_statistics"><a class="anchor" href="#_other_test_statistics"></a><a class="link" href="#_other_test_statistics">9.4. Other test statistics</a></h3>
<div class="paragraph">
<p>Choosing the best test statistic depends on what question you are trying
to address. For example, if the relevant question is whether pregnancy
lengths are different for first babies, then it makes sense to test the
absolute difference in means, as we did in the previous section.</p>
</div>
<div class="paragraph">
<p>If we had some reason to think that first babies are likely to be late,
then we would not take the absolute value of the difference; instead we
would use this test statistic:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">class DiffMeansOneSided(DiffMeansPermute):

    def TestStatistic(self, data):
        group1, group2 = data
        test_stat = group1.mean() - group2.mean()
        return test_stat</code></pre>
</div>
</div>
<div class="paragraph">
<p>DiffMeansOneSided inherits MakeModel and RunModel from DiffMeansPermute;
the only difference is that TestStatistic does not take the absolute
value of the difference. This kind of test is called <strong>one-sided</strong> because
it only counts one side of the distribution of differences. The previous
test, using both sides, is <strong>two-sided</strong>.</p>
</div>
<div class="paragraph">
<p>For this version of the test, the p-value is 0.09. In general the
p-value for a one-sided test is about half the p-value for a two-sided
test, depending on the shape of the distribution.</p>
</div>
<div class="paragraph">
<p>The one-sided hypothesis, that first babies are born late, is more
specific than the two-sided hypothesis, so the p-value is smaller. But
even for the stronger hypothesis, the difference is not statistically
significant.</p>
</div>
<div class="paragraph">
<p>We can use the same framework to test for a difference in standard
deviation. In Section <a href="#visualization">3.3</a>, we saw some evidence
that first babies are more likely to be early or late, and less likely
to be on time. So we might hypothesize that the standard deviation is
higher. Here’s how we can test that:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">class DiffStdPermute(DiffMeansPermute):

    def TestStatistic(self, data):
        group1, group2 = data
        test_stat = group1.std() - group2.std()
        return test_stat</code></pre>
</div>
</div>
<div class="paragraph">
<p>This is a one-sided test because the hypothesis is that the standard
deviation for first babies is higher, not just different. The p-value is
0.09, which is not statistically significant.</p>
</div>
</div>
<div class="sect2">
<h3 id="corrtest"><a class="anchor" href="#corrtest"></a><a class="link" href="#corrtest">9.5. Testing a correlation</a></h3>
<div class="paragraph">
<p>This framework can also test correlations. For example, in the NSFG data
set, the correlation between birth weight and mother’s age is about
0.07. It seems like older mothers have heavier babies. But could this
effect be due to chance?</p>
</div>
<div class="paragraph">
<p>For the test statistic, I use Pearson’s correlation, but Spearman’s
would work as well. If we had reason to expect positive correlation, we
would do a one-sided test. But since we have no such reason, I’ll do a
two-sided test using the absolute value of correlation.</p>
</div>
<div class="paragraph">
<p>The null hypothesis is that there is no correlation between mother’s age
and birth weight. By shuffling the observed values, we can simulate a
world where the distributions of age and birth weight are the same, but
where the variables are unrelated:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">class CorrelationPermute(thinkstats2.HypothesisTest):

    def TestStatistic(self, data):
        xs, ys = data
        test_stat = abs(thinkstats2.Corr(xs, ys))
        return test_stat

    def RunModel(self):
        xs, ys = self.data
        xs = np.random.permutation(xs)
        return xs, ys</code></pre>
</div>
</div>
<div class="paragraph">
<p>data is a pair of sequences. TestStatistic computes the absolute value
of Pearson’s correlation. RunModel shuffles the xs and returns simulated
data.</p>
</div>
<div class="paragraph">
<p>Here’s the code that reads the data and runs the test:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    live, firsts, others = first.MakeFrames()
    live = live.dropna(subset=['agepreg', 'totalwgt_lb'])
    data = live.agepreg.values, live.totalwgt_lb.values
    ht = CorrelationPermute(data)
    pvalue = ht.PValue()</code></pre>
</div>
</div>
<div class="paragraph">
<p>I use dropna with the subset argument to drop rows that are missing
either of the variables we need.</p>
</div>
<div class="paragraph">
<p>The actual correlation is 0.07. The computed p-value is 0; after 1000
iterations the largest simulated correlation is 0.04. So although the
observed correlation is small, it is statistically significant.</p>
</div>
<div class="paragraph">
<p>This example is a reminder that &#8220;statistically significant&#8221; does not
always mean that an effect is important, or significant in practice. It
only means that it is unlikely to have occurred by chance.</p>
</div>
</div>
<div class="sect2">
<h3 id="casino"><a class="anchor" href="#casino"></a><a class="link" href="#casino">9.6. Testing proportions</a></h3>
<div class="paragraph">
<p>Suppose you run a casino and you suspect that a customer is using a
crooked die; that is, one that has been modified to make one of the
faces more likely than the others. You apprehend the alleged cheater and
confiscate the die, but now you have to prove that it is crooked. You
roll the die 60 times and get the following results:</p>
</div>
<table class="tableblock frame-all grid-all spread">
<colgroup>
<col style="width: 14.2857%;">
<col style="width: 14.2857%;">
<col style="width: 14.2857%;">
<col style="width: 14.2857%;">
<col style="width: 14.2857%;">
<col style="width: 14.2857%;">
<col style="width: 14.2858%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Value</th>
<th class="tableblock halign-center valign-top">1</th>
<th class="tableblock halign-center valign-top">2</th>
<th class="tableblock halign-center valign-top">3</th>
<th class="tableblock halign-center valign-top">4</th>
<th class="tableblock halign-center valign-top">5</th>
<th class="tableblock halign-center valign-top">6</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Frequency</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">8</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">9</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">19</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">5</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">8</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">11</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>On average you expect each value to appear 10 times. In this dataset,
the value 3 appears more often than expected, and the value 4 appears
less often. But are these differences statistically significant?</p>
</div>
<div class="paragraph">
<p>To test this hypothesis, we can compute the expected frequency for each
value, the difference between the expected and observed frequencies, and
the total absolute difference. In this example, we expect each side to
come up 10 times out of 60; the deviations from this expectation are -2,
-1, 9, -5, -2, and 1; so the total absolute difference is 20. How often
would we see such a difference by chance?</p>
</div>
<div class="paragraph">
<p>Here’s a version of HypothesisTest that answers that question:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">class DiceTest(thinkstats2.HypothesisTest):

    def TestStatistic(self, data):
        observed = data
        n = sum(observed)
        expected = np.ones(6) * n / 6
        test_stat = sum(abs(observed - expected))
        return test_stat

    def RunModel(self):
        n = sum(self.data)
        values = [1, 2, 3, 4, 5, 6]
        rolls = np.random.choice(values, n, replace=True)
        hist = thinkstats2.Hist(rolls)
        freqs = hist.Freqs(values)
        return freqs</code></pre>
</div>
</div>
<div class="paragraph">
<p>The data are represented as a list of frequencies: the observed values
are ; the expected frequencies are all 10. The test statistic is the sum
of the absolute differences.</p>
</div>
<div class="paragraph">
<p>The null hypothesis is that the die is fair, so we simulate that by
drawing random samples from values. RunModel uses Hist to compute and
return the list of frequencies.</p>
</div>
<div class="paragraph">
<p>The p-value for this data is 0.13, which means that if the die is fair
we expect to see the observed total deviation, or more, about 13% of the
time. So the apparent effect is not statistically significant.</p>
</div>
</div>
<div class="sect2">
<h3 id="casino2"><a class="anchor" href="#casino2"></a><a class="link" href="#casino2">9.7. Chi-squared tests</a></h3>
<div class="paragraph">
<p>In the previous section we used total deviation as the test statistic.
But for testing proportions it is more common to use the chi-squared
statistic:</p>
</div>
<div class="stemblock">
<div class="content">
\[\chi^2 = \sum_i \frac{(O_i - E_i)^2}{E_i}\]
</div>
</div>
<div class="paragraph">
<p>Where \(O_i\) are the observed frequencies and \(E_i\)
are the expected frequencies. Here’s the Python code:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">class DiceChiTest(DiceTest):

    def TestStatistic(self, data):
        observed = data
        n = sum(observed)
        expected = np.ones(6) * n / 6
        test_stat = sum((observed - expected)**2 / expected)
        return test_stat</code></pre>
</div>
</div>
<div class="paragraph">
<p>Squaring the deviations (rather than taking absolute values) gives more
weight to large deviations. Dividing through by expected standardizes
the deviations, although in this case it has no effect because the
expected frequencies are all equal.</p>
</div>
<div class="paragraph">
<p>The p-value using the chi-squared statistic is 0.04, substantially
smaller than what we got using total deviation, 0.13. If we take the 5%
threshold seriously, we would consider this effect statistically
significant. But considering the two tests togther, I would say that the
results are borderline. I would not rule out the possibility that the
die is crooked, but I would not convict the accused cheater.</p>
</div>
<div class="paragraph">
<p>This example demonstrates an important point: the p-value depends on the
choice of test statistic and the model of the null hypothesis, and
sometimes these choices determine whether an effect is statistically
significant or not.</p>
</div>
</div>
<div class="sect2">
<h3 id="_first_babies_again"><a class="anchor" href="#_first_babies_again"></a><a class="link" href="#_first_babies_again">9.8. First babies again</a></h3>
<div class="paragraph">
<p>Earlier in this chapter we looked at pregnancy lengths for first babies
and others, and concluded that the apparent differences in mean and
standard deviation are not statistically significant. But in
Section <a href="#visualization">3.3</a>, we saw several apparent differences in
the distribution of pregnancy length, especially in the range from 35 to
43 weeks. To see whether those differences are statistically
significant, we can use a test based on a chi-squared statistic.</p>
</div>
<div class="paragraph">
<p>The code combines elements from previous examples:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">class PregLengthTest(thinkstats2.HypothesisTest):

    def MakeModel(self):
        firsts, others = self.data
        self.n = len(firsts)
        self.pool = np.hstack((firsts, others))

        pmf = thinkstats2.Pmf(self.pool)
        self.values = range(35, 44)
        self.expected_probs = np.array(pmf.Probs(self.values))

    def RunModel(self):
        np.random.shuffle(self.pool)
        data = self.pool[:self.n], self.pool[self.n:]
        return data</code></pre>
</div>
</div>
<div class="paragraph">
<p>The data are represented as two lists of pregnancy lengths. The null
hypothesis is that both samples are drawn from the same distribution.
MakeModel models that distribution by pooling the two samples using
hstack. Then RunModel generates simulated data by shuffling the pooled
sample and splitting it into two parts.</p>
</div>
<div class="paragraph">
<p>MakeModel also defines values, which is the range of weeks we’ll use,
and <code>expected_probs</code>, which is the probability of each value in the
pooled distribution.</p>
</div>
<div class="paragraph">
<p>Here’s the code that computes the test statistic:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python"># class PregLengthTest:

    def TestStatistic(self, data):
        firsts, others = data
        stat = self.ChiSquared(firsts) + self.ChiSquared(others)
        return stat

    def ChiSquared(self, lengths):
        hist = thinkstats2.Hist(lengths)
        observed = np.array(hist.Freqs(self.values))
        expected = self.expected_probs * len(lengths)
        stat = sum((observed - expected)**2 / expected)
        return stat</code></pre>
</div>
</div>
<div class="paragraph">
<p>TestStatistic computes the chi-squared statistic for first babies and
others, and adds them.</p>
</div>
<div class="paragraph">
<p>ChiSquared takes a sequence of pregnancy lengths, computes its
histogram, and computes observed, which is a list of frequencies
corresponding to self.values. To compute the list of expected
frequencies, it multiplies the pre-computed probabilities,
<code>expected_probs</code>, by the sample size. It returns the chi-squared
statistic, stat.</p>
</div>
<div class="paragraph">
<p>For the NSFG data the total chi-squared statistic is 102, which doesn’t
mean much by itself. But after 1000 iterations, the largest test
statistic generated under the null hypothesis is 32. We conclude that
the observed chi-squared statistic is unlikely under the null
hypothesis, so the apparent effect is statistically significant.</p>
</div>
<div class="paragraph">
<p>This example demonstrates a limitation of chi-squared tests: they
indicate that there is a difference between the two groups, but they
don’t say anything specific about what the difference is.</p>
</div>
</div>
<div class="sect2">
<h3 id="_errors"><a class="anchor" href="#_errors"></a><a class="link" href="#_errors">9.9. Errors</a></h3>
<div class="paragraph">
<p>In classical hypothesis testing, an effect is considered statistically
significant if the p-value is below some threshold, commonly 5%. This
procedure raises two questions:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>If the effect is actually due to chance, what is the probability that
we will wrongly consider it significant? This probability is the <strong>false
positive rate</strong>.</p>
</li>
<li>
<p>If the effect is real, what is the chance that the hypothesis test
will fail? This probability is the <strong>false negative rate</strong>.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The false positive rate is relatively easy to compute: if the threshold
is 5%, the false positive rate is 5%. Here’s why:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>If there is no real effect, the null hypothesis is true, so we can
compute the distribution of the test statistic by simulating the null
hypothesis. Call this distribution \(\mathrm{CDF}_T\).</p>
</li>
<li>
<p>Each time we run an experiment, we get a test statistic,
\(t\), which is drawn from \(CDF_T\). Then we compute a
p-value, which is the probability that a random value from
\(CDF_T\) exceeds t, so that’s \(1 - CDF_T(t)\).</p>
</li>
<li>
<p>The p-value is less than 5% if \(CDF_T(t)\) is greater than
95%; that is, if \(t\) exceeds the 95th percentile. And how
often does a value chosen from \(CDF_T\) exceed the 95th
percentile? 5% of the time.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>So if you perform one hypothesis test with a 5% threshold, you expect a
false positive 1 time in 20.</p>
</div>
</div>
<div class="sect2">
<h3 id="_power"><a class="anchor" href="#_power"></a><a class="link" href="#_power">9.10. Power</a></h3>
<div class="paragraph">
<p>The false negative rate is harder to compute because it depends on the
actual effect size, and normally we don’t know that. One option is to
compute a rate conditioned on a hypothetical effect size.</p>
</div>
<div class="paragraph">
<p>For example, if we assume that the observed difference between groups is
accurate, we can use the observed samples as a model of the population
and run hypothesis tests with simulated data:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def FalseNegRate(data, num_runs=100):
    group1, group2 = data
    count = 0

    for i in range(num_runs):
        sample1 = thinkstats2.Resample(group1)
        sample2 = thinkstats2.Resample(group2)

        ht = DiffMeansPermute((sample1, sample2))
        pvalue = ht.PValue(iters=101)
        if pvalue &gt; 0.05:
            count += 1

    return count / num_runs</code></pre>
</div>
</div>
<div class="paragraph">
<p>FalseNegRate takes data in the form of two sequences, one for each
group. Each time through the loop, it simulates an experiment by drawing
a random sample from each group and running a hypothesis test. Then it
checks the result and counts the number of false negatives.</p>
</div>
<div class="paragraph">
<p>Resample takes a sequence and draws a sample with the same length, with
replacement:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def Resample(xs):
    return np.random.choice(xs, len(xs), replace=True)</code></pre>
</div>
</div>
<div class="paragraph">
<p>Here’s the code that tests pregnancy lengths:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    live, firsts, others = first.MakeFrames()
    data = firsts.prglngth.values, others.prglngth.values
    neg_rate = FalseNegRate(data)</code></pre>
</div>
</div>
<div class="paragraph">
<p>The result is about 70%, which means that if the actual difference in
mean pregnancy length is 0.078 weeks, we expect an experiment with this
sample size to yield a negative test 70% of the time.</p>
</div>
<div class="paragraph">
<p>This result is often presented the other way around: if the actual
difference is 0.078 weeks, we should expect a positive test only 30% of
the time. This &#8220;correct positive rate&#8221; is called the <strong>power</strong> of the
test, or sometimes &#8220;sensitivity&#8221;. It reflects the ability of the test
to detect an effect of a given size.</p>
</div>
<div class="paragraph">
<p>In this example, the test had only a 30% chance of yielding a positive
result (again, assuming that the difference is 0.078 weeks). As a rule
of thumb, a power of 80% is considered acceptable, so we would say that
this test was &#8220;underpowered.&#8221;</p>
</div>
<div class="paragraph">
<p>In general a negative hypothesis test does not imply that there is no
difference between the groups; instead it suggests that if there is a
difference, it is too small to detect with this sample size.</p>
</div>
</div>
<div class="sect2">
<h3 id="_replication"><a class="anchor" href="#_replication"></a><a class="link" href="#_replication">9.11. Replication</a></h3>
<div class="paragraph">
<p>The hypothesis testing process I demonstrated in this chapter is not,
strictly speaking, good practice.</p>
</div>
<div class="paragraph">
<p>First, I performed multiple tests. If you run one hypothesis test, the
chance of a false positive is about 1 in 20, which might be acceptable.
But if you run 20 tests, you should expect at least one false positive,
most of the time.</p>
</div>
<div class="paragraph">
<p>Second, I used the same dataset for exploration and testing. If you
explore a large dataset, find a surprising effect, and then test whether
it is significant, you have a good chance of generating a false
positive.</p>
</div>
<div class="paragraph">
<p>To compensate for multiple tests, you can adjust the p-value threshold
(see <a href="https://en.wikipedia.org/wiki/Holm-Bonferroni_method" class="bare">https://en.wikipedia.org/wiki/Holm-Bonferroni_method</a>). Or you can
address both problems by partitioning the data, using one set for
exploration and the other for testing.</p>
</div>
<div class="paragraph">
<p>In some fields these practices are required or at least encouraged. But
it is also common to address these problems implicitly by replicating
published results. Typically the first paper to report a new result is
considered exploratory. Subsequent papers that replicate the result with
new data are considered confirmatory.</p>
</div>
<div class="paragraph">
<p>As it happens, we have an opportunity to replicate the results in this
chapter. The first edition of this book is based on Cycle 6 of the NSFG,
which was released in 2002. In October 2011, the CDC released additional
data based on interviews conducted from 2006–2010. nsfg2.py contains
code to read and clean this data. In the new dataset:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The difference in mean pregnancy length is 0.16 weeks and
statistically significant with \(p &lt; 0.001\) (compared to 0.078
weeks in the original dataset).</p>
</li>
<li>
<p>The difference in birth weight is 0.17 pounds with
\(p &lt; 0.001\) (compared to 0.12 lbs in the original dataset).</p>
</li>
<li>
<p>The correlation between birth weight and mother’s age is 0.08 with
\(p &lt; 0.001\) (compared to 0.07).</p>
</li>
<li>
<p>The chi-squared test is statistically significant with
\(p &lt; 0.001\) (as it was in the original).</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>In summary, all of the effects that were statistically significant in
the original dataset were replicated in the new dataset, and the
difference in pregnancy length, which was not significant in the
original, is bigger in the new dataset and significant.</p>
</div>
</div>
<div class="sect2">
<h3 id="_exercises_9"><a class="anchor" href="#_exercises_9"></a><a class="link" href="#_exercises_9">9.12. Exercises</a></h3>
<div class="paragraph">
<p>A solution to these exercises is in <code>chap09soln.py</code>.</p>
</div>
<div class="paragraph">
<p>As sample size increases, the power of a hypothesis test increases,
which means it is more likely to be positive if the effect is real.
Conversely, as sample size decreases, the test is less likely to be
positive even if the effect is real.</p>
</div>
<div class="paragraph">
<p>To investigate this behavior, run the tests in this chapter with
different subsets of the NSFG data. You can use thinkstats2.SampleRows
to select a random subset of the rows in a DataFrame.</p>
</div>
<div class="paragraph">
<p>What happens to the p-values of these tests as sample size decreases?
What is the smallest sample size that yields a positive test?</p>
</div>
<div class="paragraph">
<p>In Section <a href="#testdiff">9.3</a>, we simulated the null hypothesis by
permutation; that is, we treated the observed values as if they
represented the entire population, and randomly assigned the members of
the population to the two groups.</p>
</div>
<div class="paragraph">
<p>An alternative is to use the sample to estimate the distribution for the
population, then draw a random sample from that distribution. This
process is called <strong>resampling</strong>. There are several ways to implement
resampling, but one of the simplest is to draw a sample with replacement
from the observed values, as in Section <a href="#power">[power]</a>.</p>
</div>
<div class="paragraph">
<p>Write a class named DiffMeansResample that inherits from
DiffMeansPermute and overrides RunModel to implement resampling, rather
than permutation.</p>
</div>
<div class="paragraph">
<p>Use this model to test the differences in pregnancy length and birth
weight. How much does the model affect the results?</p>
</div>
</div>
<div class="sect2">
<h3 id="_glossary_9"><a class="anchor" href="#_glossary_9"></a><a class="link" href="#_glossary_9">9.13. Glossary</a></h3>
<div class="ulist">
<ul>
<li>
<p>hypothesis testing: The process of determining whether an apparent
effect is statistically significant.</p>
</li>
<li>
<p>test statistic: A statistic used to quantify an effect size.</p>
</li>
<li>
<p>null hypothesis: A model of a system based on the assumption that an
apparent effect is due to chance.</p>
</li>
<li>
<p>p-value: The probability that an effect could occur by chance.</p>
</li>
<li>
<p>statistically significant: An effect is statistically significant if
it is unlikely to occur by chance.</p>
</li>
<li>
<p>permutation test: A way to compute p-values by generating permutations
of an observed dataset.</p>
</li>
<li>
<p>resampling test: A way to compute p-values by generating samples, with
replacement, from an observed dataset.</p>
</li>
<li>
<p>two-sided test: A test that asks, &#8220;What is the chance of an effect as
big as the observed effect, positive or negative?&#8221;</p>
</li>
<li>
<p>one-sided test: A test that asks, &#8220;What is the chance of an effect as
big as the observed effect, and with the same sign?&#8221;</p>
</li>
<li>
<p>chi-squared test: A test that uses the chi-squared statistic as the
test statistic.</p>
</li>
<li>
<p>false positive: The conclusion that an effect is real when it is not.</p>
</li>
<li>
<p>false negative: The conclusion that an effect is due to chance when it
is not.</p>
</li>
<li>
<p>power: The probability of a positive test if the null hypothesis is
false.</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="linear"><a class="anchor" href="#linear"></a><a class="link" href="#linear">10. Linear least squares</a></h2>
<div class="sectionbody">
<div class="paragraph">
<p>The code for this chapter is in linear.py. For information about
downloading and working with this code, see Section <a href="#code">Using the code</a>.</p>
</div>
<div class="sect2">
<h3 id="_least_squares_fit"><a class="anchor" href="#_least_squares_fit"></a><a class="link" href="#_least_squares_fit">10.1. Least squares fit</a></h3>
<div class="paragraph">
<p>Correlation coefficients measure the strength and sign of a
relationship, but not the slope. There are several ways to estimate the
slope; the most common is a <strong>linear least squares fit</strong>. A &#8220;linear fit&#8221;
is a line intended to model the relationship between variables. A
&#8220;least squares&#8221; fit is one that minimizes the mean squared error (MSE)
between the line and the data.</p>
</div>
<div class="paragraph">
<p>Suppose we have a sequence of points, ys, that we want to express as a
function of another sequence xs. If there is a linear relationship
between xs and ys with intercept inter and slope slope, we expect each
y[i] to be inter + slope * x[i].</p>
</div>
<div class="paragraph">
<p>But unless the correlation is perfect, this prediction is only
approximate. The vertical deviation from the line, or <strong>residual</strong>, is</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">res = ys - (inter + slope * xs)</code></pre>
</div>
</div>
<div class="paragraph">
<p>The residuals might be due to random factors like measurement error, or
non-random factors that are unknown. For example, if we are trying to
predict weight as a function of height, unknown factors might include
diet, exercise, and body type.</p>
</div>
<div class="paragraph">
<p>If we get the parameters inter and slope wrong, the residuals get
bigger, so it makes intuitive sense that the parameters we want are the
ones that minimize the residuals.</p>
</div>
<div class="paragraph">
<p>We might try to minimize the absolute value of the residuals, or their
squares, or their cubes; but the most common choice is to minimize the
sum of squared residuals, sum(res**2).</p>
</div>
<div class="paragraph">
<p>Why? There are three good reasons and one less important one:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Squaring has the feature of treating positive and negative residuals
the same, which is usually what we want.</p>
</li>
<li>
<p>Squaring gives more weight to large residuals, but not so much weight
that the largest residual always dominates.</p>
</li>
<li>
<p>If the residuals are uncorrelated and normally distributed with mean 0
and constant (but unknown) variance, then the least squares fit is also
the maximum likelihood estimator of inter and slope. See
<a href="https://en.wikipedia.org/wiki/Linear_regression" class="bare">https://en.wikipedia.org/wiki/Linear_regression</a>.</p>
</li>
<li>
<p>The values of inter and slope that minimize the squared residuals can
be computed efficiently.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The last reason made sense when computational efficiency was more
important than choosing the method most appropriate to the problem at
hand. That’s no longer the case, so it is worth considering whether
squared residuals are the right thing to minimize.</p>
</div>
<div class="paragraph">
<p>For example, if you are using xs to predict values of ys, guessing too
high might be better (or worse) than guessing too low. In that case you
might want to compute some cost function for each residual, and minimize
total cost, sum(cost(res)). However, computing a least squares fit is
quick, easy and often good enough.</p>
</div>
</div>
<div class="sect2">
<h3 id="_implementation"><a class="anchor" href="#_implementation"></a><a class="link" href="#_implementation">10.2. Implementation</a></h3>
<div class="paragraph">
<p>thinkstats2 provides simple functions that demonstrate linear least
squares:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def LeastSquares(xs, ys):
    meanx, varx = MeanVar(xs)
    meany = Mean(ys)

    slope = Cov(xs, ys, meanx, meany) / varx
    inter = meany - slope * meanx

    return inter, slope</code></pre>
</div>
</div>
<div class="paragraph">
<p>LeastSquares takes sequences xs and ys and returns the estimated
parameters inter and slope. For details on how it works, see
<a href="http://wikipedia.org/wiki/Numerical_methods_for_linear_least_squares" class="bare">http://wikipedia.org/wiki/Numerical_methods_for_linear_least_squares</a>.</p>
</div>
<div class="paragraph">
<p>thinkstats2 also provides FitLine, which takes inter and slope and
returns the fitted line for a sequence of xs.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def FitLine(xs, inter, slope):
    fit_xs = np.sort(xs)
    fit_ys = inter + slope * fit_xs
    return fit_xs, fit_ys</code></pre>
</div>
</div>
<div class="paragraph">
<p>We can use these functions to compute the least squares fit for birth
weight as a function of mother’s age.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    live, firsts, others = first.MakeFrames()
    live = live.dropna(subset=['agepreg', 'totalwgt_lb'])
    ages = live.agepreg
    weights = live.totalwgt_lb

    inter, slope = thinkstats2.LeastSquares(ages, weights)
    fit_xs, fit_ys = thinkstats2.FitLine(ages, inter, slope)</code></pre>
</div>
</div>
<div class="paragraph">
<p>The estimated intercept and slope are 6.8 lbs and 0.017 lbs per year.
These values are hard to interpret in this form: the intercept is the
expected weight of a baby whose mother is 0 years old, which doesn’t
make sense in context, and the slope is too small to grasp easily.</p>
</div>
<div class="paragraph">
<p>Instead of presenting the intercept at \(x=0\), it is often
helpful to present the intercept at the mean of \(x\). In this
case the mean age is about 25 years and the mean baby weight for a 25
year old mother is 7.3 pounds. The slope is 0.27 ounces per year, or
0.17 pounds per decade.</p>
</div>
<div class="paragraph">
<p>image::figs/linear1.png[Scatter plot of birth weight and mother’s age
with a linear fit.,height=240]</p>
</div>
<div class="paragraph">
<p>Figure <a href="#linear1">[linear1]</a> shows a scatter plot of birth weight and age
along with the fitted line. It’s a good idea to look at a figure like
this to assess whether the relationship is linear and whether the fitted
line seems like a good model of the relationship.</p>
</div>
</div>
<div class="sect2">
<h3 id="_residuals"><a class="anchor" href="#_residuals"></a><a class="link" href="#_residuals">10.3. Residuals</a></h3>
<div class="paragraph">
<p>Another useful test is to plot the residuals. thinkstats2 provides a
function that computes residuals:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def Residuals(xs, ys, inter, slope):
    xs = np.asarray(xs)
    ys = np.asarray(ys)
    res = ys - (inter + slope * xs)
    return res</code></pre>
</div>
</div>
<div class="paragraph">
<p>Residuals takes sequences xs and ys and estimated parameters inter and
slope. It returns the differences between the actual values and the
fitted line.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="figs/linear2.png" alt="Residuals of the linear fit." height="240">
</div>
</div>
<div class="paragraph">
<p>To visualize the residuals, I group respondents by age and compute
percentiles in each group, as we saw in
Section <a href="#characterizing">7.2</a>. Figure <a href="#linear2">[linear2]</a> shows the
25th, 50th and 75th percentiles of the residuals for each age group. The
median is near zero, as expected, and the interquartile range is about 2
pounds. So if we know the mother’s age, we can guess the baby’s weight
within a pound, about 50% of the time.</p>
</div>
<div class="paragraph">
<p>Ideally these lines should be flat, indicating that the residuals are
random, and parallel, indicating that the variance of the residuals is
the same for all age groups. In fact, the lines are close to parallel,
so that’s good; but they have some curvature, indicating that the
relationship is nonlinear. Nevertheless, the linear fit is a simple
model that is probably good enough for some purposes.</p>
</div>
</div>
<div class="sect2">
<h3 id="regest"><a class="anchor" href="#regest"></a><a class="link" href="#regest">10.4. Estimation</a></h3>
<div class="paragraph">
<p>The parameters slope and inter are estimates based on a sample; like
other estimates, they are vulnerable to sampling bias, measurement
error, and sampling error. As discussed in Chapter <a href="#estimation">8</a>,
sampling bias is caused by non-representative sampling, measurement
error is caused by errors in collecting and recording data, and sampling
error is the result of measuring a sample rather than the entire
population.</p>
</div>
<div class="paragraph">
<p>To assess sampling error, we ask, &#8220;If we run this experiment again, how
much variability do we expect in the estimates?&#8221; We can answer this
question by running simulated experiments and computing sampling
distributions of the estimates.</p>
</div>
<div class="paragraph">
<p>I simulate the experiments by resampling the data; that is, I treat the
observed pregnancies as if they were the entire population and draw
samples, with replacement, from the observed sample.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def SamplingDistributions(live, iters=101):
    t = []
    for _ in range(iters):
        sample = thinkstats2.ResampleRows(live)
        ages = sample.agepreg
        weights = sample.totalwgt_lb
        estimates = thinkstats2.LeastSquares(ages, weights)
        t.append(estimates)

    inters, slopes = zip(*t)
    return inters, slopes</code></pre>
</div>
</div>
<div class="paragraph">
<p>SamplingDistributions takes a DataFrame with one row per live birth, and
iters, the number of experiments to simulate. It uses ResampleRows to
resample the observed pregnancies. We’ve already seen SampleRows, which
chooses random rows from a DataFrame. thinkstats2 also provides
ResampleRows, which returns a sample the same size as the original:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def ResampleRows(df):
    return SampleRows(df, len(df), replace=True)</code></pre>
</div>
</div>
<div class="paragraph">
<p>After resampling, we use the simulated sample to estimate parameters.
The result is two sequences: the estimated intercepts and estimated
slopes.</p>
</div>
<div class="paragraph">
<p>I summarize the sampling distributions by printing the standard error
and confidence interval:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def Summarize(estimates, actual=None):
    mean = thinkstats2.Mean(estimates)
    stderr = thinkstats2.Std(estimates, mu=actual)
    cdf = thinkstats2.Cdf(estimates)
    ci = cdf.ConfidenceInterval(90)
    print('mean, SE, CI', mean, stderr, ci)</code></pre>
</div>
</div>
<div class="paragraph">
<p>Summarize takes a sequence of estimates and the actual value. It prints
the mean of the estimates, the standard error and a 90% confidence
interval.</p>
</div>
<div class="paragraph">
<p>For the intercept, the mean estimate is 6.83, with standard error 0.07
and 90% confidence interval (6.71, 6.94). The estimated slope, in more
compact form, is 0.0174, SE 0.0028, CI (0.0126, 0.0220). There is almost
a factor of two between the low and high ends of this CI, so it should
be considered a rough estimate.</p>
</div>
<div class="paragraph">
<p>To visualize the sampling error of the estimate, we could plot all of
the fitted lines, or for a less cluttered representation, plot a 90%
confidence interval for each age. Here’s the code:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def PlotConfidenceIntervals(xs, inters, slopes,
                            percent=90, **options):
    fys_seq = []
    for inter, slope in zip(inters, slopes):
        fxs, fys = thinkstats2.FitLine(xs, inter, slope)
        fys_seq.append(fys)

    p = (100 - percent) / 2
    percents = p, 100 - p
    low, high = thinkstats2.PercentileRows(fys_seq, percents)
    thinkplot.FillBetween(fxs, low, high, **options)</code></pre>
</div>
</div>
<div class="paragraph">
<p>xs is the sequence of mother’s age. inters and slopes are the estimated
parameters generated by SamplingDistributions. percent indicates which
confidence interval to plot.</p>
</div>
<div class="paragraph">
<p>PlotConfidenceIntervals generates a fitted line for each pair of inter
and slope and stores the results in a sequence, <code>fys_seq</code>. Then it
uses PercentileRows to select the upper and lower percentiles of y for
each value of x. For a 90% confidence interval, it selects the 5th and
95th percentiles. FillBetween draws a polygon that fills the space
between two lines.</p>
</div>
<div class="paragraph">
<p>image::figs/linear3.png[50% and 90% confidence intervals showing
variability in the fitted line due to sampling error of inter and
slope.,height=240]</p>
</div>
<div class="paragraph">
<p>Figure <a href="#linear3">[linear3]</a> shows the 50% and 90% confidence intervals
for curves fitted to birth weight as a function of mother’s age. The
vertical width of the region represents the effect of sampling error;
the effect is smaller for values near the mean and larger for the
extremes.</p>
</div>
</div>
<div class="sect2">
<h3 id="goodness"><a class="anchor" href="#goodness"></a><a class="link" href="#goodness">10.5. Goodness of fit</a></h3>
<div class="paragraph">
<p>There are several ways to measure the quality of a linear model, or
<strong>goodness of fit</strong>. One of the simplest is the standard deviation of the
residuals.</p>
</div>
<div class="paragraph">
<p>If you use a linear model to make predictions, Std(res) is the root mean
squared error (RMSE) of your predictions. For example, if you use
mother’s age to guess birth weight, the RMSE of your guess would be 1.40
lbs.</p>
</div>
<div class="paragraph">
<p>If you guess birth weight without knowing the mother’s age, the RMSE of
your guess is Std(ys), which is 1.41 lbs. So in this example, knowing a
mother’s age does not improve the predictions substantially.</p>
</div>
<div class="paragraph">
<p>Another way to measure goodness of fit is the <strong>coefficient of
determination</strong>, usually denoted \(R^2\) and called
&#8220;R-squared&#8221;:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def CoefDetermination(ys, res):
    return 1 - Var(res) / Var(ys)</code></pre>
</div>
</div>
<div class="paragraph">
<p>Var(res) is the MSE of your guesses using the model, Var(ys) is the MSE
without it. So their ratio is the fraction of MSE that remains if you
use the model, and \(R^2\) is the fraction of MSE the model
eliminates.</p>
</div>
<div class="paragraph">
<p>For birth weight and mother’s age, \(R^2\) is 0.0047, which
means that mother’s age predicts about half of 1% of variance in birth
weight.</p>
</div>
<div class="paragraph">
<p>There is a simple relationship between the coefficient of determination
and Pearson’s coefficient of correlation: \(R^2 = \rho^2\). For
example, if \(\rho\) is 0.8 or -0.8, \(R^2 = 0.64\).</p>
</div>
<div class="paragraph">
<p>Although \(\rho\) and \(R^2\) are often used to quantify
the strength of a relationship, they are not easy to interpret in terms
of predictive power. In my opinion, Std(res) is the best representation
of the quality of prediction, especially if it is presented in relation
to Std(ys).</p>
</div>
<div class="paragraph">
<p>For example, when people talk about the validity of the SAT (a
standardized test used for college admission in the U.S.) they often
talk about correlations between SAT scores and other measures of
intelligence.</p>
</div>
<div class="paragraph">
<p>According to one study, there is a Pearson correlation of
\(\rho=0.72\) between total SAT scores and IQ scores, which
sounds like a strong correlation. But \(R^2 = \rho^2 = 0.52\),
so SAT scores account for only 52% of variance in IQ.</p>
</div>
<div class="paragraph">
<p>IQ scores are normalized with Std(ys) = 15, so</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; var_ys = 15**2
&gt;&gt;&gt; rho = 0.72
&gt;&gt;&gt; r2 = rho**2
&gt;&gt;&gt; var_res = (1 - r2) * var_ys
&gt;&gt;&gt; std_res = math.sqrt(var_res)
10.4096</pre>
</div>
</div>
<div class="paragraph">
<p>So using SAT score to predict IQ reduces RMSE from 15 points to 10.4
points. A correlation of 0.72 yields a reduction in RMSE of only 31%.</p>
</div>
<div class="paragraph">
<p>If you see a correlation that looks impressive, remember that
\(R^2\) is a better indicator of reduction in MSE, and reduction
in RMSE is a better indicator of predictive power.</p>
</div>
</div>
<div class="sect2">
<h3 id="_testing_a_linear_model"><a class="anchor" href="#_testing_a_linear_model"></a><a class="link" href="#_testing_a_linear_model">10.6. Testing a linear model</a></h3>
<div class="paragraph">
<p>The effect of mother’s age on birth weight is small, and has little
predictive power. So is it possible that the apparent relationship is
due to chance? There are several ways we might test the results of a
linear fit.</p>
</div>
<div class="paragraph">
<p>One option is to test whether the apparent reduction in MSE is due to
chance. In that case, the test statistic is \(R^2\) and the null
hypothesis is that there is no relationship between the variables. We
can simulate the null hypothesis by permutation, as in
Section <a href="#corrtest">9.5</a>, when we tested the correlation between
mother’s age and birth weight. In fact, because
\(R^2 = \rho^2\), a one-sided test of \(R^2\) is
equivalent to a two-sided test of \(\rho\). We’ve already done
that test, and found \(p &lt; 0.001\), so we conclude that the
apparent relationship between mother’s age and birth weight is
statistically significant.</p>
</div>
<div class="paragraph">
<p>Another approach is to test whether the apparent slope is due to chance.
The null hypothesis is that the slope is actually zero; in that case we
can model the birth weights as random variations around their mean.
Here’s a HypothesisTest for this model:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">class SlopeTest(thinkstats2.HypothesisTest):

    def TestStatistic(self, data):
        ages, weights = data
        _, slope = thinkstats2.LeastSquares(ages, weights)
        return slope

    def MakeModel(self):
        _, weights = self.data
        self.ybar = weights.mean()
        self.res = weights - self.ybar

    def RunModel(self):
        ages, _ = self.data
        weights = self.ybar + np.random.permutation(self.res)
        return ages, weights</code></pre>
</div>
</div>
<div class="paragraph">
<p>The data are represented as sequences of ages and weights. The test
statistic is the slope estimated by LeastSquares. The model of the null
hypothesis is represented by the mean weight of all babies and the
deviations from the mean. To generate simulated data, we permute the
deviations and add them to the mean.</p>
</div>
<div class="paragraph">
<p>Here’s the code that runs the hypothesis test:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    live, firsts, others = first.MakeFrames()
    live = live.dropna(subset=['agepreg', 'totalwgt_lb'])
    ht = SlopeTest((live.agepreg, live.totalwgt_lb))
    pvalue = ht.PValue()</code></pre>
</div>
</div>
<div class="paragraph">
<p>The p-value is less than \(0.001\), so although the estimated
slope is small, it is unlikely to be due to chance.</p>
</div>
<div class="paragraph">
<p>Estimating the p-value by simulating the null hypothesis is strictly
correct, but there is a simpler alternative. Remember that we already
computed the sampling distribution of the slope, in
Section <a href="#regest">10.4</a>. To do that, we assumed that the observed
slope was correct and simulated experiments by resampling.</p>
</div>
<div class="paragraph">
<p>Figure <a href="#linear4">[linear4]</a> shows the sampling distribution of the slope,
from Section <a href="#regest">10.4</a>, and the distribution of slopes
generated under the null hypothesis. The sampling distribution is
centered about the estimated slope, 0.017 lbs/year, and the slopes under
the null hypothesis are centered around 0; but other than that, the
distributions are identical. The distributions are also symmetric, for
reasons we will see in Section <a href="#CLT">14.4</a>.</p>
</div>
<div class="paragraph">
<p>image::figs/linear4.png[The sampling distribution of the estimated slope
and the distribution of slopes generated under the null hypothesis. The
vertical lines are at 0 and the observed slope, 0.017
lbs/year.,height=240]</p>
</div>
<div class="paragraph">
<p>So we could estimate the p-value two ways:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Compute the probability that the slope under the null hypothesis
exceeds the observed slope.</p>
</li>
<li>
<p>Compute the probability that the slope in the sampling distribution
falls below 0. (If the estimated slope were negative, we would compute
the probability that the slope in the sampling distribution exceeds 0.)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The second option is easier because we normally want to compute the
sampling distribution of the parameters anyway. And it is a good
approximation unless the sample size is small <em>and</em> the distribution of
residuals is skewed. Even then, it is usually good enough, because
p-values don’t have to be precise.</p>
</div>
<div class="paragraph">
<p>Here’s the code that estimates the p-value of the slope using the
sampling distribution:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    inters, slopes = SamplingDistributions(live, iters=1001)
    slope_cdf = thinkstats2.Cdf(slopes)
    pvalue = slope_cdf[0]</code></pre>
</div>
</div>
<div class="paragraph">
<p>Again, we find \(p &lt; 0.001\).</p>
</div>
</div>
<div class="sect2">
<h3 id="weighted"><a class="anchor" href="#weighted"></a><a class="link" href="#weighted">10.7. Weighted resampling</a></h3>
<div class="paragraph">
<p>So far we have treated the NSFG data as if it were a representative
sample, but as I mentioned in Section <a href="#nsfg">1.2</a>, it is not. The
survey deliberately oversamples several groups in order to improve the
chance of getting statistically significant results; that is, in order
to improve the power of tests involving these groups.</p>
</div>
<div class="paragraph">
<p>This survey design is useful for many purposes, but it means that we
cannot use the sample to estimate values for the general population
without accounting for the sampling process.</p>
</div>
<div class="paragraph">
<p>For each respondent, the NSFG data includes a variable called finalwgt,
which is the number of people in the general population the respondent
represents. This value is called a <strong>sampling weight</strong>, or just
&#8220;weight.&#8221;</p>
</div>
<div class="paragraph">
<p>As an example, if you survey 100,000 people in a country of 300 million,
each respondent represents 3,000 people. If you oversample one group by
a factor of 2, each person in the oversampled group would have a lower
weight, about 1500.</p>
</div>
<div class="paragraph">
<p>To correct for oversampling, we can use resampling; that is, we can draw
samples from the survey using probabilities proportional to sampling
weights. Then, for any quantity we want to estimate, we can generate
sampling distributions, standard errors, and confidence intervals. As an
example, I will estimate mean birth weight with and without sampling
weights.</p>
</div>
<div class="paragraph">
<p>In Section <a href="#regest">10.4</a>, we saw ResampleRows, which chooses rows
from a DataFrame, giving each row the same probability. Now we need to
do the same thing using probabilities proportional to sampling weights.
ResampleRowsWeighted takes a DataFrame, resamples rows according to the
weights in finalwgt, and returns a DataFrame containing the resampled
rows:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def ResampleRowsWeighted(df, column='finalwgt'):
    weights = df[column]
    cdf = Cdf(dict(weights))
    indices = cdf.Sample(len(weights))
    sample = df.loc[indices]
    return sample</code></pre>
</div>
</div>
<div class="paragraph">
<p>weights is a Series; converting it to a dictionary makes a map from the
indices to the weights. In cdf the values are indices and the
probabilities are proportional to the weights.</p>
</div>
<div class="paragraph">
<p>indices is a sequence of row indices; sample is a DataFrame that
contains the selected rows. Since we sample with replacement, the same
row might appear more than once.</p>
</div>
<div class="paragraph">
<p>Now we can compare the effect of resampling with and without weights.
Without weights, we generate the sampling distribution like this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    estimates = [ResampleRows(live).totalwgt_lb.mean()
                 for _ in range(iters)]</code></pre>
</div>
</div>
<div class="paragraph">
<p>With weights, it looks like this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    estimates = [ResampleRowsWeighted(live).totalwgt_lb.mean()
                 for _ in range(iters)]</code></pre>
</div>
</div>
<div class="paragraph">
<p>The following table summarizes the results:</p>
</div>
<table class="tableblock frame-all grid-all spread">
<colgroup>
<col style="width: 25%;">
<col style="width: 25%;">
<col style="width: 25%;">
<col style="width: 25%;">
</colgroup>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">mean birth</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">standard</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">90% CI</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">weight (lbs)</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">error</p></td>
<td class="tableblock halign-center valign-top"></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Unweighted</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">7.27</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">0.014</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">(7.24, 7.29)</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Weighted</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">7.35</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">0.014</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">(7.32, 7.37)</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>In this example, the effect of weighting is small but non-negligible.
The difference in estimated means, with and without weighting, is about
0.08 pounds, or 1.3 ounces. This difference is substantially larger than
the standard error of the estimate, 0.014 pounds, which implies that the
difference is not due to chance.</p>
</div>
</div>
<div class="sect2">
<h3 id="_exercises_10"><a class="anchor" href="#_exercises_10"></a><a class="link" href="#_exercises_10">10.8. Exercises</a></h3>
<div class="paragraph">
<p>A solution to this exercise is in <code>chap10soln.ipynb</code></p>
</div>
<div class="paragraph">
<p>Using the data from the BRFSS, compute the linear least squares fit for
log(weight) versus height. How would you best present the estimated
parameters for a model like this where one of the variables is
log-transformed? If you were trying to guess someone’s weight, how much
would it help to know their height?</p>
</div>
<div class="paragraph">
<p>Like the NSFG, the BRFSS oversamples some groups and provides a sampling
weight for each respondent. In the BRFSS data, the variable name for
these weights is finalwt. Use resampling, with and without weights, to
estimate the mean height of respondents in the BRFSS, the standard error
of the mean, and a 90% confidence interval. How much does correct
weighting affect the estimates?</p>
</div>
</div>
<div class="sect2">
<h3 id="_glossary_10"><a class="anchor" href="#_glossary_10"></a><a class="link" href="#_glossary_10">10.9. Glossary</a></h3>
<div class="ulist">
<ul>
<li>
<p>linear fit: a line intended to model the relationship between
variables.</p>
</li>
<li>
<p>least squares fit: A model of a dataset that minimizes the sum of
squares of the residuals.</p>
</li>
<li>
<p>residual: The deviation of an actual value from a model.</p>
</li>
<li>
<p>goodness of fit: A measure of how well a model fits data.</p>
</li>
<li>
<p>coefficient of determination: A statistic intended to quantify
goodness of fit.</p>
</li>
<li>
<p>sampling weight: A value associated with an observation in a sample
that indicates what part of the population it represents.</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_regression"><a class="anchor" href="#_regression"></a><a class="link" href="#_regression">11. Regression</a></h2>
<div class="sectionbody">
<div class="paragraph">
<p>The linear least squares fit in the previous chapter is an example of
<strong>regression</strong>, which is the more general problem of fitting any kind of
model to any kind of data. This use of the term &#8220;regression&#8221; is a
historical accident; it is only indirectly related to the original
meaning of the word.</p>
</div>
<div class="paragraph">
<p>The goal of regression analysis is to describe the relationship between
one set of variables, called the <strong>dependent variables</strong>, and another set
of variables, called independent or <strong>explanatory variables</strong>.</p>
</div>
<div class="paragraph">
<p>In the previous chapter we used mother’s age as an explanatory variable
to predict birth weight as a dependent variable. When there is only one
dependent and one explanatory variable, that’s <strong>simple regression</strong>. In
this chapter, we move on to <strong>multiple regression</strong>, with more than one
explanatory variable. If there is more than one dependent variable,
that’s multivariate regression.</p>
</div>
<div class="paragraph">
<p>If the relationship between the dependent and explanatory variable is
linear, that’s <strong>linear regression</strong>. For example, if the dependent
variable is \(y\) and the explanatory variables are
\(x_1\) and \(x_2\), we would write the following linear
regression model:</p>
</div>
<div class="stemblock">
<div class="content">
\[y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \varepsilon\]
</div>
</div>
<div class="paragraph">
<p>where \(\beta_0\) is the intercept, \(\beta_1\) is the
parameter associated with \(x_1\), \(\beta_2\) is the
parameter associated with \(x_2\), and \(\varepsilon\)
is the residual due to random variation or other unknown factors.</p>
</div>
<div class="paragraph">
<p>Given a sequence of values for \(y\) and sequences for
\(x_1\) and \(x_2\), we can find the parameters,
\(\beta_0\), \(\beta_1\), and \(\beta_2\), that
minimize the sum of \(\varepsilon^2\). This process is called
<strong>ordinary least squares</strong>. The computation is similar to
thinkstats2.LeastSquare, but generalized to deal with more than one
explanatory variable. You can find the details at
<a href="https://en.wikipedia.org/wiki/Ordinary_least_squares" class="bare">https://en.wikipedia.org/wiki/Ordinary_least_squares</a></p>
</div>
<div class="paragraph">
<p>The code for this chapter is in regression.py. For information about
downloading and working with this code, see Section <a href="#code">Using the code</a>.</p>
</div>
<div class="sect2">
<h3 id="_statsmodels"><a class="anchor" href="#_statsmodels"></a><a class="link" href="#_statsmodels">11.1. StatsModels</a></h3>
<div class="paragraph">
<p>In the previous chapter I presented thinkstats2.LeastSquares, an
implementation of simple linear regression intended to be easy to read.
For multiple regression we’ll switch to StatsModels, a Python package
that provides several forms of regression and other analyses. If you are
using Anaconda, you already have StatsModels; otherwise you might have
to install it.</p>
</div>
<div class="paragraph">
<p>As an example, I’ll run the model from the previous chapter with
StatsModels:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    import statsmodels.formula.api as smf

    live, firsts, others = first.MakeFrames()
    formula = 'totalwgt_lb ~ agepreg'
    model = smf.ols(formula, data=live)
    results = model.fit()</code></pre>
</div>
</div>
<div class="paragraph">
<p>statsmodels provides two interfaces (APIs); the &#8220;formula&#8221; API uses
strings to identify the dependent and explanatory variables. It uses a
syntax called patsy; in this example, the <code>~</code> operator separates the
dependent variable on the left from the explanatory variables on the
right.</p>
</div>
<div class="paragraph">
<p>smf.ols takes the formula string and the DataFrame, live, and returns an
OLS object that represents the model. The name ols stands for &#8220;ordinary
least squares.&#8221;</p>
</div>
<div class="paragraph">
<p>The fit method fits the model to the data and returns a
RegressionResults object that contains the results.</p>
</div>
<div class="paragraph">
<p>The results are also available as attributes. params is a Series that
maps from variable names to their parameters, so we can get the
intercept and slope like this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    inter = results.params['Intercept']
    slope = results.params['agepreg']</code></pre>
</div>
</div>
<div class="paragraph">
<p>The estimated parameters are 6.83 and 0.0175, the same as from
LeastSquares.</p>
</div>
<div class="paragraph">
<p>pvalues is a Series that maps from variable names to the associated
p-values, so we can check whether the estimated slope is statistically
significant:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    slope_pvalue = results.pvalues['agepreg']</code></pre>
</div>
</div>
<div class="paragraph">
<p>The p-value associated with agepreg is 5.7e-11, which is less than
\(0.001\), as expected.</p>
</div>
<div class="paragraph">
<p>results.rsquared contains \(R^2\), which is \(0.0047\).
results also provides <code>f_pvalue</code>, which is the p-value associated with
the model as a whole, similar to testing whether \(R^2\) is
statistically significant.</p>
</div>
<div class="paragraph">
<p>And results provides resid, a sequence of residuals, and fittedvalues, a
sequence of fitted values corresponding to agepreg.</p>
</div>
<div class="paragraph">
<p>The results object provides summary(), which represents the results in a
readable format.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    print(results.summary())</code></pre>
</div>
</div>
<div class="paragraph">
<p>But it prints a lot of information that is not relevant (yet), so I use
a simpler function called SummarizeResults. Here are the results of this
model:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>Intercept       6.83    (0)
agepreg         0.0175  (5.72e-11)
R^2 0.004738
Std(ys) 1.408
Std(res) 1.405</pre>
</div>
</div>
<div class="paragraph">
<p>Std(ys) is the standard deviation of the dependent variable, which is
the RMSE if you have to guess birth weights without the benefit of any
explanatory variables. Std(res) is the standard deviation of the
residuals, which is the RMSE if your guesses are informed by the
mother’s age. As we have already seen, knowing the mother’s age provides
no substantial improvement to the predictions.</p>
</div>
</div>
<div class="sect2">
<h3 id="multiple"><a class="anchor" href="#multiple"></a><a class="link" href="#multiple">11.2. Multiple regression</a></h3>
<div class="paragraph">
<p>In Section <a href="#birth_weights">4.5</a> we saw that first babies tend to be
lighter than others, and this effect is statistically significant. But
it is a strange result because there is no obvious mechanism that would
cause first babies to be lighter. So we might wonder whether this
relationship is <strong>spurious</strong>.</p>
</div>
<div class="paragraph">
<p>In fact, there is a possible explanation for this effect. We have seen
that birth weight depends on mother’s age, and we might expect that
mothers of first babies are younger than others.</p>
</div>
<div class="paragraph">
<p>With a few calculations we can check whether this explanation is
plausible. Then we’ll use multiple regression to investigate more
carefully. First, let’s see how big the difference in weight is:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">diff_weight = firsts.totalwgt_lb.mean() - others.totalwgt_lb.mean()</code></pre>
</div>
</div>
<div class="paragraph">
<p>First babies are 0.125 lbs lighter, or 2 ounces. And the difference in
ages:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">diff_age = firsts.agepreg.mean() - others.agepreg.mean()</code></pre>
</div>
</div>
<div class="paragraph">
<p>The mothers of first babies are 3.59 years younger. Running the linear
model again, we get the change in birth weight as a function of age:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">results = smf.ols('totalwgt_lb ~ agepreg', data=live).fit()
slope = results.params['agepreg']</code></pre>
</div>
</div>
<div class="paragraph">
<p>The slope is 0.0175 pounds per year. If we multiply the slope by the
difference in ages, we get the expected difference in birth weight for
first babies and others, due to mother’s age:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">slope * diff_age</code></pre>
</div>
</div>
<div class="paragraph">
<p>The result is 0.063, just about half of the observed difference. So we
conclude, tentatively, that the observed difference in birth weight can
be partly explained by the difference in mother’s age.</p>
</div>
<div class="paragraph">
<p>Using multiple regression, we can explore these relationships more
systematically.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    live['isfirst'] = live.birthord == 1
    formula = 'totalwgt_lb ~ isfirst'
    results = smf.ols(formula, data=live).fit()</code></pre>
</div>
</div>
<div class="paragraph">
<p>The first line creates a new column named isfirst that is True for first
babies and false otherwise. Then we fit a model using isfirst as an
explanatory variable.</p>
</div>
<div class="paragraph">
<p>Here are the results:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>Intercept         7.33   (0)
isfirst[T.True]  -0.125  (2.55e-05)
R^2 0.00196</pre>
</div>
</div>
<div class="paragraph">
<p>Because isfirst is a boolean, ols treats it as a <strong>categorical variable</strong>,
which means that the values fall into categories, like True and False,
and should not be treated as numbers. The estimated parameter is the
effect on birth weight when isfirst is true, so the result, -0.125 lbs,
is the difference in birth weight between first babies and others.</p>
</div>
<div class="paragraph">
<p>The slope and the intercept are statistically significant, which means
that they were unlikely to occur by chance, but the the \(R^2\)
value for this model is small, which means that isfirst doesn’t account
for a substantial part of the variation in birth weight.</p>
</div>
<div class="paragraph">
<p>The results are similar with agepreg:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>Intercept       6.83    (0)
agepreg         0.0175  (5.72e-11)
R^2 0.004738</pre>
</div>
</div>
<div class="paragraph">
<p>Again, the parameters are statistically significant, but \(R^2\)
is low.</p>
</div>
<div class="paragraph">
<p>These models confirm results we have already seen. But now we can fit a
single model that includes both variables. With the formula
<code>totalwgt_lb ~ isfirst + agepreg</code>, we get:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>Intercept        6.91    (0)
isfirst[T.True] -0.0698  (0.0253)
agepreg          0.0154  (3.93e-08)
R^2 0.005289</pre>
</div>
</div>
<div class="paragraph">
<p>In the combined model, the parameter for isfirst is smaller by about
half, which means that part of the apparent effect of isfirst is
actually accounted for by agepreg. And the p-value for isfirst is about
2.5%, which is on the border of statistical significance.</p>
</div>
<div class="paragraph">
<p>\(R^2\) for this model is a little higher, which indicates that
the two variables together account for more variation in birth weight
than either alone (but not by much).</p>
</div>
</div>
<div class="sect2">
<h3 id="nonlinear"><a class="anchor" href="#nonlinear"></a><a class="link" href="#nonlinear">11.3. Nonlinear relationships</a></h3>
<div class="paragraph">
<p>Remembering that the contribution of agepreg might be nonlinear, we
might consider adding a variable to capture more of this relationship.
One option is to create a column, agepreg2, that contains the squares of
the ages:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    live['agepreg2'] = live.agepreg**2
    formula = 'totalwgt_lb ~ isfirst + agepreg + agepreg2'</code></pre>
</div>
</div>
<div class="paragraph">
<p>Now by estimating parameters for agepreg and agepreg2, we are
effectively fitting a parabola:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>Intercept        5.69     (1.38e-86)
isfirst[T.True] -0.0504   (0.109)
agepreg          0.112    (3.23e-07)
agepreg2        -0.00185  (8.8e-06)
R^2 0.007462</pre>
</div>
</div>
<div class="paragraph">
<p>The parameter of agepreg2 is negative, so the parabola curves downward,
which is consistent with the shape of the lines in
Figure <a href="#linear2">[linear2]</a>.</p>
</div>
<div class="paragraph">
<p>The quadratic model of agepreg accounts for more of the variability in
birth weight; the parameter for isfirst is smaller in this model, and no
longer statistically significant.</p>
</div>
<div class="paragraph">
<p>Using computed variables like agepreg2 is a common way to fit
polynomials and other functions to data. This process is still
considered linear regression, because the dependent variable is a linear
function of the explanatory variables, regardless of whether some
variables are nonlinear functions of others.</p>
</div>
<div class="paragraph">
<p>The following table summarizes the results of these regressions:</p>
</div>
<table class="tableblock frame-all grid-all spread">
<colgroup>
<col style="width: 20%;">
<col style="width: 20%;">
<col style="width: 20%;">
<col style="width: 20%;">
<col style="width: 20%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top"></th>
<th class="tableblock halign-center valign-top">isfirst</th>
<th class="tableblock halign-center valign-top">agepreg</th>
<th class="tableblock halign-center valign-top">agepreg2</th>
<th class="tableblock halign-center valign-top">\(R^2\)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Model 1</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">-0.125 *</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">–</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">–</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">0.002</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Model 2</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">–</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">0.0175 *</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">–</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">0.0047</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Model 3</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">-0.0698 (0.025)</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">0.0154 *</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">–</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">0.0053</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Model 4</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">-0.0504 (0.11)</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">0.112 *</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">-0.00185 *</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">0.0075</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>The columns in this table are the explanatory variables and the
coefficient of determination, \(R^2\). Each entry is an
estimated parameter and either a p-value in parentheses or an asterisk
to indicate a p-value less that 0.001.</p>
</div>
<div class="paragraph">
<p>We conclude that the apparent difference in birth weight is explained,
at least in part, by the difference in mother’s age. When we include
mother’s age in the model, the effect of isfirst gets smaller, and the
remaining effect might be due to chance.</p>
</div>
<div class="paragraph">
<p>In this example, mother’s age acts as a <strong>control variable</strong>; including
agepreg in the model &#8220;controls for&#8221; the difference in age between
first-time mothers and others, making it possible to isolate the effect
(if any) of isfirst.</p>
</div>
</div>
<div class="sect2">
<h3 id="mining"><a class="anchor" href="#mining"></a><a class="link" href="#mining">11.4. Data mining</a></h3>
<div class="paragraph">
<p>So far we have used regression models for explanation; for example, in
the previous section we discovered that an apparent difference in birth
weight is actually due to a difference in mother’s age. But the
\(R^2\) values of those models is very low, which means that
they have little predictive power. In this section we’ll try to do
better.</p>
</div>
<div class="paragraph">
<p>Suppose one of your co-workers is expecting a baby and there is an
office pool to guess the baby’s birth weight (if you are not familiar
with betting pools, see <a href="https://en.wikipedia.org/wiki/Betting_pool" class="bare">https://en.wikipedia.org/wiki/Betting_pool</a>).</p>
</div>
<div class="paragraph">
<p>Now suppose that you <em>really</em> want to win the pool. What could you do to
improve your chances? Well, the NSFG dataset includes 244 variables
about each pregnancy and another 3087 variables about each respondent.
Maybe some of those variables have predictive power. To find out which
ones are most useful, why not try them all?</p>
</div>
<div class="paragraph">
<p>Testing the variables in the pregnancy table is easy, but in order to
use the variables in the respondent table, we have to match up each
pregnancy with a respondent. In theory we could iterate through the rows
of the pregnancy table, use the caseid to find the corresponding
respondent, and copy the values from the correspondent table into the
pregnancy table. But that would be slow.</p>
</div>
<div class="paragraph">
<p>A better option is to recognize this process as a <strong>join</strong> operation as
defined in SQL and other relational database languages (see
<a href="https://en.wikipedia.org/wiki/Join_(SQL)" class="bare">https://en.wikipedia.org/wiki/Join_(SQL)</a>). Join is implemented as a
DataFrame method, so we can perform the operation like this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    live = live[live.prglngth&gt;30]
    resp = chap01soln.ReadFemResp()
    resp.index = resp.caseid
    join = live.join(resp, on='caseid', rsuffix='_r')</code></pre>
</div>
</div>
<div class="paragraph">
<p>The first line selects records for pregnancies longer than 30 weeks,
assuming that the office pool is formed several weeks before the due
date.</p>
</div>
<div class="paragraph">
<p>The next line reads the respondent file. The result is a DataFrame with
integer indices; in order to look up respondents efficiently, I replace
resp.index with resp.caseid.</p>
</div>
<div class="paragraph">
<p>The join method is invoked on live, which is considered the &#8220;left&#8221;
table, and passed resp, which is the &#8220;right&#8221; table. The keyword
argument on indicates the variable used to match up rows from the two
tables.</p>
</div>
<div class="paragraph">
<p>In this example some column names appear in both tables, so we have to
provide rsuffix, which is a string that will be appended to the names of
overlapping columns from the right table. For example, both tables have
a column named race that encodes the race of the respondent. The result
of the join contains two columns named race and <code>race_r</code>.</p>
</div>
<div class="paragraph">
<p>The pandas implementation is fast. Joining the NSFG tables takes less
than a second on an ordinary desktop computer. Now we can start testing
variables.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    t = []
    for name in join.columns:
        try:
            if join[name].var() &lt; 1e-7:
                continue

            formula = 'totalwgt_lb ~ agepreg + ' + name
            model = smf.ols(formula, data=join)
            if model.nobs &lt; len(join)/2:
                continue

            results = model.fit()
        except (ValueError, TypeError):
            continue

        t.append((results.rsquared, name))</code></pre>
</div>
</div>
<div class="paragraph">
<p>For each variable we construct a model, compute \(R^2\), and
append the results to a list. The models all include agepreg, since we
already know that it has some predictive power.</p>
</div>
<div class="paragraph">
<p>I check that each explanatory variable has some variability; otherwise
the results of the regression are unreliable. I also check the number of
observations for each model. Variables that contain a large number of
nans are not good candidates for prediction.</p>
</div>
<div class="paragraph">
<p>For most of these variables, we haven’t done any cleaning. Some of them
are encoded in ways that don’t work very well for linear regression. As
a result, we might overlook some variables that would be useful if they
were cleaned properly. But maybe we will find some good candidates.</p>
</div>
</div>
<div class="sect2">
<h3 id="_prediction"><a class="anchor" href="#_prediction"></a><a class="link" href="#_prediction">11.5. Prediction</a></h3>
<div class="paragraph">
<p>The next step is to sort the results and select the variables that yield
the highest values of \(R^2\).</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    t.sort(reverse=True)
    for mse, name in t[:30]:
        print(name, mse)</code></pre>
</div>
</div>
<div class="paragraph">
<p>The first variable on the list is <code>totalwgt_lb</code>, followed by
<code>birthwgt_lb</code>. Obviously, we can’t use birth weight to predict birth
weight.</p>
</div>
<div class="paragraph">
<p>Similarly prglngth has useful predictive power, but for the office pool
we assume pregnancy length (and the related variables) are not known
yet.</p>
</div>
<div class="paragraph">
<p>The first useful predictive variable is babysex which indicates whether
the baby is male or female. In the NSFG dataset, boys are about 0.3 lbs
heavier. So, assuming that the sex of the baby is known, we can use it
for prediction.</p>
</div>
<div class="paragraph">
<p>Next is race, which indicates whether the respondent is white, black, or
other. As an explanatory variable, race can be problematic. In datasets
like the NSFG, race is correlated with many other variables, including
income and other socioeconomic factors. In a regression model, race acts
as a <strong>proxy variable</strong>, so apparent correlations with race are often
caused, at least in part, by other factors.</p>
</div>
<div class="paragraph">
<p>The next variable on the list is nbrnaliv, which indicates whether the
pregnancy yielded multiple births. Twins and triplets tend to be smaller
than other babies, so if we know whether our hypothetical co-worker is
expecting twins, that would help.</p>
</div>
<div class="paragraph">
<p>Next on the list is paydu, which indicates whether the respondent owns
her home. It is one of several income-related variables that turn out to
be predictive. In datasets like the NSFG, income and wealth are
correlated with just about everything. In this example, income is
related to diet, health, health care, and other factors likely to affect
birth weight.</p>
</div>
<div class="paragraph">
<p>Some of the other variables on the list are things that would not be
known until later, like bfeedwks, the number of weeks the baby was
breast fed. We can’t use these variables for prediction, but you might
want to speculate on reasons bfeedwks might be correlated with birth
weight.</p>
</div>
<div class="paragraph">
<p>Sometimes you start with a theory and use data to test it. Other times
you start with data and go looking for possible theories. The second
approach, which this section demonstrates, is called <strong>data mining</strong>. An
advantage of data mining is that it can discover unexpected patterns. A
hazard is that many of the patterns it discovers are either random or
spurious.</p>
</div>
<div class="paragraph">
<p>Having identified potential explanatory variables, I tested a few models
and settled on this one:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    formula = ('totalwgt_lb ~ agepreg + C(race) + babysex==1 + '
               'nbrnaliv&gt;1 + paydu==1 + totincr')
    results = smf.ols(formula, data=join).fit()</code></pre>
</div>
</div>
<div class="paragraph">
<p>This formula uses some syntax we have not seen yet: C(race) tells the
formula parser (Patsy) to treat race as a categorical variable, even
though it is encoded numerically.</p>
</div>
<div class="paragraph">
<p>The encoding for babysex is 1 for male, 2 for female; writing babysex==1
converts it to boolean, True for male and false for female.</p>
</div>
<div class="paragraph">
<p>Similarly nbrnaliv&gt;1 is True for multiple births and paydu==1 is True
for respondents who own their houses.</p>
</div>
<div class="paragraph">
<p>totincr is encoded numerically from 1-14, with each increment
representing about $5000 in annual income. So we can treat these values
as numerical, expressed in units of $5000.</p>
</div>
<div class="paragraph">
<p>Here are the results of the model:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>Intercept               6.63    (0)
C(race)[T.2]            0.357   (5.43e-29)
C(race)[T.3]            0.266   (2.33e-07)
babysex == 1[T.True]    0.295   (5.39e-29)
nbrnaliv &gt; 1[T.True]   -1.38    (5.1e-37)
paydu == 1[T.True]      0.12    (0.000114)
agepreg                 0.00741 (0.0035)
totincr                 0.0122  (0.00188)</pre>
</div>
</div>
<div class="paragraph">
<p>The estimated parameters for race are larger than I expected, especially
since we control for income. The encoding is 1 for black, 2 for white,
and 3 for other. Babies of black mothers are lighter than babies of
other races by 0.27–0.36 lbs.</p>
</div>
<div class="paragraph">
<p>As we’ve already seen, boys are heavier by about 0.3 lbs; twins and
other multiplets are lighter by 1.4 lbs.</p>
</div>
<div class="paragraph">
<p>People who own their homes have heavier babies by about 0.12 lbs, even
when we control for income. The parameter for mother’s age is smaller
than what we saw in Section <a href="#multiple">11.2</a>, which suggests that
some of the other variables are correlated with age, probably including
paydu and totincr.</p>
</div>
<div class="paragraph">
<p>All of these variables are statistically significant, some with very low
p-values, but \(R^2\) is only 0.06, still quite small. RMSE
without using the model is 1.27 lbs; with the model it drops to 1.23. So
your chance of winning the pool is not substantially improved. Sorry!</p>
</div>
</div>
<div class="sect2">
<h3 id="_logistic_regression"><a class="anchor" href="#_logistic_regression"></a><a class="link" href="#_logistic_regression">11.6. Logistic regression</a></h3>
<div class="paragraph">
<p>In the previous examples, some of the explanatory variables were
numerical and some categorical (including boolean). But the dependent
variable was always numerical.</p>
</div>
<div class="paragraph">
<p>Linear regression can be generalized to handle other kinds of dependent
variables. If the dependent variable is boolean, the generalized model
is called <strong>logistic regression</strong>. If the dependent variable is an integer
count, it’s called <strong>Poisson regression</strong>.</p>
</div>
<div class="paragraph">
<p>As an example of logistic regression, let’s consider a variation on the
office pool scenario. Suppose a friend of yours is pregnant and you want
to predict whether the baby is a boy or a girl. You could use data from
the NSFG to find factors that affect the &#8220;sex ratio&#8221;, which is
conventionally defined to be the probability of having a boy.</p>
</div>
<div class="paragraph">
<p>If you encode the dependent variable numerically, for example 0 for a
girl and 1 for a boy, you could apply ordinary least squares, but there
would be problems. The linear model might be something like this:</p>
</div>
<div class="stemblock">
<div class="content">
\[y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \varepsilon\]
</div>
</div>
<div class="paragraph">
<p>Where \(y\) is the dependent variable, and \(x_1\) and
\(x_2\) are explanatory variables. Then we could find the
parameters that minimize the residuals.</p>
</div>
<div class="paragraph">
<p>The problem with this approach is that it produces predictions that are
hard to interpret. Given estimated parameters and values for
\(x_1\) and \(x_2\), the model might predict
\(y=0.5\), but the only meaningful values of \(y\) are 0
and 1.</p>
</div>
<div class="paragraph">
<p>It is tempting to interpret a result like that as a probability; for
example, we might say that a respondent with particular values of
\(x_1\) and \(x_2\) has a 50% chance of having a boy.
But it is also possible for this model to predict \(y=1.1\) or
\(y=-0.1\), and those are not valid probabilities.</p>
</div>
<div class="paragraph">
<p>Logistic regression avoids this problem by expressing predictions in
terms of <strong>odds</strong> rather than probabilities. If you are not familiar with
odds, &#8220;odds in favor&#8221; of an event is the ratio of the probability it
will occur to the probability that it will not.</p>
</div>
<div class="paragraph">
<p>So if I think my team has a 75% chance of winning, I would say that the
odds in their favor are three to one, because the chance of winning is
three times the chance of losing.</p>
</div>
<div class="paragraph">
<p>Odds and probabilities are different representations of the same
information. Given a probability, you can compute the odds like this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    o = p / (1-p)</code></pre>
</div>
</div>
<div class="paragraph">
<p>Given odds in favor, you can convert to probability like this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    p = o / (o+1)</code></pre>
</div>
</div>
<div class="paragraph">
<p>Logistic regression is based on the following model:</p>
</div>
<div class="stemblock">
<div class="content">
\[\log o = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \varepsilon\]
</div>
</div>
<div class="paragraph">
<p>Where \(o\) is the odds in favor of a particular outcome; in the
example, \(o\) would be the odds of having a boy.</p>
</div>
<div class="paragraph">
<p>Suppose we have estimated the parameters \(\beta_0\),
\(\beta_1\), and \(\beta_2\) (I’ll explain how in a
minute). And suppose we are given values for \(x_1\) and
\(x_2\). We can compute the predicted value of
\(\log o\), and then convert to a probability:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    o = np.exp(log_o)
    p = o / (o+1)</code></pre>
</div>
</div>
<div class="paragraph">
<p>So in the office pool scenario we could compute the predictive
probability of having a boy. But how do we estimate the parameters?</p>
</div>
</div>
<div class="sect2">
<h3 id="_estimating_parameters"><a class="anchor" href="#_estimating_parameters"></a><a class="link" href="#_estimating_parameters">11.7. Estimating parameters</a></h3>
<div class="paragraph">
<p>Unlike linear regression, logistic regression does not have a closed
form solution, so it is solved by guessing an initial solution and
improving it iteratively.</p>
</div>
<div class="paragraph">
<p>The usual goal is to find the maximum-likelihood estimate (MLE), which
is the set of parameters that maximizes the likelihood of the data. For
example, suppose we have the following data:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; y = np.array([0, 1, 0, 1])
&gt;&gt;&gt; x1 = np.array([0, 0, 0, 1])
&gt;&gt;&gt; x2 = np.array([0, 1, 1, 1])</pre>
</div>
</div>
<div class="paragraph">
<p>And we start with the initial guesses \(\beta_0=-1.5\),
\(\beta_1=2.8\), and \(\beta_2=1.1\):</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; beta = [-1.5, 2.8, 1.1]</pre>
</div>
</div>
<div class="paragraph">
<p>Then for each row we can compute <code>log_o</code>:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; log_o = beta[0] + beta[1] * x1 + beta[2] * x2
[-1.5 -0.4 -0.4  2.4]</pre>
</div>
</div>
<div class="paragraph">
<p>And convert from log odds to probabilities:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; o = np.exp(log_o)
[  0.223   0.670   0.670  11.02  ]

&gt;&gt;&gt; p = o / (o+1)
[ 0.182  0.401  0.401  0.916 ]</pre>
</div>
</div>
<div class="paragraph">
<p>Notice that when <code>log_o</code> is greater than 0, o is greater than 1 and p
is greater than 0.5.</p>
</div>
<div class="paragraph">
<p>The likelihood of an outcome is p when y==1 and 1-p when y==0. For
example, if we think the probability of a boy is 0.8 and the outcome is
a boy, the likelihood is 0.8; if the outcome is a girl, the likelihood
is 0.2. We can compute that like this:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; likes = y * p + (1-y) * (1-p)
[ 0.817  0.401  0.598  0.916 ]</pre>
</div>
</div>
<div class="paragraph">
<p>The overall likelihood of the data is the product of likes:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; like = np.prod(likes)
0.18</pre>
</div>
</div>
<div class="paragraph">
<p>For these values of beta, the likelihood of the data is 0.18. The goal
of logistic regression is to find parameters that maximize this
likelihood. To do that, most statistics packages use an iterative solver
like Newton’s method (see
<a href="https://en.wikipedia.org/wiki/Logistic_regression#Model_fitting" class="bare">https://en.wikipedia.org/wiki/Logistic_regression#Model_fitting</a>).</p>
</div>
</div>
<div class="sect2">
<h3 id="implementation"><a class="anchor" href="#implementation"></a><a class="link" href="#implementation">11.8. Implementation</a></h3>
<div class="paragraph">
<p>StatsModels provides an implementation of logistic regression called
logit, named for the function that converts from probability to log
odds. To demonstrate its use, I’ll look for variables that affect the
sex ratio.</p>
</div>
<div class="paragraph">
<p>Again, I load the NSFG data and select pregnancies longer than 30 weeks:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    live, firsts, others = first.MakeFrames()
    df = live[live.prglngth&gt;30]</code></pre>
</div>
</div>
<div class="paragraph">
<p>logit requires the dependent variable to be binary (rather than
boolean), so I create a new column named boy, using astype(int) to
convert to binary integers:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    df['boy'] = (df.babysex==1).astype(int)</code></pre>
</div>
</div>
<div class="paragraph">
<p>Factors that have been found to affect sex ratio include parents’ age,
birth order, race, and social status. We can use logistic regression to
see if these effects appear in the NSFG data. I’ll start with the
mother’s age:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    import statsmodels.formula.api as smf

    model = smf.logit('boy ~ agepreg', data=df)
    results = model.fit()
    SummarizeResults(results)</code></pre>
</div>
</div>
<div class="paragraph">
<p>logit takes the same arguments as ols, a formula in Patsy syntax and a
DataFrame. The result is a Logit object that represents the model. It
contains attributes called endog and exog that contain the <strong>endogenous
variable</strong>, another name for the dependent variable, and the <strong>exogenous
variables</strong>, another name for the explanatory variables. Since they are
NumPy arrays, it is sometimes convenient to convert them to DataFrames:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    endog = pandas.DataFrame(model.endog, columns=[model.endog_names])
    exog = pandas.DataFrame(model.exog, columns=model.exog_names)</code></pre>
</div>
</div>
<div class="paragraph">
<p>The result of model.fit is a BinaryResults object, which is similar to
the RegressionResults object we got from ols. Here is a summary of the
results:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>Intercept   0.00579   (0.953)
agepreg     0.00105   (0.783)
R^2 6.144e-06</pre>
</div>
</div>
<div class="paragraph">
<p>The parameter of agepreg is positive, which suggests that older mothers
are more likely to have boys, but the p-value is 0.783, which means that
the apparent effect could easily be due to chance.</p>
</div>
<div class="paragraph">
<p>The coefficient of determination, \(R^2\), does not apply to
logistic regression, but there are several alternatives that are used as
&#8220;pseudo \(R^2\) values.&#8221; These values can be useful for
comparing models. For example, here’s a model that includes several
factors believed to be associated with sex ratio:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    formula = 'boy ~ agepreg + hpagelb + birthord + C(race)'
    model = smf.logit(formula, data=df)
    results = model.fit()</code></pre>
</div>
</div>
<div class="paragraph">
<p>Along with mother’s age, this model includes father’s age at birth
(hpagelb), birth order (birthord), and race as a categorical variable.
Here are the results:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>Intercept      -0.0301     (0.772)
C(race)[T.2]   -0.0224     (0.66)
C(race)[T.3]   -0.000457   (0.996)
agepreg        -0.00267    (0.629)
hpagelb         0.0047     (0.266)
birthord        0.00501    (0.821)
R^2 0.000144</pre>
</div>
</div>
<div class="paragraph">
<p>None of the estimated parameters are statistically significant. The
pseudo-\(R^2\) value is a little higher, but that could be due
to chance.</p>
</div>
</div>
<div class="sect2">
<h3 id="_accuracy"><a class="anchor" href="#_accuracy"></a><a class="link" href="#_accuracy">11.9. Accuracy</a></h3>
<div class="paragraph">
<p>In the office pool scenario, we are most interested in the accuracy of
the model: the number of successful predictions, compared with what we
would expect by chance.</p>
</div>
<div class="paragraph">
<p>In the NSFG data, there are more boys than girls, so the baseline
strategy is to guess &#8220;boy&#8221; every time. The accuracy of this strategy
is just the fraction of boys:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    actual = endog['boy']
    baseline = actual.mean()</code></pre>
</div>
</div>
<div class="paragraph">
<p>Since actual is encoded in binary integers, the mean is the fraction of
boys, which is 0.507.</p>
</div>
<div class="paragraph">
<p>Here’s how we compute the accuracy of the model:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    predict = (results.predict() &gt;= 0.5)
    true_pos = predict * actual
    true_neg = (1 - predict) * (1 - actual)</code></pre>
</div>
</div>
<div class="paragraph">
<p>results.predict returns a NumPy array of probabilities, which we round
off to 0 or 1. Multiplying by actual yields 1 if we predict a boy and
get it right, 0 otherwise. So, <code>true_pos</code> indicates &#8220;true
positives&#8221;.</p>
</div>
<div class="paragraph">
<p>Similarly, <code>true_neg</code> indicates the cases where we guess &#8220;girl&#8221; and
get it right. Accuracy is the fraction of correct guesses:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    acc = (sum(true_pos) + sum(true_neg)) / len(actual)</code></pre>
</div>
</div>
<div class="paragraph">
<p>The result is 0.512, slightly better than the baseline, 0.507. But, you
should not take this result too seriously. We used the same data to
build and test the model, so the model may not have predictive power on
new data.</p>
</div>
<div class="paragraph">
<p>Nevertheless, let’s use the model to make a prediction for the office
pool. Suppose your friend is 35 years old and white, her husband is 39,
and they are expecting their third child:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    columns = ['agepreg', 'hpagelb', 'birthord', 'race']
    new = pandas.DataFrame([[35, 39, 3, 2]], columns=columns)
    y = results.predict(new)</code></pre>
</div>
</div>
<div class="paragraph">
<p>To invoke results.predict for a new case, you have to construct a
DataFrame with a column for each variable in the model. The result in
this case is 0.52, so you should guess &#8220;boy.&#8221; But if the model
improves your chances of winning, the difference is very small.</p>
</div>
</div>
<div class="sect2">
<h3 id="_exercises_11"><a class="anchor" href="#_exercises_11"></a><a class="link" href="#_exercises_11">11.10. Exercises</a></h3>
<div class="paragraph">
<p>My solution to these exercises is in <code>chap11soln.ipynb</code>.</p>
</div>
<div class="paragraph">
<p>Suppose one of your co-workers is expecting a baby and you are
participating in an office pool to predict the date of birth. Assuming
that bets are placed during the 30th week of pregnancy, what variables
could you use to make the best prediction? You should limit yourself to
variables that are known before the birth, and likely to be available to
the people in the pool.</p>
</div>
<div class="paragraph">
<p>The Trivers-Willard hypothesis suggests that for many mammals the sex
ratio depends on &#8220;maternal condition&#8221;; that is, factors like the
mother’s age, size, health, and social status. See
<a href="https://en.wikipedia.org/wiki/Trivers-Willard_hypothesis" class="bare">https://en.wikipedia.org/wiki/Trivers-Willard_hypothesis</a></p>
</div>
<div class="paragraph">
<p>Some studies have shown this effect among humans, but results are mixed.
In this chapter we tested some variables related to these factors, but
didn’t find any with a statistically significant effect on sex ratio.</p>
</div>
<div class="paragraph">
<p>As an exercise, use a data mining approach to test the other variables
in the pregnancy and respondent files. Can you find any factors with a
substantial effect?</p>
</div>
<div class="paragraph">
<p>If the quantity you want to predict is a count, you can use Poisson
regression, which is implemented in StatsModels with a function called
poisson. It works the same way as ols and logit. As an exercise, let’s
use it to predict how many children a woman has born; in the NSFG
dataset, this variable is called numbabes.</p>
</div>
<div class="paragraph">
<p>Suppose you meet a woman who is 35 years old, black, and a college
graduate whose annual household income exceeds $75,000. How many
children would you predict she has born?</p>
</div>
<div class="paragraph">
<p>If the quantity you want to predict is categorical, you can use
multinomial logistic regression, which is implemented in StatsModels
with a function called mnlogit. As an exercise, let’s use it to guess
whether a woman is married, cohabitating, widowed, divorced, separated,
or never married; in the NSFG dataset, marital status is encoded in a
variable called rmarital.</p>
</div>
<div class="paragraph">
<p>Suppose you meet a woman who is 25 years old, white, and a high school
graduate whose annual household income is about $45,000. What is the
probability that she is married, cohabitating, etc?</p>
</div>
</div>
<div class="sect2">
<h3 id="_glossary_11"><a class="anchor" href="#_glossary_11"></a><a class="link" href="#_glossary_11">11.11. Glossary</a></h3>
<div class="ulist">
<ul>
<li>
<p>regression: One of several related processes for estimating parameters
that fit a model to data.</p>
</li>
<li>
<p>dependent variables: The variables in a regression model we would like
to predict. Also known as endogenous variables.</p>
</li>
<li>
<p>explanatory variables: The variables used to predict or explain the
dependent variables. Also known as independent, or exogenous, variables.</p>
</li>
<li>
<p>simple regression: A regression with only one dependent and one
explanatory variable.</p>
</li>
<li>
<p>multiple regression: A regression with multiple explanatory variables,
but only one dependent variable.</p>
</li>
<li>
<p>linear regression: A regression based on a linear model.</p>
</li>
<li>
<p>ordinary least squares: A linear regression that estimates parameters
by minimizing the squared error of the residuals.</p>
</li>
<li>
<p>spurious relationship: A relationship between two variables that is
caused by a statistical artifact or a factor, not included in the model,
that is related to both variables.</p>
</li>
<li>
<p>control variable: A variable included in a regression to eliminate or
&#8220;control for&#8221; a spurious relationship.</p>
</li>
<li>
<p>proxy variable: A variable that contributes information to a
regression model indirectly because of a relationship with another
factor, so it acts as a proxy for that factor.</p>
</li>
<li>
<p>categorical variable: A variable that can have one of a discrete set
of unordered values.</p>
</li>
<li>
<p>join: An operation that combines data from two DataFrames using a key
to match up rows in the two frames.</p>
</li>
<li>
<p>data mining: An approach to finding relationships between variables by
testing a large number of models.</p>
</li>
<li>
<p>logistic regression: A form of regression used when the dependent
variable is boolean.</p>
</li>
<li>
<p>Poisson regression: A form of regression used when the dependent
variable is a non-negative integer, usually a count.</p>
</li>
<li>
<p>odds: An alternative way of representing a probability, \(p\),
as the ratio of the probability and its complement,
\(p / (1-p)\).</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_time_series_analysis"><a class="anchor" href="#_time_series_analysis"></a><a class="link" href="#_time_series_analysis">12. Time series analysis</a></h2>
<div class="sectionbody">
<div class="paragraph">
<p>A <strong>time series</strong> is a sequence of measurements from a system that varies
in time. One famous example is the &#8220;hockey stick graph&#8221; that shows
global average temperature over time (see
<a href="https://en.wikipedia.org/wiki/Hockey_stick_graph" class="bare">https://en.wikipedia.org/wiki/Hockey_stick_graph</a>).</p>
</div>
<div class="paragraph">
<p>The example I work with in this chapter comes from Zachary M. Jones, a
researcher in political science who studies the black market for
cannabis in the U.S. (<a href="http://zmjones.com/marijuana" class="bare">http://zmjones.com/marijuana</a>). He collected data
from a web site called &#8220;Price of Weed&#8221; that crowdsources market
information by asking participants to report the price, quantity,
quality, and location of cannabis transactions
(<a href="http://www.priceofweed.com/" class="bare">http://www.priceofweed.com/</a>). The goal of his project is to investigate
the effect of policy decisions, like legalization, on markets. I find
this project appealing because it is an example that uses data to
address important political questions, like drug policy.</p>
</div>
<div class="paragraph">
<p>I hope you will find this chapter interesting, but I’ll take this
opportunity to reiterate the importance of maintaining a professional
attitude to data analysis. Whether and which drugs should be illegal are
important and difficult public policy questions; our decisions should be
informed by accurate data reported honestly.</p>
</div>
<div class="paragraph">
<p>The code for this chapter is in timeseries.py. For information about
downloading and working with this code, see Section <a href="#code">Using the code</a>.</p>
</div>
<div class="sect2">
<h3 id="_importing_and_cleaning"><a class="anchor" href="#_importing_and_cleaning"></a><a class="link" href="#_importing_and_cleaning">12.1. Importing and cleaning</a></h3>
<div class="paragraph">
<p>The data I downloaded from Mr. Jones’s site is in the repository for
this book. The following code reads it into a pandas DataFrame:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    transactions = pandas.read_csv('mj-clean.csv', parse_dates=[5])</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>parse_dates</code> tells <code>read_csv</code> to interpret values in column 5 as
dates and convert them to NumPy datetime64 objects.</p>
</div>
<div class="paragraph">
<p>The DataFrame has a row for each reported transaction and the following
columns:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>city: string city name.</p>
</li>
<li>
<p>state: two-letter state abbreviation.</p>
</li>
<li>
<p>price: price paid in dollars.</p>
</li>
<li>
<p>amount: quantity purchased in grams.</p>
</li>
<li>
<p>quality: high, medium, or low quality, as reported by the purchaser.</p>
</li>
<li>
<p>date: date of report, presumed to be shortly after date of purchase.</p>
</li>
<li>
<p>ppg: price per gram, in dollars.</p>
</li>
<li>
<p>state.name: string state name.</p>
</li>
<li>
<p>lat: approximate latitude of the transaction, based on city name.</p>
</li>
<li>
<p>lon: approximate longitude of the transaction.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Each transaction is an event in time, so we could treat this dataset as
a time series. But the events are not equally spaced in time; the number
of transactions reported each day varies from 0 to several hundred. Many
methods used to analyze time series require the measurements to be
equally spaced, or at least things are simpler if they are.</p>
</div>
<div class="paragraph">
<p>In order to demonstrate these methods, I divide the dataset into groups
by reported quality, and then transform each group into an equally
spaced series by computing the mean daily price per gram.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def GroupByQualityAndDay(transactions):
    groups = transactions.groupby('quality')
    dailies = {}
    for name, group in groups:
        dailies[name] = GroupByDay(group)

    return dailies</code></pre>
</div>
</div>
<div class="paragraph">
<p>groupby is a DataFrame method that returns a GroupBy object, groups;
used in a for loop, it iterates the names of the groups and the
DataFrames that represent them. Since the values of quality are low,
medium, and high, we get three groups with those names.</p>
</div>
<div class="paragraph">
<p>The loop iterates through the groups and calls GroupByDay, which
computes the daily average price and returns a new DataFrame:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def GroupByDay(transactions, func=np.mean):
    grouped = transactions[['date', 'ppg']].groupby('date')
    daily = grouped.aggregate(func)

    daily['date'] = daily.index
    start = daily.date[0]
    one_year = np.timedelta64(1, 'Y')
    daily['years'] = (daily.date - start) / one_year

    return daily</code></pre>
</div>
</div>
<div class="paragraph">
<p>The parameter, transactions, is a DataFrame that contains columns date
and ppg. We select these two columns, then group by date.</p>
</div>
<div class="paragraph">
<p>The result, grouped, is a map from each date to a DataFrame that
contains prices reported on that date. aggregate is a GroupBy method
that iterates through the groups and applies a function to each column
of the group; in this case there is only one column, ppg. So the result
of aggregate is a DataFrame with one row for each date and one column,
ppg.</p>
</div>
<div class="paragraph">
<p>Dates in these DataFrames are stored as NumPy datetime64 objects, which
are represented as 64-bit integers in nanoseconds. For some of the
analyses coming up, it will be convenient to work with time in more
human-friendly units, like years. So GroupByDay adds a column named date
by copying the index, then adds years, which contains the number of
years since the first transaction as a floating-point number.</p>
</div>
<div class="paragraph">
<p>The resulting DataFrame has columns ppg, date, and years.</p>
</div>
</div>
<div class="sect2">
<h3 id="_plotting"><a class="anchor" href="#_plotting"></a><a class="link" href="#_plotting">12.2. Plotting</a></h3>
<div class="paragraph">
<p>The result from GroupByQualityAndDay is a map from each quality to a
DataFrame of daily prices. Here’s the code I use to plot the three time
series:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    thinkplot.PrePlot(rows=3)
    for i, (name, daily) in enumerate(dailies.items()):
        thinkplot.SubPlot(i+1)
        title = 'price per gram ($)' if i==0 else ''
        thinkplot.Config(ylim=[0, 20], title=title)
        thinkplot.Scatter(daily.index, daily.ppg, s=10, label=name)
        if i == 2:
            pyplot.xticks(rotation=30)
        else:
            thinkplot.Config(xticks=[])</code></pre>
</div>
</div>
<div class="paragraph">
<p>PrePlot with rows=3 means that we are planning to make three subplots
laid out in three rows. The loop iterates through the DataFrames and
creates a scatter plot for each. It is common to plot time series with
line segments between the points, but in this case there are many data
points and prices are highly variable, so adding lines would not help.</p>
</div>
<div class="paragraph">
<p>Since the labels on the x-axis are dates, I use pyplot.xticks to rotate
the &#8220;ticks&#8221; 30 degrees, making them more readable.</p>
</div>
<div class="paragraph">
<p>image::figs/timeseries1.png[Time series of daily price per gram for high,
medium, and low quality cannabis.,width=336]</p>
</div>
<div class="paragraph">
<p>Figure <a href="#timeseries1">[timeseries1]</a> shows the result. One apparent feature in
these plots is a gap around November 2013. It’s possible that data
collection was not active during this time, or the data might not be
available. We will consider ways to deal with this missing data later.</p>
</div>
<div class="paragraph">
<p>Visually, it looks like the price of high quality cannabis is declining
during this period, and the price of medium quality is increasing. The
price of low quality might also be increasing, but it is harder to tell,
since it seems to be more volatile. Keep in mind that quality data is
reported by volunteers, so trends over time might reflect changes in how
participants apply these labels.</p>
</div>
</div>
<div class="sect2">
<h3 id="timeregress"><a class="anchor" href="#timeregress"></a><a class="link" href="#timeregress">12.3. Linear regression</a></h3>
<div class="paragraph">
<p>Although there are methods specific to time series analysis, for many
problems a simple way to get started is by applying general-purpose
tools like linear regression. The following function takes a DataFrame
of daily prices and computes a least squares fit, returning the model
and results objects from StatsModels:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def RunLinearModel(daily):
    model = smf.ols('ppg ~ years', data=daily)
    results = model.fit()
    return model, results</code></pre>
</div>
</div>
<div class="paragraph">
<p>Then we can iterate through the qualities and fit a model to each:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    for name, daily in dailies.items():
        model, results = RunLinearModel(daily)
        print(name)
        regression.SummarizeResults(results)</code></pre>
</div>
</div>
<div class="paragraph">
<p>Here are the results:</p>
</div>
<table class="tableblock frame-all grid-all spread">
<colgroup>
<col style="width: 25%;">
<col style="width: 25%;">
<col style="width: 25%;">
<col style="width: 25%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">quality</th>
<th class="tableblock halign-left valign-top">intercept</th>
<th class="tableblock halign-left valign-top">slope</th>
<th class="tableblock halign-center valign-top">\(R^2\)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">high</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">13.450</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">-0.708</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">0.444</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">medium</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">8.879</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">0.283</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">0.050</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">low</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">5.362</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">0.568</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">0.030</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>The estimated slopes indicate that the price of high quality cannabis
dropped by about 71 cents per year during the observed interval; for
medium quality it increased by 28 cents per year, and for low quality it
increased by 57 cents per year. These estimates are all statistically
significant with very small p-values.</p>
</div>
<div class="paragraph">
<p>The \(R^2\) value for high quality cannabis is 0.44, which means
that time as an explanatory variable accounts for 44% of the observed
variability in price. For the other qualities, the change in price is
smaller, and variability in prices is higher, so the values of
\(R^2\) are smaller (but still statistically significant).</p>
</div>
<div class="paragraph">
<p>The following code plots the observed prices and the fitted values:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def PlotFittedValues(model, results, label=''):
    years = model.exog[:,1]
    values = model.endog
    thinkplot.Scatter(years, values, s=15, label=label)
    thinkplot.Plot(years, results.fittedvalues, label='model')</code></pre>
</div>
</div>
<div class="paragraph">
<p>As we saw in Section <a href="#implementation">11.8</a>, model contains exog and
endog, NumPy arrays with the exogenous (explanatory) and endogenous
(dependent) variables.</p>
</div>
<div class="paragraph">
<p>image::figs/timeseries2.png[Time series of daily price per gram for high
quality cannabis, and a linear least squares fit.,height=240]</p>
</div>
<div class="paragraph">
<p>PlotFittedValues makes a scatter plot of the data points and a line plot
of the fitted values. Figure <a href="#timeseries2">[timeseries2]</a> shows the results
for high quality cannabis. The model seems like a good linear fit for
the data; nevertheless, linear regression is not the most appropriate
choice for this data:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>First, there is no reason to expect the long-term trend to be a line
or any other simple function. In general, prices are determined by
supply and demand, both of which vary over time in unpredictable ways.</p>
</li>
<li>
<p>Second, the linear regression model gives equal weight to all data,
recent and past. For purposes of prediction, we should probably give
more weight to recent data.</p>
</li>
<li>
<p>Finally, one of the assumptions of linear regression is that the
residuals are uncorrelated noise. With time series data, this assumption
is often false because successive values are correlated.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The next section presents an alternative that is more appropriate for
time series data.</p>
</div>
</div>
<div class="sect2">
<h3 id="_moving_averages"><a class="anchor" href="#_moving_averages"></a><a class="link" href="#_moving_averages">12.4. Moving averages</a></h3>
<div class="paragraph">
<p>Most time series analysis is based on the modeling assumption that the
observed series is the sum of three components:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Trend: A smooth function that captures persistent changes.</p>
</li>
<li>
<p>Seasonality: Periodic variation, possibly including daily, weekly,
monthly, or yearly cycles.</p>
</li>
<li>
<p>Noise: Random variation around the long-term trend.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Regression is one way to extract the trend from a series, as we saw in
the previous section. But if the trend is not a simple function, a good
alternative is a <strong>moving average</strong>. A moving average divides the series
into overlapping regions, called <strong>windows</strong>, and computes the average of
the values in each window.</p>
</div>
<div class="paragraph">
<p>One of the simplest moving averages is the <strong>rolling mean</strong>, which
computes the mean of the values in each window. For example, if the
window size is 3, the rolling mean computes the mean of values 0 through
2, 1 through 3, 2 through 4, etc.</p>
</div>
<div class="paragraph">
<p>pandas provides <code>rolling_mean</code>, which takes a Series and a window size
and returns a new Series.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; series = np.arange(10)
array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

&gt;&gt;&gt; pandas.rolling_mean(series, 3)
array([ nan,  nan,   1,   2,   3,   4,   5,   6,   7,   8])</pre>
</div>
</div>
<div class="paragraph">
<p>The first two values are nan; the next value is the mean of the first
three elements, 0, 1, and 2. The next value is the mean of 1, 2, and 3.
And so on.</p>
</div>
<div class="paragraph">
<p>Before we can apply <code>rolling_mean</code> to the cannabis data, we have to
deal with missing values. There are a few days in the observed interval
with no reported transactions for one or more quality categories, and a
period in 2013 when data collection was not active.</p>
</div>
<div class="paragraph">
<p>In the DataFrames we have used so far, these dates are absent; the index
skips days with no data. For the analysis that follows, we need to
represent this missing data explicitly. We can do that by &#8220;reindexing&#8221;
the DataFrame:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    dates = pandas.date_range(daily.index.min(), daily.index.max())
    reindexed = daily.reindex(dates)</code></pre>
</div>
</div>
<div class="paragraph">
<p>The first line computes a date range that includes every day from the
beginning to the end of the observed interval. The second line creates a
new DataFrame with all of the data from daily, but including rows for
all dates, filled with nan.</p>
</div>
<div class="paragraph">
<p>Now we can plot the rolling mean like this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    roll_mean = pandas.rolling_mean(reindexed.ppg, 30)
    thinkplot.Plot(roll_mean.index, roll_mean)</code></pre>
</div>
</div>
<div class="paragraph">
<p>The window size is 30, so each value in <code>roll_mean</code> is the mean of 30
values from reindexed.ppg.</p>
</div>
<div class="paragraph">
<p>image::figs/timeseries10.png[Daily price and a rolling mean (left) and
exponentially-weighted moving average (right).,height=240]</p>
</div>
<div class="paragraph">
<p>Figure <a href="#timeseries10">[timeseries10]</a> (left) shows the result. The rolling
mean seems to do a good job of smoothing out the noise and extracting
the trend. The first 29 values are nan, and wherever there’s a missing
value, it’s followed by another 29 nans. There are ways to fill in these
gaps, but they are a minor nuisance.</p>
</div>
<div class="paragraph">
<p>An alternative is the <strong>exponentially-weighted moving average</strong> (EWMA),
which has two advantages. First, as the name suggests, it computes a
weighted average where the most recent value has the highest weight and
the weights for previous values drop off exponentially. Second, the
pandas implementation of EWMA handles missing values better.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    ewma = pandas.ewma(reindexed.ppg, span=30)
    thinkplot.Plot(ewma.index, ewma)</code></pre>
</div>
</div>
<div class="paragraph">
<p>The <strong>span</strong> parameter corresponds roughly to the window size of a moving
average; it controls how fast the weights drop off, so it determines the
number of points that make a non-negligible contribution to each
average.</p>
</div>
<div class="paragraph">
<p>Figure <a href="#timeseries10">[timeseries10]</a> (right) shows the EWMA for the same
data. It is similar to the rolling mean, where they are both defined,
but it has no missing values, which makes it easier to work with. The
values are noisy at the beginning of the time series, because they are
based on fewer data points.</p>
</div>
</div>
<div class="sect2">
<h3 id="_missing_values"><a class="anchor" href="#_missing_values"></a><a class="link" href="#_missing_values">12.5. Missing values</a></h3>
<div class="paragraph">
<p>Now that we have characterized the trend of the time series, the next
step is to investigate seasonality, which is periodic behavior. Time
series data based on human behavior often exhibits daily, weekly,
monthly, or yearly cycles. In the next section I present methods to test
for seasonality, but they don’t work well with missing data, so we have
to solve that problem first.</p>
</div>
<div class="paragraph">
<p>A simple and common way to fill missing data is to use a moving average.
The Series method fillna does just what we want:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    reindexed.ppg.fillna(ewma, inplace=True)</code></pre>
</div>
</div>
<div class="paragraph">
<p>Wherever reindexed.ppg is nan, fillna replaces it with the corresponding
value from ewma. The inplace flag tells fillna to modify the existing
Series rather than create a new one.</p>
</div>
<div class="paragraph">
<p>A drawback of this method is that it understates the noise in the
series. We can solve that problem by adding in resampled residuals:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    resid = (reindexed.ppg - ewma).dropna()
    fake_data = ewma + thinkstats2.Resample(resid, len(reindexed))
    reindexed.ppg.fillna(fake_data, inplace=True)</code></pre>
</div>
</div>
<div class="paragraph">
<p>resid contains the residual values, not including days when ppg is nan.
<code>fake_data</code> contains the sum of the moving average and a random sample
of residuals. Finally, fillna replaces nan with values from
<code>fake_data</code>.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="figs/timeseries8.png" alt="Daily price with filled data." height="240">
</div>
</div>
<div class="paragraph">
<p>Figure <a href="#timeseries8">[timeseries8]</a> shows the result. The filled data is
visually similar to the actual values. Since the resampled residuals are
random, the results are different every time; later we’ll see how to
characterize the error created by missing values.</p>
</div>
</div>
<div class="sect2">
<h3 id="_serial_correlation"><a class="anchor" href="#_serial_correlation"></a><a class="link" href="#_serial_correlation">12.6. Serial correlation</a></h3>
<div class="paragraph">
<p>As prices vary from day to day, you might expect to see patterns. If the
price is high on Monday, you might expect it to be high for a few more
days; and if it’s low, you might expect it to stay low. A pattern like
this is called <strong>serial correlation</strong>, because each value is correlated
with the next one in the series.</p>
</div>
<div class="paragraph">
<p>To compute serial correlation, we can shift the time series by an
interval called a <strong>lag</strong>, and then compute the correlation of the shifted
series with the original:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def SerialCorr(series, lag=1):
    xs = series[lag:]
    ys = series.shift(lag)[lag:]
    corr = thinkstats2.Corr(xs, ys)
    return corr</code></pre>
</div>
</div>
<div class="paragraph">
<p>After the shift, the first lag values are nan, so I use a slice to
remove them before computing Corr.</p>
</div>
<div class="paragraph">
<p>If we apply SerialCorr to the raw price data with lag 1, we find serial
correlation 0.48 for the high quality category, 0.16 for medium and 0.10
for low. In any time series with a long-term trend, we expect to see
strong serial correlations; for example, if prices are falling, we
expect to see values above the mean in the first half of the series and
values below the mean in the second half.</p>
</div>
<div class="paragraph">
<p>It is more interesting to see if the correlation persists if you
subtract away the trend. For example, we can compute the residual of the
EWMA and then compute its serial correlation:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    ewma = pandas.ewma(reindexed.ppg, span=30)
    resid = reindexed.ppg - ewma
    corr = SerialCorr(resid, 1)</code></pre>
</div>
</div>
<div class="paragraph">
<p>With lag=1, the serial correlations for the de-trended data are -0.022
for high quality, -0.015 for medium, and 0.036 for low. These values are
small, indicating that there is little or no one-day serial correlation
in this series.</p>
</div>
<div class="paragraph">
<p>To check for weekly, monthly, and yearly seasonality, I ran the analysis
again with different lags. Here are the results:</p>
</div>
<table class="tableblock frame-all grid-all spread">
<colgroup>
<col style="width: 25%;">
<col style="width: 25%;">
<col style="width: 25%;">
<col style="width: 25%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-center valign-top">lag</th>
<th class="tableblock halign-center valign-top">high</th>
<th class="tableblock halign-center valign-top">medium</th>
<th class="tableblock halign-center valign-top">low</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-center valign-top"><p class="tableblock">1</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">-0.029</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">-0.014</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">0.034</p></td>
</tr>
<tr>
<td class="tableblock halign-center valign-top"><p class="tableblock">7</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">0.02</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">-0.042</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">-0.0097</p></td>
</tr>
<tr>
<td class="tableblock halign-center valign-top"><p class="tableblock">30</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">0.014</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">-0.0064</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">-0.013</p></td>
</tr>
<tr>
<td class="tableblock halign-center valign-top"><p class="tableblock">365</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">0.045</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">0.015</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">0.033</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>In the next section we’ll test whether these correlations are
statistically significant (they are not), but at this point we can
tentatively conclude that there are no substantial seasonal patterns in
these series, at least not with these lags.</p>
</div>
</div>
<div class="sect2">
<h3 id="_autocorrelation"><a class="anchor" href="#_autocorrelation"></a><a class="link" href="#_autocorrelation">12.7. Autocorrelation</a></h3>
<div class="paragraph">
<p>If you think a series might have some serial correlation, but you don’t
know which lags to test, you can test them all! The <strong>autocorrelation
function</strong> is a function that maps from lag to the serial correlation
with the given lag. &#8220;Autocorrelation&#8221; is another name for serial
correlation, used more often when the lag is not 1.</p>
</div>
<div class="paragraph">
<p>StatsModels, which we used for linear regression in
Section <a href="#statsmodels">[statsmodels]</a>, also provides functions for time series
analysis, including acf, which computes the autocorrelation function:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    import statsmodels.tsa.stattools as smtsa
    acf = smtsa.acf(filled.resid, nlags=365, unbiased=True)</code></pre>
</div>
</div>
<div class="paragraph">
<p>acf computes serial correlations with lags from 0 through nlags. The
unbiased flag tells acf to correct the estimates for the sample size.
The result is an array of correlations. If we select daily prices for
high quality, and extract correlations for lags 1, 7, 30, and 365, we
can confirm that acf and SerialCorr yield approximately the same
results:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; acf[0], acf[1], acf[7], acf[30], acf[365]
1.000, -0.029, 0.020, 0.014, 0.044</pre>
</div>
</div>
<div class="paragraph">
<p>With lag=0, acf computes the correlation of the series with itself,
which is always 1.</p>
</div>
<div class="paragraph">
<p>image::figs/timeseries9.png[Autocorrelation function for daily prices
(left), and daily prices with a simulated weekly seasonality
(right).,height=240]</p>
</div>
<div class="paragraph">
<p>Figure <a href="#timeseries9">[timeseries9]</a> (left) shows autocorrelation functions
for the three quality categories, with nlags=40. The gray region shows
the normal variability we would expect if there is no actual
autocorrelation; anything that falls outside this range is statistically
significant, with a p-value less than 5%. Since the false positive rate
is 5%, and we are computing 120 correlations (40 lags for each of 3
times series), we expect to see about 6 points outside this region. In
fact, there are 7. We conclude that there are no autocorrelations in
these series that could not be explained by chance.</p>
</div>
<div class="paragraph">
<p>I computed the gray regions by resampling the residuals. You can see my
code in timeseries.py; the function is called SimulateAutocorrelation.</p>
</div>
<div class="paragraph">
<p>To see what the autocorrelation function looks like when there is a
seasonal component, I generated simulated data by adding a weekly cycle.
Assuming that demand for cannabis is higher on weekends, we might expect
the price to be higher. To simulate this effect, I select dates that
fall on Friday or Saturday and add a random amount to the price, chosen
from a uniform distribution from $0 to $2.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def AddWeeklySeasonality(daily):
    frisat = (daily.index.dayofweek==4) | (daily.index.dayofweek==5)
    fake = daily.copy()
    fake.ppg[frisat] += np.random.uniform(0, 2, frisat.sum())
    return fake</code></pre>
</div>
</div>
<div class="paragraph">
<p>frisat is a boolean Series, True if the day of the week is Friday or
Saturday. fake is a new DataFrame, initially a copy of daily, which we
modify by adding random values to ppg. frisat.sum() is the total number
of Fridays and Saturdays, which is the number of random values we have
to generate.</p>
</div>
<div class="paragraph">
<p>Figure <a href="#timeseries9">[timeseries9]</a> (right) shows autocorrelation functions
for prices with this simulated seasonality. As expected, the
correlations are highest when the lag is a multiple of 7. For high and
medium quality, the new correlations are statistically significant. For
low quality they are not, because residuals in this category are large;
the effect would have to be bigger to be visible through the noise.</p>
</div>
</div>
<div class="sect2">
<h3 id="_prediction_2"><a class="anchor" href="#_prediction_2"></a><a class="link" href="#_prediction_2">12.8. Prediction</a></h3>
<div class="paragraph">
<p>Time series analysis can be used to investigate, and sometimes explain,
the behavior of systems that vary in time. It can also make predictions.</p>
</div>
<div class="paragraph">
<p>The linear regressions we used in Section <a href="#timeregress">12.3</a> can be
used for prediction. The RegressionResults class provides predict, which
takes a DataFrame containing the explanatory variables and returns a
sequence of predictions. Here’s the code:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def GenerateSimplePrediction(results, years):
    n = len(years)
    inter = np.ones(n)
    d = dict(Intercept=inter, years=years)
    predict_df = pandas.DataFrame(d)
    predict = results.predict(predict_df)
    return predict</code></pre>
</div>
</div>
<div class="paragraph">
<p>results is a RegressionResults object; years is the sequence of time
values we want predictions for. The function constructs a DataFrame,
passes it to predict, and returns the result.</p>
</div>
<div class="paragraph">
<p>If all we want is a single, best-guess prediction, we’re done. But for
most purposes it is important to quantify error. In other words, we want
to know how accurate the prediction is likely to be.</p>
</div>
<div class="paragraph">
<p>There are three sources of error we should take into account:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Sampling error: The prediction is based on estimated parameters, which
depend on random variation in the sample. If we run the experiment
again, we expect the estimates to vary.</p>
</li>
<li>
<p>Random variation: Even if the estimated parameters are perfect, the
observed data varies randomly around the long-term trend, and we expect
this variation to continue in the future.</p>
</li>
<li>
<p>Modeling error: We have already seen evidence that the long-term trend
is not linear, so predictions based on a linear model will eventually
fail.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Another source of error to consider is unexpected future events.
Agricultural prices are affected by weather, and all prices are affected
by politics and law. As I write this, cannabis is legal in two states
and legal for medical purposes in 20 more. If more states legalize it,
the price is likely to go down. But if the federal government cracks
down, the price might go up.</p>
</div>
<div class="paragraph">
<p>Modeling errors and unexpected future events are hard to quantify.
Sampling error and random variation are easier to deal with, so we’ll do
that first.</p>
</div>
<div class="paragraph">
<p>To quantify sampling error, I use resampling, as we did in
Section <a href="#regest">10.4</a>. As always, the goal is to use the actual
observations to simulate what would happen if we ran the experiment
again. The simulations are based on the assumption that the estimated
parameters are correct, but the random residuals could have been
different. Here is a function that runs the simulations:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def SimulateResults(daily, iters=101):
    model, results = RunLinearModel(daily)
    fake = daily.copy()

    result_seq = []
    for i in range(iters):
        fake.ppg = results.fittedvalues + Resample(results.resid)
        _, fake_results = RunLinearModel(fake)
        result_seq.append(fake_results)

    return result_seq</code></pre>
</div>
</div>
<div class="paragraph">
<p>daily is a DataFrame containing the observed prices; iters is the number
of simulations to run.</p>
</div>
<div class="paragraph">
<p>SimulateResults uses RunLinearModel, from
Section <a href="#timeregress">12.3</a>, to estimate the slope and intercept of
the observed values.</p>
</div>
<div class="paragraph">
<p>Each time through the loop, it generates a &#8220;fake&#8221; dataset by
resampling the residuals and adding them to the fitted values. Then it
runs a linear model on the fake data and stores the RegressionResults
object.</p>
</div>
<div class="paragraph">
<p>The next step is to use the simulated results to generate predictions:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def GeneratePredictions(result_seq, years, add_resid=False):
    n = len(years)
    d = dict(Intercept=np.ones(n), years=years, years2=years**2)
    predict_df = pandas.DataFrame(d)

    predict_seq = []
    for fake_results in result_seq:
        predict = fake_results.predict(predict_df)
        if add_resid:
            predict += thinkstats2.Resample(fake_results.resid, n)
        predict_seq.append(predict)

    return predict_seq</code></pre>
</div>
</div>
<div class="paragraph">
<p>GeneratePredictions takes the sequence of results from the previous
step, as well as years, which is a sequence of floats that specifies the
interval to generate predictions for, and <code>add_resid</code>, which indicates
whether it should add resampled residuals to the straight-line
prediction. GeneratePredictions iterates through the sequence of
RegressionResults and generates a sequence of predictions.</p>
</div>
<div class="paragraph">
<p>image::figs/timeseries4.png[Predictions based on linear fits, showing
variation due to sampling error and prediction error.,height=240]</p>
</div>
<div class="paragraph">
<p>Finally, here’s the code that plots a 90% confidence interval for the
predictions:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def PlotPredictions(daily, years, iters=101, percent=90):
    result_seq = SimulateResults(daily, iters=iters)
    p = (100 - percent) / 2
    percents = p, 100-p

    predict_seq = GeneratePredictions(result_seq, years, True)
    low, high = thinkstats2.PercentileRows(predict_seq, percents)
    thinkplot.FillBetween(years, low, high, alpha=0.3, color='gray')

    predict_seq = GeneratePredictions(result_seq, years, False)
    low, high = thinkstats2.PercentileRows(predict_seq, percents)
    thinkplot.FillBetween(years, low, high, alpha=0.5, color='gray')</code></pre>
</div>
</div>
<div class="paragraph">
<p>PlotPredictions calls GeneratePredictions twice: once with
<code>add_resid=True</code> and again with <code>add_resid=False</code>. It uses
PercentileRows to select the 5th and 95th percentiles for each year,
then plots a gray region between these bounds.</p>
</div>
<div class="paragraph">
<p>Figure <a href="#timeseries4">[timeseries4]</a> shows the result. The dark gray region
represents a 90% confidence interval for the sampling error; that is,
uncertainty about the estimated slope and intercept due to sampling.</p>
</div>
<div class="paragraph">
<p>The lighter region shows a 90% confidence interval for prediction error,
which is the sum of sampling error and random variation.</p>
</div>
<div class="paragraph">
<p>These regions quantify sampling error and random variation, but not
modeling error. In general modeling error is hard to quantify, but in
this case we can address at least one source of error, unpredictable
external events.</p>
</div>
<div class="paragraph">
<p>The regression model is based on the assumption that the system is
<strong>stationary</strong>; that is, that the parameters of the model don’t change
over time. Specifically, it assumes that the slope and intercept are
constant, as well as the distribution of residuals.</p>
</div>
<div class="paragraph">
<p>But looking at the moving averages in Figure <a href="#timeseries10">[timeseries10]</a>,
it seems like the slope changes at least once during the observed
interval, and the variance of the residuals seems bigger in the first
half than the second.</p>
</div>
<div class="paragraph">
<p>As a result, the parameters we get depend on the interval we observe. To
see how much effect this has on the predictions, we can extend
SimulateResults to use intervals of observation with different start and
end dates. My implementation is in timeseries.py.</p>
</div>
<div class="paragraph">
<p>image::figs/timeseries5.png[Predictions based on linear fits, showing
variation due to the interval of observation.,height=240]</p>
</div>
<div class="paragraph">
<p>Figure <a href="#timeseries5">[timeseries5]</a> shows the result for the medium quality
category. The lightest gray area shows a confidence interval that
includes uncertainty due to sampling error, random variation, and
variation in the interval of observation.</p>
</div>
<div class="paragraph">
<p>The model based on the entire interval has positive slope, indicating
that prices were increasing. But the most recent interval shows signs of
decreasing prices, so models based on the most recent data have negative
slope. As a result, the widest predictive interval includes the
possibility of decreasing prices over the next year.</p>
</div>
</div>
<div class="sect2">
<h3 id="_further_reading"><a class="anchor" href="#_further_reading"></a><a class="link" href="#_further_reading">12.9. Further reading</a></h3>
<div class="paragraph">
<p>Time series analysis is a big topic; this chapter has only scratched the
surface. An important tool for working with time series data is
autoregression, which I did not cover here, mostly because it turns out
not to be useful for the example data I worked with.</p>
</div>
<div class="paragraph">
<p>But once you have learned the material in this chapter, you are well
prepared to learn about autoregression. One resource I recommend is
Philipp Janert’s book, <em>Data Analysis with Open Source Tools</em>, O’Reilly
Media, 2011. His chapter on time series analysis picks up where this one
leaves off.</p>
</div>
</div>
<div class="sect2">
<h3 id="_exercises_12"><a class="anchor" href="#_exercises_12"></a><a class="link" href="#_exercises_12">12.10. Exercises</a></h3>
<div class="paragraph">
<p>My solution to these exercises is in <code>chap12soln.py</code>.</p>
</div>
<div class="paragraph">
<p>The linear model I used in this chapter has the obvious drawback that it
is linear, and there is no reason to expect prices to change linearly
over time. We can add flexibility to the model by adding a quadratic
term, as we did in Section <a href="#nonlinear">11.3</a>.</p>
</div>
<div class="paragraph">
<p>Use a quadratic model to fit the time series of daily prices, and use
the model to generate predictions. You will have to write a version of
RunLinearModel that runs that quadratic model, but after that you should
be able to reuse code in timeseries.py to generate predictions.</p>
</div>
<div class="paragraph">
<p>Write a definition for a class named SerialCorrelationTest that extends
HypothesisTest from Section <a href="#hypotest">9.2</a>. It should take a
series and a lag as data, compute the serial correlation of the series
with the given lag, and then compute the p-value of the observed
correlation.</p>
</div>
<div class="paragraph">
<p>Use this class to test whether the serial correlation in raw price data
is statistically significant. Also test the residuals of the linear
model and (if you did the previous exercise), the quadratic model.</p>
</div>
<div class="paragraph">
<p>There are several ways to extend the EWMA model to generate predictions.
One of the simplest is something like this:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Compute the EWMA of the time series and use the last point as an
intercept, inter.</p>
</li>
<li>
<p>Compute the EWMA of differences between successive elements in the
time series and use the last point as a slope, slope.</p>
</li>
<li>
<p>To predict values at future times, compute inter + slope * dt, where
dt is the difference between the time of the prediction and the time of
the last observation.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>Use this method to generate predictions for a year after the last
observation. A few hints:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Use timeseries.FillMissing to fill in missing values before running
this analysis. That way the time between consecutive elements is
consistent.</p>
</li>
<li>
<p>Use Series.diff to compute differences between successive elements.</p>
</li>
<li>
<p>Use reindex to extend the DataFrame index into the future.</p>
</li>
<li>
<p>Use fillna to put your predicted values into the DataFrame.</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_glossary_12"><a class="anchor" href="#_glossary_12"></a><a class="link" href="#_glossary_12">12.11. Glossary</a></h3>
<div class="ulist">
<ul>
<li>
<p>time series: A dataset where each value is associated with a
timestamp, often a series of measurements and the times they were
collected.</p>
</li>
<li>
<p>window: A sequence of consecutive values in a time series, often used
to compute a moving average.</p>
</li>
<li>
<p>moving average: One of several statistics intended to estimate the
underlying trend in a time series by computing averages (of some kind)
for a series of overlapping windows.</p>
</li>
<li>
<p>rolling mean: A moving average based on the mean value in each window.</p>
</li>
<li>
<p>exponentially-weighted moving average (EWMA): A moving average based
on a weighted mean that gives the highest weight to the most recent
values, and exponentially decreasing weights to earlier values.</p>
</li>
<li>
<p>span: A parameter of EWMA that determines how quickly the weights
decrease.</p>
</li>
<li>
<p>serial correlation: Correlation between a time series and a shifted or
lagged version of itself.</p>
</li>
<li>
<p>lag: The size of the shift in a serial correlation or autocorrelation.</p>
</li>
<li>
<p>autocorrelation: A more general term for a serial correlation with any
amount of lag.</p>
</li>
<li>
<p>autocorrelation function: A function that maps from lag to serial
correlation.</p>
</li>
<li>
<p>stationary: A model is stationary if the parameters and the
distribution of residuals does not change over time.</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_survival_analysis"><a class="anchor" href="#_survival_analysis"></a><a class="link" href="#_survival_analysis">13. Survival analysis</a></h2>
<div class="sectionbody">
<div class="paragraph">
<p><strong>Survival analysis</strong> is a way to describe how long things last. It is
often used to study human lifetimes, but it also applies to &#8220;survival&#8221;
of mechanical and electronic components, or more generally to intervals
in time before an event.</p>
</div>
<div class="paragraph">
<p>If someone you know has been diagnosed with a life-threatening disease,
you might have seen a &#8220;5-year survival rate,&#8221; which is the probability
of surviving five years after diagnosis. That estimate and related
statistics are the result of survival analysis.</p>
</div>
<div class="paragraph">
<p>The code in this chapter is in survival.py. For information about
downloading and working with this code, see Section <a href="#code">Using the code</a>.</p>
</div>
<div class="sect2">
<h3 id="survival"><a class="anchor" href="#survival"></a><a class="link" href="#survival">13.1. Survival curves</a></h3>
<div class="paragraph">
<p>The fundamental concept in survival analysis is the <strong>survival curve</strong>,
\(S(t)\), which is a function that maps from a duration,
\(t\), to the probability of surviving longer than
\(t\). If you know the distribution of durations, or
&#8220;lifetimes&#8221;, finding the survival curve is easy; it’s just the
complement of the CDF:</p>
</div>
<div class="stemblock">
<div class="content">
\[S(t) = 1 - \mathrm{CDF}(t)\]
</div>
</div>
<div class="paragraph">
<p>where \(CDF(t)\) is the probability of a lifetime less than or
equal to \(t\).</p>
</div>
<div class="paragraph">
<p>For example, in the NSFG dataset, we know the duration of 11189 complete
pregnancies. We can read this data and compute the CDF:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    preg = nsfg.ReadFemPreg()
    complete = preg.query('outcome in [1, 3, 4]').prglngth
    cdf = thinkstats2.Cdf(complete, label='cdf')</code></pre>
</div>
</div>
<div class="paragraph">
<p>The outcome codes 1, 3, 4 indicate live birth, stillbirth, and
miscarriage. For this analysis I am excluding induced abortions, ectopic
pregnancies, and pregnancies that were in progress when the respondent
was interviewed.</p>
</div>
<div class="paragraph">
<p>The DataFrame method query takes a boolean expression and evaluates it
for each row, selecting the rows that yield True.</p>
</div>
<div class="paragraph">
<p>image::figs/survival1.png[Cdf and survival curve for pregnancy length
(top), hazard curve (bottom).,height=288]</p>
</div>
<div class="paragraph">
<p>Figure <a href="#survival1">[survival1]</a> (top) shows the CDF of pregnancy length and
its complement, the survival curve. To represent the survival curve, I
define an object that wraps a Cdf and adapts the interface:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">class SurvivalFunction(object):
    def __init__(self, cdf, label=''):
        self.cdf = cdf
        self.label = label or cdf.label

    @property
    def ts(self):
        return self.cdf.xs

    @property
    def ss(self):
        return 1 - self.cdf.ps</code></pre>
</div>
</div>
<div class="paragraph">
<p>SurvivalFunction provides two properties: ts, which is the sequence of
lifetimes, and ss, which is the survival curve. In Python, a
&#8220;property&#8221; is a method that can be invoked as if it were a variable.</p>
</div>
<div class="paragraph">
<p>We can instantiate a SurvivalFunction by passing the CDF of lifetimes:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    sf = SurvivalFunction(cdf)</code></pre>
</div>
</div>
<div class="paragraph">
<p>SurvivalFunction also provides <code>__getitem__</code> and Prob, which evaluates
the survival curve:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python"># class SurvivalFunction

    def __getitem__(self, t):
        return self.Prob(t)

    def Prob(self, t):
        return 1 - self.cdf.Prob(t)</code></pre>
</div>
</div>
<div class="paragraph">
<p>For example, sf[13] is the fraction of pregnancies that proceed past the
first trimester:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; sf[13]
0.86022
&gt;&gt;&gt; cdf[13]
0.13978</pre>
</div>
</div>
<div class="paragraph">
<p>About 86% of pregnancies proceed past the first trimester; about 14% do
not.</p>
</div>
<div class="paragraph">
<p>SurvivalFunction provides Render, so we can plot sf using the functions
in thinkplot:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    thinkplot.Plot(sf)</code></pre>
</div>
</div>
<div class="paragraph">
<p>Figure <a href="#survival1">[survival1]</a> (top) shows the result. The curve is nearly
flat between 13 and 26 weeks, which shows that few pregnancies end in
the second trimester. And the curve is steepest around 39 weeks, which
is the most common pregnancy length.</p>
</div>
</div>
<div class="sect2">
<h3 id="hazard"><a class="anchor" href="#hazard"></a><a class="link" href="#hazard">13.2. Hazard function</a></h3>
<div class="paragraph">
<p>From the survival curve we can derive the <strong>hazard function</strong>; for
pregnancy lengths, the hazard function maps from a time, \(t\),
to the fraction of pregnancies that continue until \(t\) and
then end at \(t\). To be more precise:</p>
</div>
<div class="stemblock">
<div class="content">
\[\lambda(t) = \frac{S(t) - S(t+1)}{S(t)}\]
</div>
</div>
<div class="paragraph">
<p>The numerator is the fraction of lifetimes that end at \(t\),
which is also \(\mathrm{PMF}(t)\).</p>
</div>
<div class="paragraph">
<p>SurvivalFunction provides MakeHazard, which calculates the hazard
function:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python"># class SurvivalFunction

    def MakeHazard(self, label=''):
        ss = self.ss
        lams = {}
        for i, t in enumerate(self.ts[:-1]):
            hazard = (ss[i] - ss[i+1]) / ss[i]
            lams[t] = hazard

        return HazardFunction(lams, label=label)</code></pre>
</div>
</div>
<div class="paragraph">
<p>The HazardFunction object is a wrapper for a pandas Series:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">class HazardFunction(object):

    def __init__(self, d, label=''):
        self.series = pandas.Series(d)
        self.label = label</code></pre>
</div>
</div>
<div class="paragraph">
<p>d can be a dictionary or any other type that can initialize a Series,
including another Series. label is a string used to identify the
HazardFunction when plotted.</p>
</div>
<div class="paragraph">
<p>HazardFunction provides <code>__getitem__</code>, so we can evaluate it like
this:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; hf = sf.MakeHazard()
&gt;&gt;&gt; hf[39]
0.49689</pre>
</div>
</div>
<div class="paragraph">
<p>So of all pregnancies that proceed until week 39, about 50% end in week
39.</p>
</div>
<div class="paragraph">
<p>Figure <a href="#survival1">[survival1]</a> (bottom) shows the hazard function for
pregnancy lengths. For times after week 42, the hazard function is
erratic because it is based on a small number of cases. Other than that
the shape of the curve is as expected: it is highest around 39 weeks,
and a little higher in the first trimester than in the second.</p>
</div>
<div class="paragraph">
<p>The hazard function is useful in its own right, but it is also an
important tool for estimating survival curves, as we’ll see in the next
section.</p>
</div>
</div>
<div class="sect2">
<h3 id="_inferring_survival_curves"><a class="anchor" href="#_inferring_survival_curves"></a><a class="link" href="#_inferring_survival_curves">13.3. Inferring survival curves</a></h3>
<div class="paragraph">
<p>If someone gives you the CDF of lifetimes, it is easy to compute the
survival and hazard functions. But in many real-world scenarios, we
can’t measure the distribution of lifetimes directly. We have to infer
it.</p>
</div>
<div class="paragraph">
<p>For example, suppose you are following a group of patients to see how
long they survive after diagnosis. Not all patients are diagnosed on the
same day, so at any point in time, some patients have survived longer
than others. If some patients have died, we know their survival times.
For patients who are still alive, we don’t know survival times, but we
have a lower bound.</p>
</div>
<div class="paragraph">
<p>If we wait until all patients are dead, we can compute the survival
curve, but if we are evaluating the effectiveness of a new treatment, we
can’t wait that long! We need a way to estimate survival curves using
incomplete information.</p>
</div>
<div class="paragraph">
<p>As a more cheerful example, I will use NSFG data to quantify how long
respondents &#8220;survive&#8221; until they get married for the first time. The
range of respondents’ ages is 14 to 44 years, so the dataset provides a
snapshot of women at different stages in their lives.</p>
</div>
<div class="paragraph">
<p>For women who have been married, the dataset includes the date of their
first marriage and their age at the time. For women who have not been
married, we know their age when interviewed, but have no way of knowing
when or if they will get married.</p>
</div>
<div class="paragraph">
<p>Since we know the age at first marriage for <em>some</em> women, it might be
tempting to exclude the rest and compute the CDF of the known data. That
is a bad idea. The result would be doubly misleading: (1) older women
would be overrepresented, because they are more likely to be married
when interviewed, and (2) married women would be overrepresented! In
fact, this analysis would lead to the conclusion that all women get
married, which is obviously incorrect.</p>
</div>
</div>
<div class="sect2">
<h3 id="_kaplan_meier_estimation"><a class="anchor" href="#_kaplan_meier_estimation"></a><a class="link" href="#_kaplan_meier_estimation">13.4. Kaplan-Meier estimation</a></h3>
<div class="paragraph">
<p>In this example it is not only desirable but necessary to include
observations of unmarried women, which brings us to one of the central
algorithms in survival analysis, <strong>Kaplan-Meier estimation</strong>.</p>
</div>
<div class="paragraph">
<p>The general idea is that we can use the data to estimate the hazard
function, then convert the hazard function to a survival curve. To
estimate the hazard function, we consider, for each age, (1) the number
of women who got married at that age and (2) the number of women &#8220;at
risk&#8221; of getting married, which includes all women who were not married
at an earlier age.</p>
</div>
<div class="paragraph">
<p>Here’s the code:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def EstimateHazardFunction(complete, ongoing, label=''):

    hist_complete = Counter(complete)
    hist_ongoing = Counter(ongoing)

    ts = list(hist_complete | hist_ongoing)
    ts.sort()

    at_risk = len(complete) + len(ongoing)

    lams = pandas.Series(index=ts)
    for t in ts:
        ended = hist_complete[t]
        censored = hist_ongoing[t]

        lams[t] = ended / at_risk
        at_risk -= ended + censored

    return HazardFunction(lams, label=label)</code></pre>
</div>
</div>
<div class="paragraph">
<p>complete is the set of complete observations; in this case, the ages
when respondents got married. ongoing is the set of incomplete
observations; that is, the ages of unmarried women when they were
interviewed.</p>
</div>
<div class="paragraph">
<p>First, we precompute <code>hist_complete</code>, which is a Counter that maps
from each age to the number of women married at that age, and
<code>hist_ongoing</code> which maps from each age to the number of unmarried
women interviewed at that age.</p>
</div>
<div class="paragraph">
<p>ts is the union of ages when respondents got married and ages when
unmarried women were interviewed, sorted in increasing order.</p>
</div>
<div class="paragraph">
<p><code>at_risk</code> keeps track of the number of respondents considered &#8220;at
risk&#8221; at each age; initially, it is the total number of respondents.</p>
</div>
<div class="paragraph">
<p>The result is stored in a Pandas Series that maps from each age to the
estimated hazard function at that age.</p>
</div>
<div class="paragraph">
<p>Each time through the loop, we consider one age, t, and compute the
number of events that end at t (that is, the number of respondents
married at that age) and the number of events censored at t (that is,
the number of women interviewed at t whose future marriage dates are
censored). In this context, &#8220;censored&#8221; means that the data are
unavailable because of the data collection process.</p>
</div>
<div class="paragraph">
<p>The estimated hazard function is the fraction of the cases at risk that
end at t.</p>
</div>
<div class="paragraph">
<p>At the end of the loop, we subtract from <code>at_risk</code> the number of cases
that ended or were censored at t.</p>
</div>
<div class="paragraph">
<p>Finally, we pass lams to the HazardFunction constructor and return the
result.</p>
</div>
</div>
<div class="sect2">
<h3 id="_the_marriage_curve"><a class="anchor" href="#_the_marriage_curve"></a><a class="link" href="#_the_marriage_curve">13.5. The marriage curve</a></h3>
<div class="paragraph">
<p>To test this function, we have to do some data cleaning and
transformation. The NSFG variables we need are:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>cmbirth: The respondent’s date of birth, known for all respondents.</p>
</li>
<li>
<p>cmintvw: The date the respondent was interviewed, known for all
respondents.</p>
</li>
<li>
<p>cmmarrhx: The date the respondent was first married, if applicable and
known.</p>
</li>
<li>
<p>evrmarry: 1 if the respondent had been married prior to the date of
interview, 0 otherwise.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The first three variables are encoded in &#8220;century-months&#8221;; that is,
the integer number of months since December 1899. So century-month 1 is
January 1900.</p>
</div>
<div class="paragraph">
<p>First, we read the respondent file and replace invalid values of
cmmarrhx:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    resp = chap01soln.ReadFemResp()
    resp.cmmarrhx.replace([9997, 9998, 9999], np.nan, inplace=True)</code></pre>
</div>
</div>
<div class="paragraph">
<p>Then we compute each respondent’s age when married and age when
interviewed:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    resp['agemarry'] = (resp.cmmarrhx - resp.cmbirth) / 12.0
    resp['age'] = (resp.cmintvw - resp.cmbirth) / 12.0</code></pre>
</div>
</div>
<div class="paragraph">
<p>Next we extract complete, which is the age at marriage for women who
have been married, and ongoing, which is the age at interview for women
who have not:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    complete = resp[resp.evrmarry==1].agemarry
    ongoing = resp[resp.evrmarry==0].age</code></pre>
</div>
</div>
<div class="paragraph">
<p>Finally we compute the hazard function.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    hf = EstimateHazardFunction(complete, ongoing)</code></pre>
</div>
</div>
<div class="paragraph">
<p>Figure <a href="#survival2">[survival2]</a> (top) shows the estimated hazard function;
it is low in the teens, higher in the 20s, and declining in the 30s. It
increases again in the 40s, but that is an artifact of the estimation
process; as the number of respondents &#8220;at risk&#8221; decreases, a small
number of women getting married yields a large estimated hazard. The
survival curve will smooth out this noise.</p>
</div>
</div>
<div class="sect2">
<h3 id="_estimating_the_survival_curve"><a class="anchor" href="#_estimating_the_survival_curve"></a><a class="link" href="#_estimating_the_survival_curve">13.6. Estimating the survival curve</a></h3>
<div class="paragraph">
<p>Once we have the hazard function, we can estimate the survival curve.
The chance of surviving past time t is the chance of surviving all times
up through t, which is the cumulative product of the complementary
hazard function:</p>
</div>
<div class="stemblock">
<div class="content">
\[[1-\lambda(0)] [1-\lambda(1)] ... [1-\lambda(t)]\]
</div>
</div>
<div class="paragraph">
<p>The HazardFunction class provides MakeSurvival, which computes this
product:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python"># class HazardFunction:

    def MakeSurvival(self):
        ts = self.series.index
        ss = (1 - self.series).cumprod()
        cdf = thinkstats2.Cdf(ts, 1-ss)
        sf = SurvivalFunction(cdf)
        return sf</code></pre>
</div>
</div>
<div class="paragraph">
<p>ts is the sequence of times where the hazard function is estimated. ss
is the cumulative product of the complementary hazard function, so it is
the survival curve.</p>
</div>
<div class="paragraph">
<p>Because of the way SurvivalFunction is implemented, we have to compute
the complement of ss, make a Cdf, and then instantiate a
SurvivalFunction object.</p>
</div>
<div class="paragraph">
<p>image::figs/survival2.png[Hazard function for age at first marriage (top)
and survival curve (bottom).,height=240]</p>
</div>
<div class="paragraph">
<p>Figure <a href="#survival2">[survival2]</a> (bottom) shows the result. The survival
curve is steepest between 25 and 35, when most women get married.
Between 35 and 45, the curve is nearly flat, indicating that women who
do not marry before age 35 are unlikely to get married.</p>
</div>
<div class="paragraph">
<p>A curve like this was the basis of a famous magazine article in 1986;
<em>Newsweek</em> reported that a 40-year old unmarried woman was &#8220;more likely
to be killed by a terrorist&#8221; than get married. These statistics were
widely reported and became part of popular culture, but they were wrong
then (because they were based on faulty analysis) and turned out to be
even more wrong (because of cultural changes that were already in
progress and continued). In 2006, <em>Newsweek</em> ran an another article
admitting that they were wrong.</p>
</div>
<div class="paragraph">
<p>I encourage you to read more about this article, the statistics it was
based on, and the reaction. It should remind you of the ethical
obligation to perform statistical analysis with care, interpret the
results with appropriate skepticism, and present them to the public
accurately and honestly.</p>
</div>
</div>
<div class="sect2">
<h3 id="_confidence_intervals"><a class="anchor" href="#_confidence_intervals"></a><a class="link" href="#_confidence_intervals">13.7. Confidence intervals</a></h3>
<div class="paragraph">
<p>Kaplan-Meier analysis yields a single estimate of the survival curve,
but it is also important to quantify the uncertainty of the estimate. As
usual, there are three possible sources of error: measurement error,
sampling error, and modeling error.</p>
</div>
<div class="paragraph">
<p>In this example, measurement error is probably small. People generally
know when they were born, whether they’ve been married, and when. And
they can be expected to report this information accurately.</p>
</div>
<div class="paragraph">
<p>We can quantify sampling error by resampling. Here’s the code:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def ResampleSurvival(resp, iters=101):
    low, high = resp.agemarry.min(), resp.agemarry.max()
    ts = np.arange(low, high, 1/12.0)

    ss_seq = []
    for i in range(iters):
        sample = thinkstats2.ResampleRowsWeighted(resp)
        hf, sf = EstimateSurvival(sample)
        ss_seq.append(sf.Probs(ts))

    low, high = thinkstats2.PercentileRows(ss_seq, [5, 95])
    thinkplot.FillBetween(ts, low, high)</code></pre>
</div>
</div>
<div class="paragraph">
<p>ResampleSurvival takes resp, a DataFrame of respondents, and iters, the
number of times to resample. It computes ts, which is the sequence of
ages where we will evaluate the survival curves.</p>
</div>
<div class="paragraph">
<p>Inside the loop, ResampleSurvival:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Resamples the respondents using ResampleRowsWeighted, which we saw in
Section <a href="#weighted">10.7</a>.</p>
</li>
<li>
<p>Calls EstimateSurvival, which uses the process in the previous
sections to estimate the hazard and survival curves, and</p>
</li>
<li>
<p>Evaluates the survival curve at each age in ts.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><code>ss_seq</code> is a sequence of evaluated survival curves. PercentileRows
takes this sequence and computes the 5th and 95th percentiles, returning
a 90% confidence interval for the survival curve.</p>
</div>
<div class="paragraph">
<p>image::figs/survival3.png[Survival curve for age at first marriage (dark
line) and a 90% confidence interval based on weighted resampling (gray
line).,height=240]</p>
</div>
<div class="paragraph">
<p>Figure <a href="#survival3">[survival3]</a> shows the result along with the survival
curve we estimated in the previous section. The confidence interval
takes into account the sampling weights, unlike the estimated curve. The
discrepancy between them indicates that the sampling weights have a
substantial effect on the estimate—we will have to keep that in mind.</p>
</div>
</div>
<div class="sect2">
<h3 id="_cohort_effects"><a class="anchor" href="#_cohort_effects"></a><a class="link" href="#_cohort_effects">13.8. Cohort effects</a></h3>
<div class="paragraph">
<p>One of the challenges of survival analysis is that different parts of
the estimated curve are based on different groups of respondents. The
part of the curve at time t is based on respondents whose age was at
least t when they were interviewed. So the leftmost part of the curve
includes data from all respondents, but the rightmost part includes only
the oldest respondents.</p>
</div>
<div class="paragraph">
<p>If the relevant characteristics of the respondents are not changing over
time, that’s fine, but in this case it seems likely that marriage
patterns are different for women born in different generations. We can
investigate this effect by grouping respondents according to their
decade of birth. Groups like this, defined by date of birth or similar
events, are called <strong>cohorts</strong>, and differences between the groups are
called <strong>cohort effects</strong>.</p>
</div>
<div class="paragraph">
<p>To investigate cohort effects in the NSFG marriage data, I gathered the
Cycle 6 data from 2002 used throughout this book; the Cycle 7 data from
2006–2010 used in Section <a href="#replication">[replication]</a>; and the Cycle 5 data
from 1995. In total these datasets include 30,769 respondents.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    resp5 = ReadFemResp1995()
    resp6 = ReadFemResp2002()
    resp7 = ReadFemResp2010()
    resps = [resp5, resp6, resp7]</code></pre>
</div>
</div>
<div class="paragraph">
<p>For each DataFrame, resp, I use cmbirth to compute the decade of birth
for each respondent:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    month0 = pandas.to_datetime('1899-12-15')
    dates = [month0 + pandas.DateOffset(months=cm)
             for cm in resp.cmbirth]
    resp['decade'] = (pandas.DatetimeIndex(dates).year - 1900) // 10</code></pre>
</div>
</div>
<div class="paragraph">
<p>cmbirth is encoded as the integer number of months since December 1899;
month0 represents that date as a Timestamp object. For each birth date,
we instantiate a DateOffset that contains the century-month and add it
to month0; the result is a sequence of Timestamps, which is converted to
a DateTimeIndex. Finally, we extract year and compute decades.</p>
</div>
<div class="paragraph">
<p>To take into account the sampling weights, and also to show variability
due to sampling error, I resample the data, group respondents by decade,
and plot survival curves:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    for i in range(iters):
        samples = [thinkstats2.ResampleRowsWeighted(resp)
                   for resp in resps]
        sample = pandas.concat(samples, ignore_index=True)
        groups = sample.groupby('decade')

        EstimateSurvivalByDecade(groups, alpha=0.2)</code></pre>
</div>
</div>
<div class="paragraph">
<p>Data from the three NSFG cycles use different sampling weights, so I
resample them separately and then use concat to merge them into a single
DataFrame. The parameter <code>ignore_index</code> tells concat not to match up
respondents by index; instead it creates a new index from 0 to 30768.</p>
</div>
<div class="paragraph">
<p>EstimateSurvivalByDecade plots survival curves for each cohort:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def EstimateSurvivalByDecade(resp):
    for name, group in groups:
        hf, sf = EstimateSurvival(group)
        thinkplot.Plot(sf)</code></pre>
</div>
</div>
<div class="paragraph">
<p>image::figs/survival4.png[Survival curves for respondents born during
different decades.,height=240]</p>
</div>
<div class="paragraph">
<p>Figure <a href="#survival4">[survival4]</a> shows the results. Several patterns are
visible:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Women born in the 50s married earliest, with successive cohorts
marrying later and later, at least until age 30 or so.</p>
</li>
<li>
<p>Women born in the 60s follow a surprising pattern. Prior to age 25,
they were marrying at slower rates than their predecessors. After age
25, they were marrying faster. By age 32 they had overtaken the 50s
cohort, and at age 44 they are substantially more likely to have
married.</p>
<div class="paragraph">
<p>Women born in the 60s turned 25 between 1985 and 1995. Remembering that
the <em>Newsweek</em> article I mentioned was published in 1986, it is tempting
to imagine that the article triggered a marriage boom. That explanation
would be too pat, but it is possible that the article and the reaction
to it were indicative of a mood that affected the behavior of this
cohort.</p>
</div>
</li>
<li>
<p>The pattern of the 70s cohort is similar. They are less likely than
their predecessors to be married before age 25, but at age 35 they have
caught up with both of the previous cohorts.</p>
</li>
<li>
<p>Women born in the 80s are even less likely to marry before age 25.
What happens after that is not clear; for more data, we have to wait for
the next cycle of the NSFG.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>In the meantime we can make some predictions.</p>
</div>
</div>
<div class="sect2">
<h3 id="_extrapolation"><a class="anchor" href="#_extrapolation"></a><a class="link" href="#_extrapolation">13.9. Extrapolation</a></h3>
<div class="paragraph">
<p>The survival curve for the 70s cohort ends at about age 38; for the 80s
cohort it ends at age 28, and for the 90s cohort we hardly have any data
at all.</p>
</div>
<div class="paragraph">
<p>We can extrapolate these curves by &#8220;borrowing&#8221; data from the previous
cohort. HazardFunction provides a method, Extend, that copies the tail
from another longer HazardFunction:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python"># class HazardFunction

    def Extend(self, other):
        last = self.series.index[-1]
        more = other.series[other.series.index &gt; last]
        self.series = pandas.concat([self.series, more])</code></pre>
</div>
</div>
<div class="paragraph">
<p>As we saw in Section <a href="#hazard">13.2</a>, the HazardFunction contains a
Series that maps from \(t\) to \(\lambda(t)\). Extend
finds last, which is the last index in self.series, selects values from
other that come later than last, and appends them onto self.series.</p>
</div>
<div class="paragraph">
<p>Now we can extend the HazardFunction for each cohort, using values from
the predecessor:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def PlotPredictionsByDecade(groups):
    hfs = []
    for name, group in groups:
        hf, sf = EstimateSurvival(group)
        hfs.append(hf)

    thinkplot.PrePlot(len(hfs))
    for i, hf in enumerate(hfs):
        if i &gt; 0:
            hf.Extend(hfs[i-1])
        sf = hf.MakeSurvival()
        thinkplot.Plot(sf)</code></pre>
</div>
</div>
<div class="paragraph">
<p>groups is a GroupBy object with respondents grouped by decade of birth.
The first loop computes the HazardFunction for each group.</p>
</div>
<div class="paragraph">
<p>The second loop extends each HazardFunction with values from its
predecessor, which might contain values from the previous group, and so
on. Then it converts each HazardFunction to a SurvivalFunction and plots
it.</p>
</div>
<div class="paragraph">
<p>image::figs/survival5.png[Survival curves for respondents born during
different decades, with predictions for the later cohorts.,height=240]</p>
</div>
<div class="paragraph">
<p>Figure <a href="#survival5">[survival5]</a> shows the results; I’ve removed the 50s
cohort to make the predictions more visible. These results suggest that
by age 40, the most recent cohorts will converge with the 60s cohort,
with fewer than 20% never married.</p>
</div>
</div>
<div class="sect2">
<h3 id="_expected_remaining_lifetime"><a class="anchor" href="#_expected_remaining_lifetime"></a><a class="link" href="#_expected_remaining_lifetime">13.10. Expected remaining lifetime</a></h3>
<div class="paragraph">
<p>Given a survival curve, we can compute the expected remaining lifetime
as a function of current age. For example, given the survival curve of
pregnancy length from Section <a href="#survival">13.1</a>, we can compute the
expected time until delivery.</p>
</div>
<div class="paragraph">
<p>The first step is to extract the PMF of lifetimes. SurvivalFunction
provides a method that does that:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python"># class SurvivalFunction

    def MakePmf(self, filler=None):
        pmf = thinkstats2.Pmf()
        for val, prob in self.cdf.Items():
            pmf.Set(val, prob)

        cutoff = self.cdf.ps[-1]
        if filler is not None:
            pmf[filler] = 1-cutoff

        return pmf</code></pre>
</div>
</div>
<div class="paragraph">
<p>Remember that the SurvivalFunction contains the Cdf of lifetimes. The
loop copies the values and probabilities from the Cdf into a Pmf.</p>
</div>
<div class="paragraph">
<p>cutoff is the highest probability in the Cdf, which is 1 if the Cdf is
complete, and otherwise less than 1. If the Cdf is incomplete, we plug
in the provided value, filler, to cap it off.</p>
</div>
<div class="paragraph">
<p>The Cdf of pregnancy lengths is complete, so we don’t have to worry
about this detail yet.</p>
</div>
<div class="paragraph">
<p>The next step is to compute the expected remaining lifetime, where
&#8220;expected&#8221; means average. SurvivalFunction provides a method that does
that, too:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python"># class SurvivalFunction

    def RemainingLifetime(self, filler=None, func=thinkstats2.Pmf.Mean):
        pmf = self.MakePmf(filler=filler)
        d = {}
        for t in sorted(pmf.Values())[:-1]:
            pmf[t] = 0
            pmf.Normalize()
            d[t] = func(pmf) - t

        return pandas.Series(d)</code></pre>
</div>
</div>
<div class="paragraph">
<p>RemainingLifetime takes filler, which is passed along to MakePmf, and
func which is the function used to summarize the distribution of
remaining lifetimes.</p>
</div>
<div class="paragraph">
<p>pmf is the Pmf of lifetimes extracted from the SurvivalFunction. d is a
dictionary that contains the results, a map from current age, t, to
expected remaining lifetime.</p>
</div>
<div class="paragraph">
<p>The loop iterates through the values in the Pmf. For each value of t it
computes the conditional distribution of lifetimes, given that the
lifetime exceeds t. It does that by removing values from the Pmf one at
a time and renormalizing the remaining values.</p>
</div>
<div class="paragraph">
<p>Then it uses func to summarize the conditional distribution. In this
example the result is the mean pregnancy length, given that the length
exceeds t. By subtracting t we get the mean remaining pregnancy length.</p>
</div>
<div class="paragraph">
<p>image::figs/survival6.png[Expected remaining pregnancy length (left) and
years until first marriage (right).,height=240]</p>
</div>
<div class="paragraph">
<p>Figure <a href="#survival6">[survival6]</a> (left) shows the expected remaining
pregnancy length as a function of the current duration. For example,
during Week 0, the expected remaining duration is about 34 weeks. That’s
less than full term (39 weeks) because terminations of pregnancy in the
first trimester bring the average down.</p>
</div>
<div class="paragraph">
<p>The curve drops slowly during the first trimester. After 13 weeks, the
expected remaining lifetime has dropped by only 9 weeks, to 25. After
that the curve drops faster, by about a week per week.</p>
</div>
<div class="paragraph">
<p>Between Week 37 and 42, the curve levels off between 1 and 2 weeks. At
any time during this period, the expected remaining lifetime is the
same; with each week that passes, the destination gets no closer.
Processes with this property are called <strong>memoryless</strong> because the past
has no effect on the predictions. This behavior is the mathematical
basis of the infuriating mantra of obstetrics nurses: &#8220;any day now.&#8221;</p>
</div>
<div class="paragraph">
<p>Figure <a href="#survival6">[survival6]</a> (right) shows the median remaining time
until first marriage, as a function of age. For an 11 year-old girl, the
median time until first marriage is about 14 years. The curve decreases
until age 22 when the median remaining time is about 7 years. After that
it increases again: by age 30 it is back where it started, at 14 years.</p>
</div>
<div class="paragraph">
<p>Based on this data, young women have decreasing remaining &#8220;lifetimes&#8221;.
Mechanical components with this property are called <strong>NBUE</strong> for &#8220;new
better than used in expectation,&#8221; meaning that a new part is expected
to last longer.</p>
</div>
<div class="paragraph">
<p>Women older than 22 have increasing remaining time until first marriage.
Components with this property are called <strong>UBNE</strong> for &#8220;used better than
new in expectation.&#8221; That is, the older the part, the longer it is
expected to last. Newborns and cancer patients are also UBNE; their life
expectancy increases the longer they live.</p>
</div>
<div class="paragraph">
<p>For this example I computed median, rather than mean, because the Cdf is
incomplete; the survival curve projects that about 20% of respondents
will not marry before age 44. The age of first marriage for these women
is unknown, and might be non-existent, so we can’t compute a mean.</p>
</div>
<div class="paragraph">
<p>I deal with these unknown values by replacing them with np.inf, a
special value that represents infinity. That makes the mean infinity for
all ages, but the median is well-defined as long as more than 50% of the
remaining lifetimes are finite, which is true until age 30. After that
it is hard to define a meaningful expected remaining lifetime.</p>
</div>
<div class="paragraph">
<p>Here’s the code that computes and plots these functions:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    rem_life1 = sf1.RemainingLifetime()
    thinkplot.Plot(rem_life1)

    func = lambda pmf: pmf.Percentile(50)
    rem_life2 = sf2.RemainingLifetime(filler=np.inf, func=func)
    thinkplot.Plot(rem_life2)</code></pre>
</div>
</div>
<div class="paragraph">
<p>sf1 is the survival curve for pregnancy length; in this case we can use
the default values for RemainingLifetime.</p>
</div>
<div class="paragraph">
<p>sf2 is the survival curve for age at first marriage; func is a function
that takes a Pmf and computes its median (50th percentile).</p>
</div>
</div>
<div class="sect2">
<h3 id="_exercises_13"><a class="anchor" href="#_exercises_13"></a><a class="link" href="#_exercises_13">13.11. Exercises</a></h3>
<div class="paragraph">
<p>My solution to this exercise is in <code>chap13soln.py</code>.</p>
</div>
<div class="paragraph">
<p>In NSFG Cycles 6 and 7, the variable cmdivorcx contains the date of
divorce for the respondent’s first marriage, if applicable, encoded in
century-months.</p>
</div>
<div class="paragraph">
<p>Compute the duration of marriages that have ended in divorce, and the
duration, so far, of marriages that are ongoing. Estimate the hazard and
survival curve for the duration of marriage.</p>
</div>
<div class="paragraph">
<p>Use resampling to take into account sampling weights, and plot data from
several resamples to visualize sampling error.</p>
</div>
<div class="paragraph">
<p>Consider dividing the respondents into groups by decade of birth, and
possibly by age at first marriage.</p>
</div>
</div>
<div class="sect2">
<h3 id="_glossary_13"><a class="anchor" href="#_glossary_13"></a><a class="link" href="#_glossary_13">13.12. Glossary</a></h3>
<div class="ulist">
<ul>
<li>
<p>survival analysis: A set of methods for describing and predicting
lifetimes, or more generally time until an event occurs.</p>
</li>
<li>
<p>survival curve: A function that maps from a time, \(t\), to
the probability of surviving past \(t\).</p>
</li>
<li>
<p>hazard function: A function that maps from \(t\) to the
fraction of people alive until \(t\) who die at \(t\).</p>
</li>
<li>
<p>Kaplan-Meier estimation: An algorithm for estimating hazard and
survival functions.</p>
</li>
<li>
<p>cohort: a group of subjects defined by an event, like date of birth,
in a particular interval of time.</p>
</li>
<li>
<p>cohort effect: a difference between cohorts.</p>
</li>
<li>
<p>NBUE: A property of expected remaining lifetime, &#8220;New better than
used in expectation.&#8221;</p>
</li>
<li>
<p>UBNE: A property of expected remaining lifetime, &#8220;Used better than
new in expectation.&#8221;</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="analysis"><a class="anchor" href="#analysis"></a><a class="link" href="#analysis">14. Analytic methods</a></h2>
<div class="sectionbody">
<div class="paragraph">
<p>This book has focused on computational methods like simulation and
resampling, but some of the problems we solved have analytic solutions
that can be much faster.</p>
</div>
<div class="paragraph">
<p>I present some of these methods in this chapter, and explain how they
work. At the end of the chapter, I make suggestions for integrating
computational and analytic methods for exploratory data analysis.</p>
</div>
<div class="paragraph">
<p>The code in this chapter is in normal.py. For information about
downloading and working with this code, see Section <a href="#code">Using the code</a>.</p>
</div>
<div class="sect2">
<h3 id="why_normal"><a class="anchor" href="#why_normal"></a><a class="link" href="#why_normal">14.1. Normal distributions</a></h3>
<div class="paragraph">
<p>As a motivating example, let’s review the problem from
Section <a href="#gorilla">8.3</a>:</p>
</div>
<div class="quoteblock">
<blockquote>
<div class="paragraph">
<p>Suppose you are a scientist studying gorillas in a wildlife preserve.
Having weighed 9 gorillas, you find sample mean \(\bar{x}=90\)
kg and sample standard deviation, \(S=7.5\) kg. If you use
\(\bar{x}\) to estimate the population mean, what is the
standard error of the estimate?</p>
</div>
</blockquote>
</div>
<div class="paragraph">
<p>To answer that question, we need the sampling distribution of
\(\bar{x}\). In Section <a href="#gorilla">8.3</a> we approximated this
distribution by simulating the experiment (weighing 9 gorillas),
computing \(\bar{x}\) for each simulated experiment, and
accumulating the distribution of estimates.</p>
</div>
<div class="paragraph">
<p>The result is an approximation of the sampling distribution. Then we use
the sampling distribution to compute standard errors and confidence
intervals:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>The standard deviation of the sampling distribution is the standard
error of the estimate; in the example, it is about 2.5 kg.</p>
</li>
<li>
<p>The interval between the 5th and 95th percentile of the sampling
distribution is a 90% confidence interval. If we run the experiment many
times, we expect the estimate to fall in this interval 90% of the time.
In the example, the 90% CI is \((86, 94)\) kg.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>Now we’ll do the same calculation analytically. We take advantage of the
fact that the weights of adult female gorillas are roughly normally
distributed. Normal distributions have two properties that make them
amenable for analysis: they are &#8220;closed&#8221; under linear transformation
and addition. To explain what that means, I need some notation.</p>
</div>
<div class="paragraph">
<p>If the distribution of a quantity, \(X\), is normal with
parameters \(\mu\) and \(\sigma\), you can write</p>
</div>
<div class="stemblock">
<div class="content">
\[X \sim \mathcal{N}~(\mu, \sigma^{2})\]
</div>
</div>
<div class="paragraph">
<p>where the symbol \(\sim\) means &#8220;is distributed&#8221; and the
script letter \(\mathcal{N}\) stands for &#8220;normal.&#8221;</p>
</div>
<div class="paragraph">
<p>A linear transformation of \(X\) is something like
\(X' = a X + b\), where \(a\) and \(b\) are real
numbers. A family of distributions is closed under linear transformation
if \(X'\) is in the same family as \(X\). The normal
distribution has this property; if \(X \sim \mathcal{N}~(\mu,
\sigma^2)\),</p>
</div>
<div class="stemblock">
<div class="content">
\[X' \sim \mathcal{N}~(a \mu + b, a^{2} \sigma^2) \tag*{(1)}\]
</div>
</div>
<div class="paragraph">
<p>Normal distributions are also closed under addition. If
\(Z = X + Y\) and
\(X \sim \mathcal{N}~(\mu_{X}, \sigma_{X}^{2})\) and
\(Y \sim \mathcal{N}~(\mu_{Y}, \sigma_{Y}^{2})\) then</p>
</div>
<div class="stemblock">
<div class="content">
\[Z \sim \mathcal{N}~(\mu_X + \mu_Y, \sigma_X^2 + \sigma_Y^2)  \tag*{(2)}\]
</div>
</div>
<div class="paragraph">
<p>In the special case \(Z = X + X\), we have</p>
</div>
<div class="stemblock">
<div class="content">
\[Z \sim \mathcal{N}~(2 \mu_X, 2 \sigma_X^2)\]
</div>
</div>
<div class="paragraph">
<p>and in general if we draw \(n\) values of \(X\) and add
them up, we have</p>
</div>
<div class="stemblock">
<div class="content">
\[Z \sim \mathcal{N}~(n \mu_X, n \sigma_X^2)  \tag*{(3)}\]
</div>
</div>
</div>
<div class="sect2">
<h3 id="sampling-distributions"><a class="anchor" href="#sampling-distributions"></a><a class="link" href="#sampling-distributions">14.2. Sampling distributions</a></h3>
<div class="paragraph">
<p>Now we have everything we need to compute the sampling distribution of
\(\bar{x}\). Remember that we compute \(\bar{x}\) by
weighing \(n\) gorillas, adding up the total weight, and
dividing by \(n\).</p>
</div>
<div class="paragraph">
<p>Assume that the distribution of gorilla weights, \(X\), is
approximately normal:</p>
</div>
<div class="stemblock">
<div class="content">
\[X \sim \mathcal{N}~(\mu, \sigma^2)\]
</div>
</div>
<div class="paragraph">
<p>If we weigh \(n\) gorillas, the total weight, \(Y\), is
distributed</p>
</div>
<div class="stemblock">
<div class="content">
\[Y \sim \mathcal{N}~(n \mu, n \sigma^2)\]
</div>
</div>
<div class="paragraph">
<p>using Equation 3. And if we divide by \(n\), the sample mean,
\(Z\), is distributed</p>
</div>
<div class="stemblock">
<div class="content">
\[Z \sim \mathcal{N}~(\mu, \sigma^2/n)\]
</div>
</div>
<div class="paragraph">
<p>using Equation 1 with \(a = 1/n\).</p>
</div>
<div class="paragraph">
<p>The distribution of \(Z\) is the sampling distribution of
\(\bar{x}\). The mean of \(Z\) is \(\mu\), which
shows that \(\bar{x}\) is an unbiased estimate of
\(\mu\). The variance of the sampling distribution is
\(\sigma^2 / n\).</p>
</div>
<div class="paragraph">
<p>So the standard deviation of the sampling distribution, which is the
standard error of the estimate, is \(\sigma / \sqrt{n}\). In the
example, \(\sigma\) is 7.5 kg and \(n\) is 9, so the
standard error is 2.5 kg. That result is consistent with what we
estimated by simulation, but much faster to compute!</p>
</div>
<div class="paragraph">
<p>We can also use the sampling distribution to compute confidence
intervals. A 90% confidence interval for \(\bar{x}\) is the
interval between the 5th and 95th percentiles of \(Z\). Since
\(Z\) is normally distributed, we can compute percentiles by
evaluating the inverse CDF.</p>
</div>
<div class="paragraph">
<p>There is no closed form for the CDF of the normal distribution or its
inverse, but there are fast numerical methods and they are implemented
in SciPy, as we saw in Section <a href="#normal">5.2</a>. thinkstats2 provides a
wrapper function that makes the SciPy function a little easier to use:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def EvalNormalCdfInverse(p, mu=0, sigma=1):
    return scipy.stats.norm.ppf(p, loc=mu, scale=sigma)</code></pre>
</div>
</div>
<div class="paragraph">
<p>Given a probability, p, it returns the corresponding percentile from a
normal distribution with parameters mu and sigma. For the 90% confidence
interval of \(\bar{x}\), we compute the 5th and 95th percentiles
like this:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; thinkstats2.EvalNormalCdfInverse(0.05, mu=90, sigma=2.5)
85.888

&gt;&gt;&gt; thinkstats2.EvalNormalCdfInverse(0.95, mu=90, sigma=2.5)
94.112</pre>
</div>
</div>
<div class="paragraph">
<p>So if we run the experiment many times, we expect the estimate,
\(\bar{x}\), to fall in the range \((85.9, 94.1)\) about
90% of the time. Again, this is consistent with the result we got by
simulation.</p>
</div>
</div>
<div class="sect2">
<h3 id="_representing_normal_distributions"><a class="anchor" href="#_representing_normal_distributions"></a><a class="link" href="#_representing_normal_distributions">14.3. Representing normal distributions</a></h3>
<div class="paragraph">
<p>To make these calculations easier, I have defined a class called Normal
that represents a normal distribution and encodes the equations in the
previous sections. Here’s what it looks like:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">class Normal(object):

    def __init__(self, mu, sigma2):
        self.mu = mu
        self.sigma2 = sigma2

    def __str__(self):
        return 'N(%g, %g)' % (self.mu, self.sigma2)</code></pre>
</div>
</div>
<div class="paragraph">
<p>So we can instantiate a Normal that represents the distribution of
gorilla weights:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; dist = Normal(90, 7.5**2)
&gt;&gt;&gt; dist
N(90, 56.25)</pre>
</div>
</div>
<div class="paragraph">
<p>Normal provides Sum, which takes a sample size, n, and returns the
distribution of the sum of n values, using Equation 3:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    def Sum(self, n):
        return Normal(n * self.mu, n * self.sigma2)</code></pre>
</div>
</div>
<div class="paragraph">
<p>Normal also knows how to multiply and divide using Equation 1:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    def __mul__(self, factor):
        return Normal(factor * self.mu, factor**2 * self.sigma2)

    def __div__(self, divisor):
        return 1 / divisor * self</code></pre>
</div>
</div>
<div class="paragraph">
<p>So we can compute the sampling distribution of the mean with sample size
9:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; dist_xbar = dist.Sum(9) / 9
&gt;&gt;&gt; dist_xbar.sigma
2.5</pre>
</div>
</div>
<div class="paragraph">
<p>The standard deviation of the sampling distribution is 2.5 kg, as we saw
in the previous section. Finally, Normal provides Percentile, which we
can use to compute a confidence interval:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; dist_xbar.Percentile(5), dist_xbar.Percentile(95)
85.888 94.113</pre>
</div>
</div>
<div class="paragraph">
<p>And that’s the same answer we got before. We’ll use the Normal class
again later, but before we go on, we need one more bit of analysis.</p>
</div>
</div>
<div class="sect2">
<h3 id="CLT"><a class="anchor" href="#CLT"></a><a class="link" href="#CLT">14.4. Central limit theorem</a></h3>
<div class="paragraph">
<p>As we saw in the previous sections, if we add values drawn from normal
distributions, the distribution of the sum is normal. Most other
distributions don’t have this property; if we add values drawn from
other distributions, the sum does not generally have an analytic
distribution.</p>
</div>
<div class="paragraph">
<p>But if we add up n values from almost any distribution, the distribution
of the sum converges to normal as n increases.</p>
</div>
<div class="paragraph">
<p>More specifically, if the distribution of the values has mean and
standard deviation \(\mu\) and \(\sigma\), the
distribution of the sum is approximately
\(\mathcal{N}(n \mu, n \sigma^2)\).</p>
</div>
<div class="paragraph">
<p>This result is the Central Limit Theorem (CLT). It is one of the most
useful tools for statistical analysis, but it comes with caveats:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The values have to be drawn independently. If they are correlated, the
CLT doesn’t apply (although this is seldom a problem in practice).</p>
</li>
<li>
<p>The values have to come from the same distribution (although this
requirement can be relaxed).</p>
</li>
<li>
<p>The values have to be drawn from a distribution with finite mean and
variance. So most Pareto distributions are out.</p>
</li>
<li>
<p>The rate of convergence depends on the skewness of the distribution.
Sums from an exponential distribution converge for small n. Sums from a
lognormal distribution require larger sizes.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The Central Limit Theorem explains the prevalence of normal
distributions in the natural world. Many characteristics of living
things are affected by genetic and environmental factors whose effect is
additive. The characteristics we measure are the sum of a large number
of small effects, so their distribution tends to be normal.</p>
</div>
</div>
<div class="sect2">
<h3 id="_testing_the_clt"><a class="anchor" href="#_testing_the_clt"></a><a class="link" href="#_testing_the_clt">14.5. Testing the CLT</a></h3>
<div class="paragraph">
<p>To see how the Central Limit Theorem works, and when it doesn’t, let’s
try some experiments. First, we’ll try an exponential distribution:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def MakeExpoSamples(beta=2.0, iters=1000):
    samples = []
    for n in [1, 10, 100]:
        sample = [np.sum(np.random.exponential(beta, n))
                  for _ in range(iters)]
        samples.append((n, sample))
    return samples</code></pre>
</div>
</div>
<div class="paragraph">
<p>MakeExpoSamples generates samples of sums of exponential values (I use
&#8220;exponential values&#8221; as shorthand for &#8220;values from an exponential
distribution&#8221;). beta is the parameter of the distribution; iters is the
number of sums to generate.</p>
</div>
<div class="paragraph">
<p>To explain this function, I’ll start from the inside and work my way
out. Each time we call np.random.exponential, we get a sequence of n
exponential values and compute its sum. sample is a list of these sums,
with length iters.</p>
</div>
<div class="paragraph">
<p>It is easy to get n and iters confused: n is the number of terms in each
sum; iters is the number of sums we compute in order to characterize the
distribution of sums.</p>
</div>
<div class="paragraph">
<p>The return value is a list of (n, sample) pairs. For each pair, we make
a normal probability plot:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def NormalPlotSamples(samples, plot=1, ylabel=''):
    for n, sample in samples:
        thinkplot.SubPlot(plot)
        thinkstats2.NormalProbabilityPlot(sample)

        thinkplot.Config(title='n=%d' % n, ylabel=ylabel)
        plot += 1</code></pre>
</div>
</div>
<div class="paragraph">
<p>NormalPlotSamples takes the list of pairs from MakeExpoSamples and
generates a row of normal probability plots.</p>
</div>
<div class="paragraph">
<p>image::figs/normal1.png[Distributions of sums of exponential values (top
row) and lognormal values (bottom row).,height=336]</p>
</div>
<div class="paragraph">
<p>Figure <a href="#normal1">[normal1]</a> (top row) shows the results. With n=1, the
distribution of the sum is still exponential, so the normal probability
plot is not a straight line. But with n=10 the distribution of the sum
is approximately normal, and with n=100 it is all but indistinguishable
from normal.</p>
</div>
<div class="paragraph">
<p>Figure <a href="#normal1">[normal1]</a> (bottom row) shows similar results for a
lognormal distribution. Lognormal distributions are generally more
skewed than exponential distributions, so the distribution of sums takes
longer to converge. With n=10 the normal probability plot is nowhere
near straight, but with n=100 it is approximately normal.</p>
</div>
<div class="paragraph">
<p>image::figs/normal2.png[Distributions of sums of Pareto values (top row)
and correlated exponential values (bottom row).,height=336]</p>
</div>
<div class="paragraph">
<p>Pareto distributions are even more skewed than lognormal. Depending on
the parameters, many Pareto distributions do not have finite mean and
variance. As a result, the Central Limit Theorem does not apply.
Figure <a href="#normal2">[normal2]</a> (top row) shows distributions of sums of
Pareto values. Even with n=100 the normal probability plot is far from
straight.</p>
</div>
<div class="paragraph">
<p>I also mentioned that CLT does not apply if the values are correlated.
To test that, I generate correlated values from an exponential
distribution. The algorithm for generating correlated values is (1)
generate correlated normal values, (2) use the normal CDF to transform
the values to uniform, and (3) use the inverse exponential CDF to
transform the uniform values to exponential.</p>
</div>
<div class="paragraph">
<p>GenerateCorrelated returns an iterator of n normal values with serial
correlation rho:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def GenerateCorrelated(rho, n):
    x = random.gauss(0, 1)
    yield x

    sigma = math.sqrt(1 - rho**2)
    for _ in range(n-1):
        x = random.gauss(x*rho, sigma)
        yield x</code></pre>
</div>
</div>
<div class="paragraph">
<p>The first value is a standard normal value. Each subsequent value
depends on its predecessor: if the previous value is x, the mean of the
next value is x*rho, with variance 1-rho**2. Note that random.gauss
takes the standard deviation as the second argument, not variance.</p>
</div>
<div class="paragraph">
<p>GenerateExpoCorrelated takes the resulting sequence and transforms it to
exponential:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def GenerateExpoCorrelated(rho, n):
    normal = list(GenerateCorrelated(rho, n))
    uniform = scipy.stats.norm.cdf(normal)
    expo = scipy.stats.expon.ppf(uniform)
    return expo</code></pre>
</div>
</div>
<div class="paragraph">
<p>normal is a list of correlated normal values. uniform is a sequence of
uniform values between 0 and 1. expo is a correlated sequence of
exponential values. ppf stands for &#8220;percent point function,&#8221; which is
another name for the inverse CDF.</p>
</div>
<div class="paragraph">
<p>Figure <a href="#normal2">[normal2]</a> (bottom row) shows distributions of sums of
correlated exponential values with rho=0.9. The correlation slows the
rate of convergence; nevertheless, with n=100 the normal probability
plot is nearly straight. So even though CLT does not strictly apply when
the values are correlated, moderate correlations are seldom a problem in
practice.</p>
</div>
<div class="paragraph">
<p>These experiments are meant to show how the Central Limit Theorem works,
and what happens when it doesn’t. Now let’s see how we can use it.</p>
</div>
</div>
<div class="sect2">
<h3 id="usingCLT"><a class="anchor" href="#usingCLT"></a><a class="link" href="#usingCLT">14.6. Applying the CLT</a></h3>
<div class="paragraph">
<p>To see why the Central Limit Theorem is useful, let’s get back to the
example in Section <a href="#testdiff">9.3</a>: testing the apparent difference
in mean pregnancy length for first babies and others. As we’ve seen, the
apparent difference is about 0.078 weeks:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; live, firsts, others = first.MakeFrames()
&gt;&gt;&gt; delta = firsts.prglngth.mean() - others.prglngth.mean()
0.078</pre>
</div>
</div>
<div class="paragraph">
<p>Remember the logic of hypothesis testing: we compute a p-value, which is
the probability of the observed difference under the null hypothesis; if
it is small, we conclude that the observed difference is unlikely to be
due to chance.</p>
</div>
<div class="paragraph">
<p>In this example, the null hypothesis is that the distribution of
pregnancy lengths is the same for first babies and others. So we can
compute the sampling distribution of the mean like this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    dist1 = SamplingDistMean(live.prglngth, len(firsts))
    dist2 = SamplingDistMean(live.prglngth, len(others))</code></pre>
</div>
</div>
<div class="paragraph">
<p>Both sampling distributions are based on the same population, which is
the pool of all live births. SamplingDistMean takes this sequence of
values and the sample size, and returns a Normal object representing the
sampling distribution:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def SamplingDistMean(data, n):
    mean, var = data.mean(), data.var()
    dist = Normal(mean, var)
    return dist.Sum(n) / n</code></pre>
</div>
</div>
<div class="paragraph">
<p>mean and var are the mean and variance of data. We approximate the
distribution of the data with a normal distribution, dist.</p>
</div>
<div class="paragraph">
<p>In this example, the data are not normally distributed, so this
approximation is not very good. But then we compute dist.Sum(n) / n,
which is the sampling distribution of the mean of n values. Even if the
data are not normally distributed, the sampling distribution of the mean
is, by the Central Limit Theorem.</p>
</div>
<div class="paragraph">
<p>Next, we compute the sampling distribution of the difference in the
means. The Normal class knows how to perform subtraction using Equation
2:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    def __sub__(self, other):
        return Normal(self.mu - other.mu,
                      self.sigma2 + other.sigma2)</code></pre>
</div>
</div>
<div class="paragraph">
<p>So we can compute the sampling distribution of the difference like this:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; dist = dist1 - dist2
N(0, 0.0032)</pre>
</div>
</div>
<div class="paragraph">
<p>The mean is 0, which makes sense because we expect two samples from the
same distribution to have the same mean, on average. The variance of the
sampling distribution is 0.0032.</p>
</div>
<div class="paragraph">
<p>Normal provides Prob, which evaluates the normal CDF. We can use Prob to
compute the probability of a difference as large as delta under the null
hypothesis:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; 1 - dist.Prob(delta)
0.084</pre>
</div>
</div>
<div class="paragraph">
<p>Which means that the p-value for a one-sided test is 0.84. For a
two-sided test we would also compute</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; dist.Prob(-delta)
0.084</pre>
</div>
</div>
<div class="paragraph">
<p>Which is the same because the normal distribution is symmetric. The sum
of the tails is 0.168, which is consistent with the estimate in
Section <a href="#testdiff">9.3</a>, which was 0.17.</p>
</div>
</div>
<div class="sect2">
<h3 id="_correlation_test"><a class="anchor" href="#_correlation_test"></a><a class="link" href="#_correlation_test">14.7. Correlation test</a></h3>
<div class="paragraph">
<p>In Section <a href="#corrtest">9.5</a> we used a permutation test for the
correlation between birth weight and mother’s age, and found that it is
statistically significant, with p-value less than 0.001.</p>
</div>
<div class="paragraph">
<p>Now we can do the same thing analytically. The method is based on this
mathematical result: given two variables that are normally distributed
and uncorrelated, if we generate a sample with size \(n\),
compute Pearson’s correlation, \(r\), and then compute the
transformed correlation</p>
</div>
<div class="stemblock">
<div class="content">
\[t = r \sqrt{\frac{n-2}{1-r^2}}\]
</div>
</div>
<div class="paragraph">
<p>the distribution of \(t\) is Student’s t-distribution with
parameter \(n-2\). The t-distribution is an analytic
distribution; the CDF can be computed efficiently using gamma functions.</p>
</div>
<div class="paragraph">
<p>We can use this result to compute the sampling distribution of
correlation under the null hypothesis; that is, if we generate
uncorrelated sequences of normal values, what is the distribution of
their correlation? StudentCdf takes the sample size, n, and returns the
sampling distribution of correlation:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def StudentCdf(n):
    ts = np.linspace(-3, 3, 101)
    ps = scipy.stats.t.cdf(ts, df=n-2)
    rs = ts / np.sqrt(n - 2 + ts**2)
    return thinkstats2.Cdf(rs, ps)</code></pre>
</div>
</div>
<div class="paragraph">
<p>ts is a NumPy array of values for \(t\), the transformed
correlation. ps contains the corresponding probabilities, computed using
the CDF of the Student’s t-distribution implemented in SciPy. The
parameter of the t-distribution, df, stands for &#8220;degrees of freedom.&#8221;
I won’t explain that term, but you can read about it at
<a href="http://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics" class="bare">http://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics</a>).</p>
</div>
<div class="paragraph">
<p>image::figs/normal4.png[Sampling distribution of correlations for
uncorrelated normal variables.,height=240]</p>
</div>
<div class="paragraph">
<p>To get from ts to the correlation coefficients, rs, we apply the inverse
transform,</p>
</div>
<div class="stemblock">
<div class="content">
\[r = t / \sqrt{n - 2 + t^2}\]
</div>
</div>
<div class="paragraph">
<p>The result is the sampling distribution of \(r\) under the null
hypothesis. Figure <a href="#normal4">[normal4]</a> shows this distribution along
with the distribution we generated in Section <a href="#corrtest">9.5</a> by
resampling. They are nearly identical. Although the actual distributions
are not normal, Pearson’s coefficient of correlation is based on sample
means and variances. By the Central Limit Theorem, these moment-based
statistics are normally distributed even if the data are not.</p>
</div>
<div class="paragraph">
<p>From Figure <a href="#normal4">[normal4]</a>, we can see that the observed
correlation, 0.07, is unlikely to occur if the variables are actually
uncorrelated. Using the analytic distribution, we can compute just how
unlikely:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    t = r * math.sqrt((n-2) / (1-r**2))
    p_value = 1 - scipy.stats.t.cdf(t, df=n-2)</code></pre>
</div>
</div>
<div class="paragraph">
<p>We compute the value of t that corresponds to r=0.07, and then evaluate
the t-distribution at t. The result is 2.9e-11. This example
demonstrates an advantage of the analytic method: we can compute very
small p-values. But in practice it usually doesn’t matter.</p>
</div>
</div>
<div class="sect2">
<h3 id="_chi_squared_test"><a class="anchor" href="#_chi_squared_test"></a><a class="link" href="#_chi_squared_test">14.8. Chi-squared test</a></h3>
<div class="paragraph">
<p>In Section <a href="#casino2">9.7</a> we used the chi-squared statistic to test
whether a die is crooked. The chi-squared statistic measures the total
normalized deviation from the expected values in a table:</p>
</div>
<div class="stemblock">
<div class="content">
\[\chi^2 = \sum_i \frac{{(O_i - E_i)}^2}{E_i}\]
</div>
</div>
<div class="paragraph">
<p>One reason the chi-squared statistic is widely used is that its sampling
distribution under the null hypothesis is analytic; by a remarkable
coincidence<sup class="footnote">[<a id="_footnoteref_6" class="footnote" href="#_footnote_6" title="View footnote.">6</a>]</sup>, it is called the chi-squared
distribution. Like the t-distribution, the chi-squared CDF can be
computed efficiently using gamma functions.</p>
</div>
<div class="paragraph">
<p>image::figs/normal5.png[Sampling distribution of chi-squared statistics
for a fair six-sided die.,height=240]</p>
</div>
<div class="paragraph">
<p>SciPy provides an implementation of the chi-squared distribution, which
we use to compute the sampling distribution of the chi-squared
statistic:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def ChiSquaredCdf(n):
    xs = np.linspace(0, 25, 101)
    ps = scipy.stats.chi2.cdf(xs, df=n-1)
    return thinkstats2.Cdf(xs, ps)</code></pre>
</div>
</div>
<div class="paragraph">
<p>Figure <a href="#normal5">[normal5]</a> shows the analytic result along with the
distribution we got by resampling. They are very similar, especially in
the tail, which is the part we usually care most about.</p>
</div>
<div class="paragraph">
<p>We can use this distribution to compute the p-value of the observed test
statistic, chi2:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    p_value = 1 - scipy.stats.chi2.cdf(chi2, df=n-1)</code></pre>
</div>
</div>
<div class="paragraph">
<p>The result is 0.041, which is consistent with the result from
Section <a href="#casino2">9.7</a>.</p>
</div>
<div class="paragraph">
<p>The parameter of the chi-squared distribution is &#8220;degrees of freedom&#8221;
again. In this case the correct parameter is n-1, where n is the size of
the table, 6. Choosing this parameter can be tricky; to be honest, I am
never confident that I have it right until I generate something like
Figure <a href="#normal5">[normal5]</a> to compare the analytic results to the
resampling results.</p>
</div>
</div>
<div class="sect2">
<h3 id="_discussion"><a class="anchor" href="#_discussion"></a><a class="link" href="#_discussion">14.9. Discussion</a></h3>
<div class="paragraph">
<p>This book focuses on computational methods like resampling and
permutation. These methods have several advantages over analysis:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>They are easier to explain and understand. For example, one of the
most difficult topics in an introductory statistics class is hypothesis
testing. Many students don’t really understand what p-values are. I
think the approach I presented in Chapter <a href="#testing">9</a>—simulating
the null hypothesis and computing test statistics—makes the fundamental
idea clearer.</p>
</li>
<li>
<p>They are robust and versatile. Analytic methods are often based on
assumptions that might not hold in practice. Computational methods
require fewer assumptions, and can be adapted and extended more easily.</p>
</li>
<li>
<p>They are debuggable. Analytic methods are often like a black box: you
plug in numbers and they spit out results. But it’s easy to make subtle
errors, hard to be confident that the results are right, and hard to
find the problem if they are not. Computational methods lend themselves
to incremental development and testing, which fosters confidence in the
results.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>But there is one drawback: computational methods can be slow. Taking
into account these pros and cons, I recommend the following process:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Use computational methods during exploration. If you find a
satisfactory answer and the run time is acceptable, you can stop.</p>
</li>
<li>
<p>If run time is not acceptable, look for opportunities to optimize.
Using analytic methods is one of several methods of optimization.</p>
</li>
<li>
<p>If replacing a computational method with an analytic method is
appropriate, use the computational method as a basis of comparison,
providing mutual validation between the computational and analytic
results.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>For the vast majority of problems I have worked on, I didn’t have to go
past Step 1.</p>
</div>
</div>
<div class="sect2">
<h3 id="_exercises_14"><a class="anchor" href="#_exercises_14"></a><a class="link" href="#_exercises_14">14.10. Exercises</a></h3>
<div class="paragraph">
<p>A solution to these exercises is in <code>chap14soln.py</code></p>
</div>
<div class="paragraph">
<p><a id="log_clt"></a>[log_clt] In Section <a href="#lognormal">5.4</a>, we saw
that the distribution of adult weights is approximately lognormal. One
possible explanation is that the weight a person gains each year is
proportional to their current weight. In that case, adult weight is the
product of a large number of multiplicative factors:</p>
</div>
<div class="stemblock">
<div class="content">
\[w = w_0 f_1 f_2 ... f_n\]
</div>
</div>
<div class="paragraph">
<p>where \(w\) is adult weight, \(w_0\) is birth weight,
and \(f_i\) is the weight gain factor for year \(i\).</p>
</div>
<div class="paragraph">
<p>The log of a product is the sum of the logs of the factors:</p>
</div>
<div class="stemblock">
<div class="content">
\[\log w = \log w_0 + \log f_1 + \log f_2 + ... + \log f_n\]
</div>
</div>
<div class="paragraph">
<p>So by the Central Limit Theorem, the distribution of \(\log w\)
is approximately normal for large \(n\), which implies that the
distribution of \(w\) is lognormal.</p>
</div>
<div class="paragraph">
<p>To model this phenomenon, choose a distribution for \(f\) that
seems reasonable, then generate a sample of adult weights by choosing a
random value from the distribution of birth weights, choosing a sequence
of factors from the distribution of \(f\), and computing the
product. What value of \(n\) is needed to converge to a
lognormal distribution?</p>
</div>
<div class="paragraph">
<p>In Section <a href="#usingCLT">14.6</a> we used the Central Limit Theorem to
find the sampling distribution of the difference in means,
\(\delta\), under the null hypothesis that both samples are
drawn from the same population.</p>
</div>
<div class="paragraph">
<p>We can also use this distribution to find the standard error of the
estimate and confidence intervals, but that would only be approximately
correct. To be more precise, we should compute the sampling distribution
of \(\delta\) under the alternate hypothesis that the samples
are drawn from different populations.</p>
</div>
<div class="paragraph">
<p>Compute this distribution and use it to calculate the standard error and
a 90% confidence interval for the difference in means.</p>
</div>
<div class="paragraph">
<p>In a recent paper<sup class="footnote">[<a id="_footnoteref_7" class="footnote" href="#_footnote_7" title="View footnote.">7</a>]</sup>, Stein et al. investigate the effects of an
intervention intended to mitigate gender-stereotypical task allocation
within student engineering teams.</p>
</div>
<div class="paragraph">
<p>Before and after the intervention, students responded to a survey that
asked them to rate their contribution to each aspect of class projects
on a 7-point scale.</p>
</div>
<div class="paragraph">
<p>Before the intervention, male students reported higher scores for the
programming aspect of the project than female students; on average men
reported a score of 3.57 with standard error 0.28. Women reported 1.91,
on average, with standard error 0.32.</p>
</div>
<div class="paragraph">
<p>Compute the sampling distribution of the gender gap (the difference in
means), and test whether it is statistically significant. Because you
are given standard errors for the estimated means, you don’t need to
know the sample size to figure out the sampling distributions.</p>
</div>
<div class="paragraph">
<p>After the intervention, the gender gap was smaller: the average score
for men was 3.44 (SE 0.16); the average score for women was 3.18 (SE
0.16). Again, compute the sampling distribution of the gender gap and
test it.</p>
</div>
<div class="paragraph">
<p>Finally, estimate the change in gender gap; what is the sampling
distribution of this change, and is it statistically significant?</p>
</div>
</div>
</div>
</div>
</div>
<div id="footnotes">
<hr>
<div class="footnote" id="_footnote_1">
<a href="#_footnoteref_1">1</a>. This example is based on information and data from Dunn, &#8220;A Simple Dataset for Demonstrating Common Distributions,&#8221; Journal of Statistics Education v.7, n.3 (1999).
</div>
<div class="footnote" id="_footnote_2">
<a href="#_footnoteref_2">2</a>. I was tipped off to this possibility by a comment (without citation) at <a href="http://mathworld.wolfram.com/LogNormalDistribution.html" class="bare">http://mathworld.wolfram.com/LogNormalDistribution.html</a>. Subsequently I found a paper that proposes the log transform and suggests a cause: Penman and Johnson, &#8220;The Changing Shape of the Body Mass Index Distribution Curve in the Population,&#8221; Preventing Chronic Disease, 2006 July; 3(3): A74. Online at <a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1636707" class="bare">http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1636707</a>.
</div>
<div class="footnote" id="_footnote_3">
<a href="#_footnoteref_3">3</a>. Centers for Disease Control and Prevention (CDC). Behavioral Risk Factor Surveillance System Survey Data. Atlanta, Georgia: U.S. Department of Health and Human Services, Centers for Disease Control and Prevention, 2008.
</div>
<div class="footnote" id="_footnote_4">
<a href="#_footnoteref_4">4</a>. For more about Bayesian inference, see the sequel to this book, <em>Think Bayes</em>.
</div>
<div class="footnote" id="_footnote_5">
<a href="#_footnoteref_5">5</a>. Adapted from MacKay, <em>Information Theory, Inference, and Learning Algorithms</em>, 2003.
</div>
<div class="footnote" id="_footnote_6">
<a href="#_footnoteref_6">6</a>. Not really.
</div>
<div class="footnote" id="_footnote_7">
<a href="#_footnoteref_7">7</a>. &#8220;Evidence for the persistent effects of an intervention to mitigate gender-sterotypical task allocation within student engineering teams,&#8221; Proceedings of the IEEE Frontiers in Education Conference, 2014.
</div>
</div>
<div id="footer">
<div id="footer-text">
Version 2.0.38<br>
Last updated 2019-09-02 00:01:49 AEST
</div>
</div>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlighting()</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  messageStyle: "none",
  tex2jax: {
    inlineMath: [["\\(", "\\)"]],
    displayMath: [["\\[", "\\]"]],
    ignoreClass: "nostem|nolatexmath"
  },
  asciimath2jax: {
    delimiters: [["\\$", "\\$"]],
    ignoreClass: "nostem|noasciimath"
  },
  TeX: { equationNumbers: { autoNumber: "none" } }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.6.0/MathJax.js?config=TeX-MML-AM_HTMLorMML"></script>
</body>
</html>