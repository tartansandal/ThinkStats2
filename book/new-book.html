<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<!--[if IE]><meta http-equiv="X-UA-Compatible" content="IE=edge"><![endif]-->
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Asciidoctor 1.5.6.1">
<meta name="author" content="Allen B. Downey">
<title>Think Stats: Exploratory Data Analysis in Python</title>
<style>
@import "asciidoctor.css";

/* local rules here */

/* Make the page width approx the same as the book */
#content, #header {
    max-width: 45em;
}

/* If the screen is wide enough, position the advert-bar to the right */
@media only screen and (min-width: 1200px) {
    .sidebarblock.advert-bar {
        width: 15em;
        font-size: smaller;

        position: absolute;
        left: auto;
        right: 0;
        top: 0;

        padding: 0em 1em;
        margin-top: 0!important;
        background-color: #f8f8f7;

        border-right-width: 0;
        border-left: 1px solid #efefed;
        border-top-width: 0!important;
        border-bottom-width: 0!important;

        z-index: 1000;
        overflow: auto;
    }

    .sidebarblock.advert-bar h2 {
        font-size: 1.3575em;
    }

    /* Add some space for the advert-bar */
    #content, #header, #footnotes {
        padding-right: 15.9375em; /* 0.9375em + 15em */
        max-width: 60em;          /* 45em     + 15em */
    }
}

/* Match image sizes to the book */
div.content > img { height: 240pt; }

#scatter2 div.content > img,
#scatter1 div.content > img,
#class_size1 div.content > img,
#survival1 div.content > img,
#probability_nsfg_pmf div.content > img
{ height: 288pt; }

#density_wtkg2_kde div.content > img,
#density_totalwgt_kde div.content > img,
#dist_framework div.content > img,
#pdf_example div.content > img
{ height: 211pt; }

#normal1 div.content > img,
#normal2 div.content > img
{ height: 336pt; }

#timeseries1 div.content > img
{ width: 336pt; }

</style>
</head>
<body class="book toc2 toc-left">
<div id="header">
<h1>Think Stats: Exploratory Data Analysis in Python</h1>
<div class="details">
<span id="author" class="author">Allen B. Downey</span><br>
<span id="revnumber">version 2.0.38</span>
</div>
<div id="toc" class="toc2">
<div id="toctitle">Table of Contents</div>
<ul class="sectlevel1">
<li><a href="#_colphon">Colphon</a></li>
<li><a href="#_preface">Preface</a>
<ul class="sectlevel2">
<li><a href="#_how_i_wrote_this_book">How I wrote this book</a></li>
<li><a href="#code">Using the code</a></li>
<li><a href="#_contributor_list">Contributor List</a></li>
</ul>
</li>
<li><a href="#intro">1. Exploratory data analysis</a>
<ul class="sectlevel2">
<li><a href="#_a_statistical_approach">1.1. A statistical approach</a></li>
<li><a href="#nsfg">1.2. The National Survey of Family Growth</a></li>
<li><a href="#_importing_the_data">1.3. Importing the data</a></li>
<li><a href="#dataframe">1.4. DataFrames</a></li>
<li><a href="#_variables">1.5. Variables</a></li>
<li><a href="#cleaning">1.6. Transformation</a></li>
<li><a href="#_validation">1.7. Validation</a></li>
<li><a href="#_interpretation">1.8. Interpretation</a></li>
<li><a href="#_exercises">1.9. Exercises</a></li>
<li><a href="#_glossary">1.10. Glossary</a></li>
</ul>
</li>
<li><a href="#descriptive">2. Distributions</a>
<ul class="sectlevel2">
<li><a href="#_histograms">2.1. Histograms</a></li>
<li><a href="#_representing_histograms">2.2. Representing histograms</a></li>
<li><a href="#_plotting_histograms">2.3. Plotting histograms</a></li>
<li><a href="#_nsfg_variables">2.4. NSFG variables</a></li>
<li><a href="#_outliers">2.5. Outliers</a></li>
<li><a href="#_first_babies">2.6. First babies</a></li>
<li><a href="#mean">2.7. Summarizing distributions</a></li>
<li><a href="#_variance">2.8. Variance</a></li>
<li><a href="#_effect_size">2.9. Effect size</a></li>
<li><a href="#_reporting_results">2.10. Reporting results</a></li>
<li><a href="#_exercises_2">2.11. Exercises</a></li>
<li><a href="#_glossary_2">2.12. Glossary</a></li>
</ul>
</li>
<li><a href="#_probability_mass_functions">3. Probability mass functions</a>
<ul class="sectlevel2">
<li><a href="#_pmfs">3.1. Pmfs</a></li>
<li><a href="#_plotting_pmfs">3.2. Plotting PMFs</a></li>
<li><a href="#visualization">3.3. Other visualizations</a></li>
<li><a href="#_the_class_size_paradox">3.4. The class size paradox</a></li>
<li><a href="#_dataframe_indexing">3.5. DataFrame indexing</a></li>
<li><a href="#_exercises_3">3.6. Exercises</a></li>
<li><a href="#_glossary_3">3.7. Glossary</a></li>
</ul>
</li>
<li><a href="#cumulative">4. Cumulative distribution functions</a>
<ul class="sectlevel2">
<li><a href="#_the_limits_of_pmfs">4.1. The limits of PMFs</a></li>
<li><a href="#_percentiles">4.2. Percentiles</a></li>
<li><a href="#_cdfs">4.3. CDFs</a></li>
<li><a href="#_representing_cdfs">4.4. Representing CDFs</a></li>
<li><a href="#birth_weights">4.5. Comparing CDFs</a></li>
<li><a href="#_percentile_based_statistics">4.6. Percentile-based statistics</a></li>
<li><a href="#random">4.7. Random numbers</a></li>
<li><a href="#_comparing_percentile_ranks">4.8. Comparing percentile ranks</a></li>
<li><a href="#_exercises_4">4.9. Exercises</a></li>
<li><a href="#_glossary_4">4.10. Glossary</a></li>
</ul>
</li>
<li><a href="#modeling">5. Modeling distributions</a>
<ul class="sectlevel2">
<li><a href="#exponential">5.1. The exponential distribution</a></li>
<li><a href="#normal">5.2. The normal distribution</a></li>
<li><a href="#_normal_probability_plot">5.3. Normal probability plot</a></li>
<li><a href="#lognormal">5.4. The lognormal distribution</a></li>
<li><a href="#_the_pareto_distribution">5.5. The Pareto distribution</a></li>
<li><a href="#_generating_random_numbers">5.6. Generating random numbers</a></li>
<li><a href="#_why_model">5.7. Why model?</a></li>
<li><a href="#_exercises_5">5.8. Exercises</a></li>
<li><a href="#_glossary_5">5.9. Glossary</a></li>
</ul>
</li>
<li><a href="#density">6. Probability density functions</a>
<ul class="sectlevel2">
<li><a href="#_pdfs">6.1. PDFs</a></li>
<li><a href="#_kernel_density_estimation">6.2. Kernel density estimation</a></li>
<li><a href="#_the_distribution_framework">6.3. The distribution framework</a></li>
<li><a href="#_hist_implementation">6.4. Hist implementation</a></li>
<li><a href="#_pmf_implementation">6.5. Pmf implementation</a></li>
<li><a href="#_cdf_implementation">6.6. Cdf implementation</a></li>
<li><a href="#_moments">6.7. Moments</a></li>
<li><a href="#_skewness">6.8. Skewness</a></li>
<li><a href="#_exercises_6">6.9. Exercises</a></li>
<li><a href="#_glossary_6">6.10. Glossary</a></li>
</ul>
</li>
<li><a href="#_relationships_between_variables">7. Relationships between variables</a>
<ul class="sectlevel2">
<li><a href="#_scatter_plots">7.1. Scatter plots</a></li>
<li><a href="#characterizing">7.2. Characterizing relationships</a></li>
<li><a href="#_correlation">7.3. Correlation</a></li>
<li><a href="#_covariance">7.4. Covariance</a></li>
<li><a href="#_pearson_s_correlation">7.5. Pearson’s correlation</a></li>
<li><a href="#_nonlinear_relationships">7.6. Nonlinear relationships</a></li>
<li><a href="#_spearman_s_rank_correlation">7.7. Spearman’s rank correlation</a></li>
<li><a href="#_correlation_and_causation">7.8. Correlation and causation</a></li>
<li><a href="#_exercises_7">7.9. Exercises</a></li>
<li><a href="#_glossary_7">7.10. Glossary</a></li>
</ul>
</li>
<li><a href="#estimation">8. Estimation</a>
<ul class="sectlevel2">
<li><a href="#_the_estimation_game">8.1. The estimation game</a></li>
<li><a href="#_guess_the_variance">8.2. Guess the variance</a></li>
<li><a href="#gorilla">8.3. Sampling distributions</a></li>
<li><a href="#_sampling_bias">8.4. Sampling bias</a></li>
<li><a href="#_exponential_distributions">8.5. Exponential distributions</a></li>
<li><a href="#_exercises_8">8.6. Exercises</a></li>
<li><a href="#_glossary_8">8.7. Glossary</a></li>
</ul>
</li>
<li><a href="#testing">9. Hypothesis testing</a>
<ul class="sectlevel2">
<li><a href="#_classical_hypothesis_testing">9.1. Classical hypothesis testing</a></li>
<li><a href="#hypotest">9.2. HypothesisTest</a></li>
<li><a href="#testdiff">9.3. Testing a difference in means</a></li>
<li><a href="#_other_test_statistics">9.4. Other test statistics</a></li>
<li><a href="#corrtest">9.5. Testing a correlation</a></li>
<li><a href="#casino">9.6. Testing proportions</a></li>
<li><a href="#casino2">9.7. Chi-squared tests</a></li>
<li><a href="#_first_babies_again">9.8. First babies again</a></li>
<li><a href="#_errors">9.9. Errors</a></li>
<li><a href="#power">9.10. Power</a></li>
<li><a href="#replication">9.11. Replication</a></li>
<li><a href="#_exercises_9">9.12. Exercises</a></li>
<li><a href="#_glossary_9">9.13. Glossary</a></li>
</ul>
</li>
<li><a href="#linear">10. Linear least squares</a>
<ul class="sectlevel2">
<li><a href="#_least_squares_fit">10.1. Least squares fit</a></li>
<li><a href="#_implementation">10.2. Implementation</a></li>
<li><a href="#_residuals">10.3. Residuals</a></li>
<li><a href="#regest">10.4. Estimation</a></li>
<li><a href="#goodness">10.5. Goodness of fit</a></li>
<li><a href="#_testing_a_linear_model">10.6. Testing a linear model</a></li>
<li><a href="#weighted">10.7. Weighted resampling</a></li>
<li><a href="#_exercises_10">10.8. Exercises</a></li>
<li><a href="#_glossary_10">10.9. Glossary</a></li>
</ul>
</li>
<li><a href="#regression">11. Regression</a>
<ul class="sectlevel2">
<li><a href="#statsmodels">11.1. StatsModels</a></li>
<li><a href="#multiple">11.2. Multiple regression</a></li>
<li><a href="#nonlinear">11.3. Nonlinear relationships</a></li>
<li><a href="#mining">11.4. Data mining</a></li>
<li><a href="#_prediction">11.5. Prediction</a></li>
<li><a href="#_logistic_regression">11.6. Logistic regression</a></li>
<li><a href="#_estimating_parameters">11.7. Estimating parameters</a></li>
<li><a href="#implementation">11.8. Implementation</a></li>
<li><a href="#_accuracy">11.9. Accuracy</a></li>
<li><a href="#_exercises_11">11.10. Exercises</a></li>
<li><a href="#_glossary_11">11.11. Glossary</a></li>
</ul>
</li>
<li><a href="#_time_series_analysis">12. Time series analysis</a>
<ul class="sectlevel2">
<li><a href="#_importing_and_cleaning">12.1. Importing and cleaning</a></li>
<li><a href="#_plotting">12.2. Plotting</a></li>
<li><a href="#timeregress">12.3. Linear regression</a></li>
<li><a href="#_moving_averages">12.4. Moving averages</a></li>
<li><a href="#_missing_values">12.5. Missing values</a></li>
<li><a href="#_serial_correlation">12.6. Serial correlation</a></li>
<li><a href="#_autocorrelation">12.7. Autocorrelation</a></li>
<li><a href="#_prediction_2">12.8. Prediction</a></li>
<li><a href="#_further_reading">12.9. Further reading</a></li>
<li><a href="#_exercises_12">12.10. Exercises</a></li>
<li><a href="#_glossary_12">12.11. Glossary</a></li>
</ul>
</li>
<li><a href="#_survival_analysis">13. Survival analysis</a>
<ul class="sectlevel2">
<li><a href="#survival">13.1. Survival curves</a></li>
<li><a href="#hazard">13.2. Hazard function</a></li>
<li><a href="#_inferring_survival_curves">13.3. Inferring survival curves</a></li>
<li><a href="#_kaplan_meier_estimation">13.4. Kaplan-Meier estimation</a></li>
<li><a href="#_the_marriage_curve">13.5. The marriage curve</a></li>
<li><a href="#_estimating_the_survival_curve">13.6. Estimating the survival curve</a></li>
<li><a href="#_confidence_intervals">13.7. Confidence intervals</a></li>
<li><a href="#_cohort_effects">13.8. Cohort effects</a></li>
<li><a href="#_extrapolation">13.9. Extrapolation</a></li>
<li><a href="#_expected_remaining_lifetime">13.10. Expected remaining lifetime</a></li>
<li><a href="#_exercises_13">13.11. Exercises</a></li>
<li><a href="#_glossary_13">13.12. Glossary</a></li>
</ul>
</li>
<li><a href="#analysis">14. Analytic methods</a>
<ul class="sectlevel2">
<li><a href="#why_normal">14.1. Normal distributions</a></li>
<li><a href="#sampling-distributions">14.2. Sampling distributions</a></li>
<li><a href="#_representing_normal_distributions">14.3. Representing normal distributions</a></li>
<li><a href="#CLT">14.4. Central limit theorem</a></li>
<li><a href="#_testing_the_clt">14.5. Testing the CLT</a></li>
<li><a href="#usingCLT">14.6. Applying the CLT</a></li>
<li><a href="#_correlation_test">14.7. Correlation test</a></li>
<li><a href="#_chi_squared_test">14.8. Chi-squared test</a></li>
<li><a href="#_discussion">14.9. Discussion</a></li>
<li><a href="#_exercises_14">14.10. Exercises</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div id="content">
<div id="preamble">
<div class="sectionbody">
<div class="sidebarblock advert-bar">
<div class="content">
<h2 id="_are_you_using_one_of_our_books_in_a_class" class="discrete">Are you using one of our books in a class?</h2>
<div class="paragraph">
<p>We&#8217;d like to know about it. Please consider filling out
<a href="http://spreadsheets.google.com/viewform?formkey=dC0tNUZkMjBEdXVoRGljNm9FRmlTMHc6MA">this
short survey</a>.</p>
</div>
<hr>
<div class="paragraph">
<p><a href="http://www.amazon.com/gp/product/1449370780/ref=as_li_qf_sp_asin_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=1449370780&amp;linkCode=as2&amp;tag=greenteapre01-20">Think
Bayes</a><span class="image"><img src="http://ir-na.amazon-adsystem.com/e/ir?t=greenteapre01-20&amp;l=as2&amp;o=1&amp;a=1449370780" alt="image" width="1" height="1"></span></p>
</div>
<div class="paragraph">
<p><a href="http://www.amazon.com/gp/product/1449370780/ref=as_li_qf_sp_asin_il?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=1449370780&amp;linkCode=as2&amp;tag=greenteapre01-20"><span class="image"><img src="http://ws-na.amazon-adsystem.com/widgets/q?<em>encoding=UTF8&amp;ASIN=1449370780&amp;Format=_SL160</em>&amp;ID=AsinImage&amp;MarketPlace=US&amp;ServiceVersion=20070822&amp;WS=1&amp;tag=greenteapre01-20" alt="image"></span></a><span class="image"><img src="http://ir-na.amazon-adsystem.com/e/ir?t=greenteapre01-20&amp;l=as2&amp;o=1&amp;a=1449370780" alt="image" width="1" height="1"></span></p>
</div>
<div class="paragraph">
<p><a href="http://www.amazon.com/gp/product/144933072X/ref=as_li_tf_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=144933072X&amp;linkCode=as2&amp;tag=greenteapre01-20">Think
Python</a><span class="image"><img src="http://www.assoc-amazon.com/e/ir?t=greenteapre01-20&amp;l=as2&amp;o=1&amp;a=144933072X" alt="image" width="1" height="1"></span></p>
</div>
<div class="paragraph">
<p><a href="http://www.amazon.com/gp/product/144933072X/ref=as_li_tf_il?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=144933072X&amp;linkCode=as2&amp;tag=greenteapre01-20"><span class="image"><img src="http://ws-na.amazon-adsystem.com/widgets/q?<em>encoding=UTF8&amp;ASIN=144933072X&amp;Format=_SL160</em>&amp;ID=AsinImage&amp;MarketPlace=US&amp;ServiceVersion=20070822&amp;WS=1&amp;tag=greenteapre01-20" alt="image"></span></a><span class="image"><img src="http://www.assoc-amazon.com/e/ir?t=greenteapre01-20&amp;l=as2&amp;o=1&amp;a=144933072X" alt="image" width="1" height="1"></span></p>
</div>
<div class="paragraph">
<p><a href="http://www.amazon.com/gp/product/1491907339/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=1491907339&amp;linkCode=as2&amp;tag=greenteapre01-20&amp;linkId=O7WYM6H6YBYUFNWU">Think
Stats</a><span class="image"><img src="http://ir-na.amazon-adsystem.com/e/ir?t=greenteapre01-20&amp;l=as2&amp;o=1&amp;a=1491907339" alt="image" width="1" height="1"></span></p>
</div>
<div class="paragraph">
<p><a href="http://www.amazon.com/gp/product/1491907339/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=1491907339&amp;linkCode=as2&amp;tag=greenteapre01-20&amp;linkId=JVSYKQHYSUIEYRHL"><span class="image"><img src="http://ws-na.amazon-adsystem.com/widgets/q?<em>encoding=UTF8&amp;ASIN=1491907339&amp;Format=_SL160</em>&amp;ID=AsinImage&amp;MarketPlace=US&amp;ServiceVersion=20070822&amp;WS=1&amp;tag=greenteapre01-20" alt="image"></span></a><span class="image"><img src="http://ir-na.amazon-adsystem.com/e/ir?t=greenteapre01-20&amp;l=as2&amp;o=1&amp;a=1491907339" alt="image" width="1" height="1"></span></p>
</div>
<div class="paragraph">
<p><a href="http://www.amazon.com/gp/product/1449314635/ref=as_li_tf_tl?ie=UTF8&amp;tag=greenteapre01-20&amp;linkCode=as2&amp;camp=1789&amp;creative=9325&amp;creativeASIN=1449314635">Think
Complexity</a><span class="image"><img src="http://www.assoc-amazon.com/e/ir?t=greenteapre01-20&amp;l=as2&amp;o=1&amp;a=1449314635" alt="image" width="1" height="1"></span></p>
</div>
<div class="paragraph">
<p><a href="http://www.amazon.com/gp/product/1449314635/ref=as_li_tf_il?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=1449314635&amp;linkCode=as2&amp;tag=greenteapre01-20"><span class="image"><img src="http://ws-na.amazon-adsystem.com/widgets/q?<em>encoding=UTF8&amp;ASIN=1449314635&amp;Format=_SL160</em>&amp;ID=AsinImage&amp;MarketPlace=US&amp;ServiceVersion=20070822&amp;WS=1&amp;tag=greenteapre01-20" alt="image"></span></a><span class="image"><img src="http://www.assoc-amazon.com/e/ir?t=greenteapre01-20&amp;l=as2&amp;o=1&amp;a=1449314635" alt="image" width="1" height="1"></span></p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_colphon"><a class="anchor" href="#_colphon"></a><a class="link" href="#_colphon">Colphon</a></h2>
<div class="sectionbody">
<div class="paragraph">
<p>Green Tea Press</p>
</div>
<div class="paragraph">
<p>Needham, Massachusetts</p>
</div>
<div class="paragraph">
<p>Copyright © 2014 Allen B. Downey.</p>
</div>
<div class="paragraph">
<p>Green Tea Press<br>
9 Washburn Ave<br>
Needham MA 02492</p>
</div>
<div class="paragraph">
<p>Permission is granted to copy, distribute, and/or modify this document
under the terms of the Creative Commons
Attribution-NonCommercial-ShareAlike 4.0 International License, which is
available at <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/" class="bare">http://creativecommons.org/licenses/by-nc-sa/4.0/</a>.</p>
</div>
<div class="paragraph">
<p>The original form of this book is LaTeX source code. Compiling this code
has the effect of generating a device-independent representation of a
textbook, which can be converted to other formats and printed.</p>
</div>
<div class="paragraph">
<p>The LaTeX source for this book is available from <a href="http://thinkstats2.com" class="bare">http://thinkstats2.com</a>.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_preface"><a class="anchor" href="#_preface"></a><a class="link" href="#_preface">Preface</a></h2>
<div class="sectionbody">
<div class="paragraph">
<p>This book is an introduction to the practical tools of exploratory data
analysis. The organization of the book follows the process I use when I
start working with a dataset:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Importing and cleaning: Whatever format the data is in, it usually
takes some time and effort to read the data, clean and transform it, and
check that everything made it through the translation process intact.</p>
</li>
<li>
<p>Single variable explorations: I usually start by examining one
variable at a time, finding out what the variables mean, looking at
distributions of the values, and choosing appropriate summary
statistics.</p>
</li>
<li>
<p>Pair-wise explorations: To identify possible relationships between
variables, I look at tables and scatter plots, and compute correlations
and linear fits.</p>
</li>
<li>
<p>Multivariate analysis: If there are apparent relationships between
variables, I use multiple regression to add control variables and
investigate more complex relationships.</p>
</li>
<li>
<p>Estimation and hypothesis testing: When reporting statistical results,
it is important to answer three questions: How big is the effect? How
much variability should we expect if we run the same measurement again?
Is it possible that the apparent effect is due to chance?</p>
</li>
<li>
<p>Visualization: During exploration, visualization is an important tool
for finding possible relationships and effects. Then if an apparent
effect holds up to scrutiny, visualization is an effective way to
communicate results.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>This book takes a computational approach, which has several advantages
over mathematical approaches:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>I present most ideas using Python code, rather than mathematical
notation. In general, Python code is more readable; also, because it is
executable, readers can download it, run it, and modify it.</p>
</li>
<li>
<p>Each chapter includes exercises readers can do to develop and solidify
their learning. When you write programs, you express your understanding
in code; while you are debugging the program, you are also correcting
your understanding.</p>
</li>
<li>
<p>Some exercises involve experiments to test statistical behavior. For
example, you can explore the Central Limit Theorem (CLT) by generating
random samples and computing their sums. The resulting visualizations
demonstrate why the CLT works and when it doesn’t.</p>
</li>
<li>
<p>Some ideas that are hard to grasp mathematically are easy to
understand by simulation. For example, we approximate p-values by
running random simulations, which reinforces the meaning of the p-value.</p>
</li>
<li>
<p>Because the book is based on a general-purpose programming language
(Python), readers can import data from almost any source. They are not
limited to datasets that have been cleaned and formatted for a
particular statistics tool.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The book lends itself to a project-based approach. In my class, students
work on a semester-long project that requires them to pose a statistical
question, find a dataset that can address it, and apply each of the
techniques they learn to their own data.</p>
</div>
<div class="paragraph">
<p>To demonstrate my approach to statistical analysis, the book presents a
case study that runs through all of the chapters. It uses data from two
sources:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The National Survey of Family Growth (NSFG), conducted by the U.S.
Centers for Disease Control and Prevention (CDC) to gather &#8220;information
on family life, marriage and divorce, pregnancy, infertility, use of
contraception, and men’s and women’s health.&#8221; (See
<a href="http://cdc.gov/nchs/nsfg.htm." class="bare">http://cdc.gov/nchs/nsfg.htm.</a>)</p>
</li>
<li>
<p>The Behavioral Risk Factor Surveillance System (BRFSS), conducted by
the National Center for Chronic Disease Prevention and Health Promotion
to &#8220;track health conditions and risk behaviors in the United States.&#8221;
(See <a href="http://cdc.gov/BRFSS/." class="bare">http://cdc.gov/BRFSS/.</a>)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Other examples use data from the IRS, the U.S. Census, and the Boston
Marathon.</p>
</div>
<div class="paragraph">
<p>This second edition of <em>Think Stats</em> includes the chapters from the
first edition, many of them substantially revised, and new chapters on
regression, time series analysis, survival analysis, and analytic
methods. The previous edition did not use pandas, SciPy, or StatsModels,
so all of that material is new.</p>
</div>
<div class="sect2">
<h3 id="_how_i_wrote_this_book"><a class="anchor" href="#_how_i_wrote_this_book"></a><a class="link" href="#_how_i_wrote_this_book">How I wrote this book</a></h3>
<div class="paragraph">
<p>When people write a new textbook, they usually start by reading a stack
of old textbooks. As a result, most books contain the same material in
pretty much the same order.</p>
</div>
<div class="paragraph">
<p>I did not do that. In fact, I used almost no printed material while I
was writing this book, for several reasons:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>My goal was to explore a new approach to this material, so I didn’t
want much exposure to existing approaches.</p>
</li>
<li>
<p>Since I am making this book available under a free license, I wanted
to make sure that no part of it was encumbered by copyright
restrictions.</p>
</li>
<li>
<p>Many readers of my books don’t have access to libraries of printed
material, so I tried to make references to resources that are freely
available on the Internet.</p>
</li>
<li>
<p>Some proponents of old media think that the exclusive use of
electronic resources is lazy and unreliable. They might be right about
the first part, but I think they are wrong about the second, so I wanted
to test my theory.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The resource I used more than any other is Wikipedia. In general, the
articles I read on statistical topics were very good (although I made a
few small changes along the way). I include references to Wikipedia
pages throughout the book and I encourage you to follow those links; in
many cases, the Wikipedia page picks up where my description leaves off.
The vocabulary and notation in this book are generally consistent with
Wikipedia, unless I had a good reason to deviate. Other resources I
found useful were Wolfram MathWorld and the Reddit statistics forum,
<a href="http://www.reddit.com/r/statistics" class="bare">http://www.reddit.com/r/statistics</a>.</p>
</div>
</div>
<div class="sect2">
<h3 id="code"><a class="anchor" href="#code"></a><a class="link" href="#code">Using the code</a></h3>
<div class="paragraph">
<p>The code and data used in this book are available from
<a href="https://github.com/AllenDowney/ThinkStats2" class="bare">https://github.com/AllenDowney/ThinkStats2</a>. Git is a version control
system that allows you to keep track of the files that make up a
project. A collection of files under Git’s control is called a
<strong>repository</strong>. GitHub is a hosting service that provides storage for Git
repositories and a convenient web interface.</p>
</div>
<div class="paragraph">
<p>The GitHub homepage for my repository provides several ways to work with
the code:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>You can create a copy of my repository on GitHub by pressing the Fork
button. If you don’t already have a GitHub account, you’ll need to
create one. After forking, you’ll have your own repository on GitHub
that you can use to keep track of code you write while working on this
book. Then you can clone the repo, which means that you make a copy of
the files on your computer.</p>
</li>
<li>
<p>Or you could clone my repository. You don’t need a GitHub account to
do this, but you won’t be able to write your changes back to GitHub.</p>
</li>
<li>
<p>If you don’t want to use Git at all, you can download the files in a
Zip file using the button in the lower-right corner of the GitHub page.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>All of the code is written to work in both Python 2 and Python 3 with no
translation.</p>
</div>
<div class="paragraph">
<p>I developed this book using Anaconda from Continuum Analytics, which is
a free Python distribution that includes all the packages you’ll need to
run the code (and lots more). I found Anaconda easy to install. By
default it does a user-level installation, not system-level, so you
don’t need administrative privileges. And it supports both Python 2 and
Python 3. You can download Anaconda from <a href="http://continuum.io/downloads" class="bare">http://continuum.io/downloads</a>.</p>
</div>
<div class="paragraph">
<p>If you don’t want to use Anaconda, you will need the following packages:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>pandas for representing and analyzing data, <a href="http://pandas.pydata.org/" class="bare">http://pandas.pydata.org/</a>;</p>
</li>
<li>
<p>NumPy for basic numerical computation, <a href="http://www.numpy.org/" class="bare">http://www.numpy.org/</a>;</p>
</li>
<li>
<p>SciPy for scientific computation including statistics,
<a href="http://www.scipy.org/" class="bare">http://www.scipy.org/</a>;</p>
</li>
<li>
<p>StatsModels for regression and other statistical analysis,
<a href="http://statsmodels.sourceforge.net/" class="bare">http://statsmodels.sourceforge.net/</a>; and</p>
</li>
<li>
<p>matplotlib for visualization, <a href="http://matplotlib.org/" class="bare">http://matplotlib.org/</a>.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Although these are commonly used packages, they are not included with
all Python installations, and they can be hard to install in some
environments. If you have trouble installing them, I strongly recommend
using Anaconda or one of the other Python distributions that include
these packages.</p>
</div>
<div class="paragraph">
<p>After you clone the repository or unzip the zip file, you should have a
folder called <code>ThinkStats2/code</code> with a file called <code>nsfg.py</code>. If
you run <code>nsfg.py</code>, it should read a data file, run some tests, and
print a message like, &#8220;All tests passed.&#8221; If you get import errors, it
probably means there are packages you need to install.</p>
</div>
<div class="paragraph">
<p>Most exercises use Python scripts, but some also use the IPython
notebook. If you have not used IPython notebook before, I suggest you
start with the documentation at
<a href="http://ipython.org/ipython-doc/stable/notebook/notebook.html" class="bare">http://ipython.org/ipython-doc/stable/notebook/notebook.html</a>.</p>
</div>
<div class="paragraph">
<p>I wrote this book assuming that the reader is familiar with core Python,
including object-oriented features, but not pandas, NumPy, and SciPy. If
you are already familiar with these modules, you can skip a few
sections.</p>
</div>
<div class="paragraph">
<p>I assume that the reader knows basic mathematics, including logarithms,
for example, and summations. I refer to calculus concepts in a few
places, but you don’t have to do any calculus.</p>
</div>
<div class="paragraph">
<p>If you have never studied statistics, I think this book is a good place
to start. And if you have taken a traditional statistics class, I hope
this book will help repair the damage.</p>
</div>
<div class="paragraph">
<p>—</p>
</div>
<div class="paragraph">
<p>Allen B. Downey is a Professor of Computer Science at the Franklin W.
Olin College of Engineering in Needham, MA.</p>
</div>
</div>
<div class="sect2">
<h3 id="_contributor_list"><a class="anchor" href="#_contributor_list"></a><a class="link" href="#_contributor_list">Contributor List</a></h3>
<div class="paragraph">
<p>If you have a suggestion or correction, please send email to
<code>downey@allendowney.com</code>. If I make a change based on your feedback, I
will add you to the contributor list (unless you ask to be omitted).</p>
</div>
<div class="paragraph">
<p>If you include at least part of the sentence the error appears in, that
makes it easy for me to search. Page and section numbers are fine, too,
but not quite as easy to work with. Thanks!</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Lisa Downey and June Downey read an early draft and made many
corrections and suggestions.</p>
</li>
<li>
<p>Steven Zhang found several errors.</p>
</li>
<li>
<p>Andy Pethan and Molly Farison helped debug some of the solutions, and
Molly spotted several typos.</p>
</li>
<li>
<p>Dr. Nikolas Akerblom knows how big a Hyracotherium is.</p>
</li>
<li>
<p>Alex Morrow clarified one of the code examples.</p>
</li>
<li>
<p>Jonathan Street caught an error in the nick of time.</p>
</li>
<li>
<p>Many thanks to Kevin Smith and Tim Arnold for their work on plasTeX,
which I used to convert this book to DocBook.</p>
</li>
<li>
<p>George Caplan sent several suggestions for improving clarity.</p>
</li>
<li>
<p>Julian Ceipek found an error and a number of typos.</p>
</li>
<li>
<p>Stijn Debrouwere, Leo Marihart III, Jonathan Hammler, and Kent Johnson
found errors in the first print edition.</p>
</li>
<li>
<p>Jörg Beyer found typos in the book and made many corrections in the
docstrings of the accompanying code.</p>
</li>
<li>
<p>Tommie Gannert sent a patch file with a number of corrections.</p>
</li>
<li>
<p>Christoph Lendenmann submitted several errata.</p>
</li>
<li>
<p>Michael Kearney sent me many excellent suggestions.</p>
</li>
<li>
<p>Alex Birch made a number of helpful suggestions.</p>
</li>
<li>
<p>Lindsey Vanderlyn, Griffin Tschurwald, and Ben Small read an early
version of this book and found many errors.</p>
</li>
<li>
<p>John Roth, Carol Willing, and Carol Novitsky performed technical
reviews of the book. They found many errors and made many helpful
suggestions.</p>
</li>
<li>
<p>David Palmer sent many helpful suggestions and corrections.</p>
</li>
<li>
<p>Erik Kulyk found many typos.</p>
</li>
<li>
<p>Nir Soffer sent several excellent pull requests for both the book and
the supporting code.</p>
</li>
<li>
<p>GitHub user flothesof sent a number of corrections.</p>
</li>
<li>
<p>Toshiaki Kurokawa, who is working on the Japanese translation of this
book, has sent many corrections and helpful suggestions.</p>
</li>
<li>
<p>Benjamin White suggested more idiomatic Pandas code.</p>
</li>
<li>
<p>Takashi Sato spotted an code error.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Other people who found typos and similar errors are Andrew Heine, Gábor
Lipták, Dan Kearney, Alexander Gryzlov, Martin Veillette, Haitao Ma,
Jeff Pickhardt, Rohit Deshpande, Joanne Pratt, Lucian Ursu, Paul Glezen,
Ting-kuang Lin, Scott Miller, Luigi Patruno.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="intro"><a class="anchor" href="#intro"></a><a class="link" href="#intro">1. Exploratory data analysis</a></h2>
<div class="sectionbody">
<div class="paragraph">
<p>The thesis of this book is that data combined with practical methods can
answer questions and guide decisions under uncertainty.</p>
</div>
<div class="paragraph">
<p>As an example, I present a case study motivated by a question I heard
when my wife and I were expecting our first child: do first babies tend
to arrive late?</p>
</div>
<div class="paragraph">
<p>If you Google this question, you will find plenty of discussion. Some
people claim it’s true, others say it’s a myth, and some people say it’s
the other way around: first babies come early.</p>
</div>
<div class="paragraph">
<p>In many of these discussions, people provide data to support their
claims. I found many examples like these:</p>
</div>
<div class="quoteblock">
<blockquote>
<div class="paragraph">
<p>My two friends that have given birth recently to their first babies,
BOTH went almost 2 weeks overdue before going into labour or being
induced.</p>
</div>
<div class="paragraph">
<p>My first one came 2 weeks late and now I think the second one is going
to come out two weeks early!!</p>
</div>
<div class="paragraph">
<p>I don’t think that can be true because my sister was my mother’s first
and she was early, as with many of my cousins.</p>
</div>
</blockquote>
</div>
<div class="paragraph">
<p>Reports like these are called <strong>anecdotal evidence</strong> because they are
based on data that is unpublished and usually personal. In casual
conversation, there is nothing wrong with anecdotes, so I don’t mean to
pick on the people I quoted.</p>
</div>
<div class="paragraph">
<p>But we might want evidence that is more persuasive and an answer that is
more reliable. By those standards, anecdotal evidence usually fails,
because:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Small number of observations: If pregnancy length is longer for first
babies, the difference is probably small compared to natural variation.
In that case, we might have to compare a large number of pregnancies to
be sure that a difference exists.</p>
</li>
<li>
<p>Selection bias: People who join a discussion of this question might be
interested because their first babies were late. In that case the
process of selecting data would bias the results.</p>
</li>
<li>
<p>Confirmation bias: People who believe the claim might be more likely
to contribute examples that confirm it. People who doubt the claim are
more likely to cite counterexamples.</p>
</li>
<li>
<p>Inaccuracy: Anecdotes are often personal stories, and often
misremembered, misrepresented, repeated inaccurately, etc.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>So how can we do better?</p>
</div>
<div class="sect2">
<h3 id="_a_statistical_approach"><a class="anchor" href="#_a_statistical_approach"></a><a class="link" href="#_a_statistical_approach">1.1. A statistical approach</a></h3>
<div class="paragraph">
<p>To address the limitations of anecdotes, we will use the tools of
statistics, which include:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Data collection: We will use data from a large national survey that
was designed explicitly with the goal of generating statistically valid
inferences about the U.S. population.</p>
</li>
<li>
<p>Descriptive statistics: We will generate statistics that summarize the
data concisely, and evaluate different ways to visualize data.</p>
</li>
<li>
<p>Exploratory data analysis: We will look for patterns, differences, and
other features that address the questions we are interested in. At the
same time we will check for inconsistencies and identify limitations.</p>
</li>
<li>
<p>Estimation: We will use data from a sample to estimate characteristics
of the general population.</p>
</li>
<li>
<p>Hypothesis testing: Where we see apparent effects, like a difference
between two groups, we will evaluate whether the effect might have
happened by chance.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>By performing these steps with care to avoid pitfalls, we can reach
conclusions that are more justifiable and more likely to be correct.</p>
</div>
</div>
<div class="sect2">
<h3 id="nsfg"><a class="anchor" href="#nsfg"></a><a class="link" href="#nsfg">1.2. The National Survey of Family Growth</a></h3>
<div class="paragraph">
<p>Since 1973 the U.S. Centers for Disease Control and Prevention (CDC)
have conducted the National Survey of Family Growth (NSFG), which is
intended to gather &#8220;information on family life, marriage and divorce,
pregnancy, infertility, use of contraception, and men’s and women’s
health. The survey results are used &#8230;&#8203; to plan health services and
health education programs, and to do statistical studies of families,
fertility, and health.&#8221; See <a href="http://cdc.gov/nchs/nsfg.htm" class="bare">http://cdc.gov/nchs/nsfg.htm</a>.</p>
</div>
<div class="paragraph">
<p>We will use data collected by this survey to investigate whether first
babies tend to come late, and other questions. In order to use this data
effectively, we have to understand the design of the study.</p>
</div>
<div class="paragraph">
<p>The NSFG is a <strong>cross-sectional</strong> study, which means that it captures a
snapshot of a group at a point in time. The most common alternative is a
<strong>longitudinal</strong> study, which observes a group repeatedly over a period of
time.</p>
</div>
<div class="paragraph">
<p>The NSFG has been conducted seven times; each deployment is called a
<strong>cycle</strong>. We will use data from Cycle 6, which was conducted from January
2002 to March 2003.</p>
</div>
<div class="paragraph">
<p>The goal of the survey is to draw conclusions about a <strong>population</strong>; the
target population of the NSFG is people in the United States aged 15-44.
Ideally surveys would collect data from every member of the population,
but that’s seldom possible. Instead we collect data from a subset of the
population called a <strong>sample</strong>. The people who participate in a survey are
called <strong>respondents</strong>.</p>
</div>
<div class="paragraph">
<p>In general, cross-sectional studies are meant to be <strong>representative</strong>,
which means that every member of the target population has an equal
chance of participating. That ideal is hard to achieve in practice, but
people who conduct surveys come as close as they can.</p>
</div>
<div class="paragraph">
<p>The NSFG is not representative; instead it is deliberately
<strong>oversampled</strong>. The designers of the study recruited three
groups—Hispanics, African-Americans and teenagers—at rates higher than
their representation in the U.S. population, in order to make sure that
the number of respondents in each of these groups is large enough to
draw valid statistical inferences.</p>
</div>
<div class="paragraph">
<p>Of course, the drawback of oversampling is that it is not as easy to
draw conclusions about the general population based on statistics from
the survey. We will come back to this point later.</p>
</div>
<div class="paragraph">
<p>When working with this kind of data, it is important to be familiar with
the <strong>codebook</strong>, which documents the design of the study, the survey
questions, and the encoding of the responses. The codebook and user’s
guide for the NSFG data are available from
<a href="http://www.cdc.gov/nchs/nsfg/nsfg_cycle6.htm" class="bare">http://www.cdc.gov/nchs/nsfg/nsfg_cycle6.htm</a></p>
</div>
</div>
<div class="sect2">
<h3 id="_importing_the_data"><a class="anchor" href="#_importing_the_data"></a><a class="link" href="#_importing_the_data">1.3. Importing the data</a></h3>
<div class="paragraph">
<p>The code and data used in this book are available from
<a href="https://github.com/AllenDowney/ThinkStats2" class="bare">https://github.com/AllenDowney/ThinkStats2</a>. For information about
downloading and working with this code, see <a href="#code">Using the code</a>.</p>
</div>
<div class="paragraph">
<p>Once you download the code, you should have a file called
<code>ThinkStats2/code/nsfg.py</code>. If you run it, it should read a data file,
run some tests, and print a message like, &#8220;All tests passed.&#8221;</p>
</div>
<div class="paragraph">
<p>Let’s see what it does. Pregnancy data from Cycle 6 of the NSFG is in a
file called <code>2002FemPreg.dat.gz</code>; it is a gzip-compressed data file in
plain text (ASCII), with fixed width columns. Each line in the file is a
<strong>record</strong> that contains data about one pregnancy.</p>
</div>
<div class="paragraph">
<p>The format of the file is documented in <code>2002FemPreg.dct</code>, which is a
Stata dictionary file. Stata is a statistical software system; a
&#8220;dictionary&#8221; in this context is a list of variable names, types, and
indices that identify where in each line to find each variable.</p>
</div>
<div class="paragraph">
<p>For example, here are a few lines from <code>2002FemPreg.dct</code>:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>infile dictionary {
  _column(1)  str12  caseid    %12s  "RESPONDENT ID NUMBER"
  _column(13) byte   pregordr   %2f  "PREGNANCY ORDER (NUMBER)"
}</pre>
</div>
</div>
<div class="paragraph">
<p>This dictionary describes two variables: <code>caseid</code> is a 12-character
string that represents the respondent ID; <code>pregordr</code> is a one-byte
integer that indicates which pregnancy this record describes for this
respondent.</p>
</div>
<div class="paragraph">
<p>The code you downloaded includes <code>thinkstats2.py</code>, which is a Python
module that contains many classes and functions used in this book,
including functions that read the Stata dictionary and the NSFG data
file. Here’s how they are used in <code>nsfg.py</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def ReadFemPreg(dct_file='2002FemPreg.dct',
                dat_file='2002FemPreg.dat.gz'):
    dct = thinkstats2.ReadStataDct(dct_file)
    df = dct.ReadFixedWidth(dat_file, compression='gzip')
    CleanFemPreg(df)
    return df</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>ReadStataDct</code> takes the name of the dictionary file and returns
<code>dct</code>, a <code>FixedWidthVariables</code> object that contains the information
from the dictionary file. <code>dct</code> provides <code>ReadFixedWidth</code>, which
reads the data file.</p>
</div>
</div>
<div class="sect2">
<h3 id="dataframe"><a class="anchor" href="#dataframe"></a><a class="link" href="#dataframe">1.4. DataFrames</a></h3>
<div class="paragraph">
<p>The result of <code>ReadFixedWidth</code> is a DataFrame, which is the
fundamental data structure provided by pandas, which is a Python data
and statistics package we’ll use throughout this book. A DataFrame
contains a row for each record, in this case one row per pregnancy, and
a column for each variable.</p>
</div>
<div class="paragraph">
<p>In addition to the data, a DataFrame also contains the variable names
and their types, and it provides methods for accessing and modifying the
data.</p>
</div>
<div class="paragraph">
<p>If you print <code>df</code> you get a truncated view of the rows and columns,
and the shape of the DataFrame, which is 13593 rows/records and 244
columns/variables.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; import nsfg
&gt;&gt;&gt; df = nsfg.ReadFemPreg()
&gt;&gt;&gt; df
...
[13593 rows x 244 columns]</pre>
</div>
</div>
<div class="paragraph">
<p>The DataFrame is too big to display, so the output is truncated. The
last line reports the number of rows and columns.</p>
</div>
<div class="paragraph">
<p>The attribute <code>columns</code> returns a sequence of column names as Unicode
strings:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; df.columns
Index([u'caseid', u'pregordr', u'howpreg_n', u'howpreg_p', ... ])</pre>
</div>
</div>
<div class="paragraph">
<p>The result is an Index, which is another pandas data structure. We’ll
learn more about Index later, but for now we’ll treat it like a list:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; df.columns[1]
'pregordr'</pre>
</div>
</div>
<div class="paragraph">
<p>To access a column from a DataFrame, you can use the column name as a
key:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; pregordr = df['pregordr']
&gt;&gt;&gt; type(pregordr)
&lt;class 'pandas.core.series.Series'&gt;</pre>
</div>
</div>
<div class="paragraph">
<p>The result is a Series, yet another pandas data structure. A Series is
like a Python list with some additional features. When you print a
Series, you get the indices and the corresponding values:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; pregordr
0     1
1     2
2     1
3     2
...
13590    3
13591    4
13592    5
Name: pregordr, Length: 13593, dtype: int64</pre>
</div>
</div>
<div class="paragraph">
<p>In this example the indices are integers from 0 to 13592, but in general
they can be any sortable type. The elements are also integers, but they
can be any type.</p>
</div>
<div class="paragraph">
<p>The last line includes the variable name, Series length, and data type;
<code>int64</code> is one of the types provided by NumPy. If you run this example
on a 32-bit machine you might see <code>int32</code>.</p>
</div>
<div class="paragraph">
<p>You can access the elements of a Series using integer indices and
slices:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; pregordr[0]
1
&gt;&gt;&gt; pregordr[2:5]
2    1
3    2
4    3
Name: pregordr, dtype: int64</pre>
</div>
</div>
<div class="paragraph">
<p>The result of the index operator is an <code>int64</code>; the result of the
slice is another Series.</p>
</div>
<div class="paragraph">
<p>You can also access the columns of a DataFrame using dot notation:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; pregordr = df.pregordr</pre>
</div>
</div>
<div class="paragraph">
<p>This notation only works if the column name is a valid Python
identifier, so it has to begin with a letter, can’t contain spaces, etc.</p>
</div>
</div>
<div class="sect2">
<h3 id="_variables"><a class="anchor" href="#_variables"></a><a class="link" href="#_variables">1.5. Variables</a></h3>
<div class="paragraph">
<p>We have already seen two variables in the NSFG dataset, <code>caseid</code> and
<code>pregordr</code>, and we have seen that there are 244 variables in total.
For the explorations in this book, I use the following variables:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>caseid</code> is the integer ID of the respondent.</p>
</li>
<li>
<p><code>prglngth</code> is the integer duration of the pregnancy in weeks.</p>
</li>
<li>
<p><code>outcome</code> is an integer code for the outcome of the pregnancy. The
code 1 indicates a live birth.</p>
</li>
<li>
<p><code>pregordr</code> is a pregnancy serial number; for example, the code for a
respondent’s first pregnancy is 1, for the second pregnancy is 2, and so
on.</p>
</li>
<li>
<p><code>birthord</code> is a serial number for live births; the code for a
respondent’s first child is 1, and so on. For outcomes other than live
birth, this field is blank.</p>
</li>
<li>
<p><code>birthwgt_lb</code> and <code>birthwgt_oz</code> contain the pounds and ounces
parts of the birth weight of the baby.</p>
</li>
<li>
<p><code>agepreg</code> is the mother’s age at the end of the pregnancy.</p>
</li>
<li>
<p><code>finalwgt</code> is the statistical weight associated with the respondent.
It is a floating-point value that indicates the number of people in the
U.S. population this respondent represents.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>If you read the codebook carefully, you will see that many of the
variables are <strong>recodes</strong>, which means that they are not part of the <strong>raw
data</strong> collected by the survey; they are calculated using the raw data.</p>
</div>
<div class="paragraph">
<p>For example, <code>prglngth</code> for live births is equal to the raw variable
<code>wksgest</code> (weeks of gestation) if it is available; otherwise it is
estimated using <code>mosgest * 4.33</code> (months of gestation times the
average number of weeks in a month).</p>
</div>
<div class="paragraph">
<p>Recodes are often based on logic that checks the consistency and
accuracy of the data. In general it is a good idea to use recodes when
they are available, unless there is a compelling reason to process the
raw data yourself.</p>
</div>
</div>
<div class="sect2">
<h3 id="cleaning"><a class="anchor" href="#cleaning"></a><a class="link" href="#cleaning">1.6. Transformation</a></h3>
<div class="paragraph">
<p>When you import data like this, you often have to check for errors, deal
with special values, convert data into different formats, and perform
calculations. These operations are called <strong>data cleaning</strong>.</p>
</div>
<div class="paragraph">
<p><code>nsfg.py</code> includes <code>CleanFemPreg</code>, a function that cleans the
variables I am planning to use.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def CleanFemPreg(df):
    df.agepreg /= 100.0

    na_vals = [97, 98, 99]
    df.birthwgt_lb.replace(na_vals, np.nan, inplace=True)
    df.birthwgt_oz.replace(na_vals, np.nan, inplace=True)

    df['totalwgt_lb'] = df.birthwgt_lb + df.birthwgt_oz / 16.0</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>agepreg</code> contains the mother’s age at the end of the pregnancy. In
the data file, <code>agepreg</code> is encoded as an integer number of
centiyears. So the first line divides each element of <code>agepreg</code> by
100, yielding a floating-point value in years.</p>
</div>
<div class="paragraph">
<p><code>birthwgt_lb</code> and <code>birthwgt_oz</code> contain the weight of the baby, in
pounds and ounces, for pregnancies that end in live birth. In addition
it uses several special codes:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>97      NOT ASCERTAINED
98      REFUSED
99      DON'T KNOW</pre>
</div>
</div>
<div class="paragraph">
<p>Special values encoded as numbers are <em>dangerous</em> because if they are
not handled properly, they can generate bogus results, like a 99-pound
baby. The <code>replace</code> method replaces these values with <code>np.nan</code>, a
special floating-point value that represents &#8220;not a number.&#8221; The
<code>inplace</code> flag tells <code>replace</code> to modify the existing Series rather
than create a new one.</p>
</div>
<div class="paragraph">
<p>As part of the IEEE floating-point standard, all mathematical operations
return <code>nan</code> if either argument is <code>nan</code>:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; np.nan / 100.0
nan</pre>
</div>
</div>
<div class="paragraph">
<p>So computations with <code>nan</code> tend to do the right thing, and most pandas
functions handle <code>nan</code> appropriately. But dealing with missing data
will be a recurring issue.</p>
</div>
<div class="paragraph">
<p>The last line of <code>CleanFemPreg</code> creates a new column <code>totalwgt_lb</code>
that combines pounds and ounces into a single quantity, in pounds.</p>
</div>
<div class="paragraph">
<p>One important note: when you add a new column to a DataFrame, you must
use dictionary syntax, like this</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    # CORRECT
    df['totalwgt_lb'] = df.birthwgt_lb + df.birthwgt_oz / 16.0</code></pre>
</div>
</div>
<div class="paragraph">
<p>Not dot notation, like this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    # WRONG!
    df.totalwgt_lb = df.birthwgt_lb + df.birthwgt_oz / 16.0</code></pre>
</div>
</div>
<div class="paragraph">
<p>The version with dot notation adds an attribute to the DataFrame object,
but that attribute is not treated as a new column.</p>
</div>
</div>
<div class="sect2">
<h3 id="_validation"><a class="anchor" href="#_validation"></a><a class="link" href="#_validation">1.7. Validation</a></h3>
<div class="paragraph">
<p>When data is exported from one software environment and imported into
another, errors might be introduced. And when you are getting familiar
with a new dataset, you might interpret data incorrectly or introduce
other misunderstandings. If you take time to validate the data, you can
save time later and avoid errors.</p>
</div>
<div class="paragraph">
<p>One way to validate data is to compute basic statistics and compare them
with published results. For example, the NSFG codebook includes tables
that summarize each variable. Here is the table for <code>outcome</code>, which
encodes the outcome of each pregnancy:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>value   label                  Total
1       LIVE BIRTH              9148
2       INDUCED ABORTION        1862
3       STILLBIRTH               120
4       MISCARRIAGE             1921
5       ECTOPIC PREGNANCY        190
6       CURRENT PREGNANCY        352</pre>
</div>
</div>
<div class="paragraph">
<p>The Series class provides a method, <code>value_counts</code>, that counts the
number of times each value appears. If we select the <code>outcome</code> Series
from the DataFrame, we can use <code>value_counts</code> to compare with the
published data:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; df.outcome.value_counts().sort_index()
1    9148
2    1862
3     120
4    1921
5     190
6     352</pre>
</div>
</div>
<div class="paragraph">
<p>The result of <code>value_counts</code> is a Series; <code>sort_index()</code> sorts the
Series by index, so the values appear in order.</p>
</div>
<div class="paragraph">
<p>Comparing the results with the published table, it looks like the values
in <code>outcome</code> are correct. Similarly, here is the published table for
<code>birthwgt_lb</code></p>
</div>
<div class="literalblock">
<div class="content">
<pre>value   label                  Total
.       INAPPLICABLE            4449
0-5     UNDER 6 POUNDS          1125
6       6 POUNDS                2223
7       7 POUNDS                3049
8       8 POUNDS                1889
9-95    9 POUNDS OR MORE         799</pre>
</div>
</div>
<div class="paragraph">
<p>And here are the value counts:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; df.birthwgt_lb.value_counts(sort=False)
0        8
1       40
2       53
3       98
4      229
5      697
6     2223
7     3049
8     1889
9      623
10     132
11      26
12      10
13       3
14       3
15       1
51       1</pre>
</div>
</div>
<div class="paragraph">
<p>The counts for 6, 7, and 8 pounds check out, and if you add up the
counts for 0-5 and 9-95, they check out, too. But if you look more
closely, you will notice one value that has to be an error, a 51 pound
baby!</p>
</div>
<div class="paragraph">
<p>To deal with this error, I added a line to <code>CleanFemPreg</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">df.loc[df.birthwgt_lb &gt; 20, 'birthwgt_lb'] = np.nan</code></pre>
</div>
</div>
<div class="paragraph">
<p>This statement replaces invalid values with <code>np.nan</code>. The attribute
<code>loc</code> provides several ways to select rows and columns from a
DataFrame. In this example, the first expression in brackets is the row
indexer; the second expression selects the column.</p>
</div>
<div class="paragraph">
<p>The expression <code>df.birthwgt_lb &gt; 20</code> yields a Series of type <code>bool</code>,
where True indicates that the condition is true. When a boolean Series
is used as an index, it selects only the elements that satisfy the
condition.</p>
</div>
</div>
<div class="sect2">
<h3 id="_interpretation"><a class="anchor" href="#_interpretation"></a><a class="link" href="#_interpretation">1.8. Interpretation</a></h3>
<div class="paragraph">
<p>To work with data effectively, you have to think on two levels at the
same time: the level of statistics and the level of context.</p>
</div>
<div class="paragraph">
<p>As an example, let’s look at the sequence of outcomes for a few
respondents. Because of the way the data files are organized, we have to
do some processing to collect the pregnancy data for each respondent.
Here’s a function that does that:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def MakePregMap(df):
    d = defaultdict(list)
    for index, caseid in df.caseid.iteritems():
        d[caseid].append(index)
    return d</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>df</code> is the DataFrame with pregnancy data. The <code>iteritems</code> method
enumerates the index (row number) and <code>caseid</code> for each pregnancy.</p>
</div>
<div class="paragraph">
<p><code>d</code> is a dictionary that maps from each case ID to a list of indices.
If you are not familiar with <code>defaultdict</code>, it is in the Python
<code>collections</code> module. Using <code>d</code>, we can look up a respondent and get
the indices of that respondent’s pregnancies.</p>
</div>
<div class="paragraph">
<p>This example looks up one respondent and prints a list of outcomes for
her pregnancies:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; caseid = 10229
&gt;&gt;&gt; preg_map = nsfg.MakePregMap(df)
&gt;&gt;&gt; indices = preg_map[caseid]
&gt;&gt;&gt; df.outcome[indices].values
[4 4 4 4 4 4 1]</pre>
</div>
</div>
<div class="paragraph">
<p><code>indices</code> is the list of indices for pregnancies corresponding to
respondent <code>10229</code>.</p>
</div>
<div class="paragraph">
<p>Using this list as an index into <code>df.outcome</code> selects the indicated
rows and yields a Series. Instead of printing the whole Series, I
selected the <code>values</code> attribute, which is a NumPy array.</p>
</div>
<div class="paragraph">
<p>The outcome code <code>1</code> indicates a live birth. Code <code>4</code> indicates a
miscarriage; that is, a pregnancy that ended spontaneously, usually with
no known medical cause.</p>
</div>
<div class="paragraph">
<p>Statistically this respondent is not unusual. Miscarriages are common
and there are other respondents who reported as many or more.</p>
</div>
<div class="paragraph">
<p>But remembering the context, this data tells the story of a woman who
was pregnant six times, each time ending in miscarriage. Her seventh and
most recent pregnancy ended in a live birth. If we consider this data
with empathy, it is natural to be moved by the story it tells.</p>
</div>
<div class="paragraph">
<p>Each record in the NSFG dataset represents a person who provided honest
answers to many personal and difficult questions. We can use this data
to answer statistical questions about family life, reproduction, and
health. At the same time, we have an obligation to consider the people
represented by the data, and to afford them respect and gratitude.</p>
</div>
</div>
<div class="sect2">
<h3 id="_exercises"><a class="anchor" href="#_exercises"></a><a class="link" href="#_exercises">1.9. Exercises</a></h3>
<div class="paragraph">
<p></p>
</div>
<div class="paragraph">
<p><strong>Exercise 1.1</strong></p>
</div>
<div class="paragraph">
<p>In the repository you downloaded, you should find a file named
<code>chap01ex.ipynb</code>, which is an IPython notebook. You can launch IPython
notebook from the command line like this:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>$ ipython notebook &amp;</pre>
</div>
</div>
<div class="paragraph">
<p>If IPython is installed, it should launch a server that runs in the
background and open a browser to view the notebook. If you are not
familiar with IPython, I suggest you start at
<a href="http://ipython.org/ipython-doc/stable/notebook/notebook.html" class="bare">http://ipython.org/ipython-doc/stable/notebook/notebook.html</a>.</p>
</div>
<div class="paragraph">
<p>To launch the IPython notebook server, run:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>$ ipython notebook &amp;</pre>
</div>
</div>
<div class="paragraph">
<p>It should open a new browser window, but if not, the startup message
provides a URL you can load in a browser, usually <a href="http://localhost:8888" class="bare">http://localhost:8888</a>.
The new window should list the notebooks in the repository.</p>
</div>
<div class="paragraph">
<p>Open <code>chap01ex.ipynb</code>. Some cells are already filled in, and you
should execute them. Other cells give you instructions for exercises you
should try.</p>
</div>
<div class="paragraph">
<p>A solution to this exercise is in <code>chap01soln.ipynb</code></p>
</div>
<div class="paragraph">
<p><strong>Exercise 1.2</strong></p>
</div>
<div class="paragraph">
<p>In the repository you downloaded, you should find a file named
<code>chap01ex.py</code>; using this file as a starting place, write a function
that reads the respondent file, <code>2002FemResp.dat.gz</code>.</p>
</div>
<div class="paragraph">
<p>The variable <code>pregnum</code> is a recode that indicates how many times each
respondent has been pregnant. Print the value counts for this variable
and compare them to the published results in the NSFG codebook.</p>
</div>
<div class="paragraph">
<p>You can also cross-validate the respondent and pregnancy files by
comparing <code>pregnum</code> for each respondent with the number of records in
the pregnancy file.</p>
</div>
<div class="paragraph">
<p>You can use <code>nsfg.MakePregMap</code> to make a dictionary that maps from
each <code>caseid</code> to a list of indices into the pregnancy DataFrame.</p>
</div>
<div class="paragraph">
<p>A solution to this exercise is in <code>chap01soln.py</code></p>
</div>
<div class="paragraph">
<p><strong>Exercise 1.3</strong></p>
</div>
<div class="paragraph">
<p>The best way to learn about statistics is to work on a project you are
interested in. Is there a question like, &#8220;Do first babies arrive
late,&#8221; that you want to investigate?</p>
</div>
<div class="paragraph">
<p>Think about questions you find personally interesting, or items of
conventional wisdom, or controversial topics, or questions that have
political consequences, and see if you can formulate a question that
lends itself to statistical inquiry.</p>
</div>
<div class="paragraph">
<p>Look for data to help you address the question. Governments are good
sources because data from public research is often freely available.
Good places to start include <a href="http://www.data.gov/" class="bare">http://www.data.gov/</a>, and
<a href="http://www.science.gov/" class="bare">http://www.science.gov/</a>, and in the United Kingdom, <a href="http://data.gov.uk/" class="bare">http://data.gov.uk/</a>.</p>
</div>
<div class="paragraph">
<p>Two of my favorite data sets are the General Social Survey at
<a href="http://www3.norc.org/gss+website/" class="bare">http://www3.norc.org/gss+website/</a>, and the European Social Survey at
<a href="http://www.europeansocialsurvey.org/" class="bare">http://www.europeansocialsurvey.org/</a>.</p>
</div>
<div class="paragraph">
<p>If it seems like someone has already answered your question, look
closely to see whether the answer is justified. There might be flaws in
the data or the analysis that make the conclusion unreliable. In that
case you could perform a different analysis of the same data, or look
for a better source of data.</p>
</div>
<div class="paragraph">
<p>If you find a published paper that addresses your question, you should
be able to get the raw data. Many authors make their data available on
the web, but for sensitive data you might have to write to the authors,
provide information about how you plan to use the data, or agree to
certain terms of use. Be persistent!</p>
</div>
</div>
<div class="sect2">
<h3 id="_glossary"><a class="anchor" href="#_glossary"></a><a class="link" href="#_glossary">1.10. Glossary</a></h3>
<div class="ulist">
<ul>
<li>
<p><strong>anecdotal evidence</strong>: Evidence, often personal, that is collected
casually rather than by a well-designed study.</p>
</li>
<li>
<p><strong>population</strong>: A group we are interested in studying. &#8220;Population&#8221;
often refers to a group of people, but the term is used for other
subjects, too.</p>
</li>
<li>
<p><strong>cross-sectional study</strong>: A study that collects data about a population
at a particular point in time.</p>
</li>
<li>
<p><strong>cycle</strong>: In a repeated cross-sectional study, each repetition of the
study is called a cycle.</p>
</li>
<li>
<p><strong>longitudinal study</strong>: A study that follows a population over time,
collecting data from the same group repeatedly.</p>
</li>
<li>
<p><strong>record</strong>: In a dataset, a collection of information about a single
person or other subject.</p>
</li>
<li>
<p><strong>respondent</strong>: A person who responds to a survey.</p>
</li>
<li>
<p><strong>sample</strong>: The subset of a population used to collect data.</p>
</li>
<li>
<p><strong>representative</strong>: A sample is representative if every member of the
population has the same chance of being in the sample.</p>
</li>
<li>
<p><strong>oversampling</strong>: The technique of increasing the representation of a
sub-population in order to avoid errors due to small sample sizes.</p>
</li>
<li>
<p><strong>raw data</strong>: Values collected and recorded with little or no checking,
calculation or interpretation.</p>
</li>
<li>
<p><strong>recode</strong>: A value that is generated by calculation and other logic
applied to raw data.</p>
</li>
<li>
<p><strong>data cleaning</strong>: Processes that include validating data, identifying
errors, translating between data types and representations, etc.</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="descriptive"><a class="anchor" href="#descriptive"></a><a class="link" href="#descriptive">2. Distributions</a></h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_histograms"><a class="anchor" href="#_histograms"></a><a class="link" href="#_histograms">2.1. Histograms</a></h3>
<div class="paragraph">
<p>One of the best ways to describe a variable is to report the values that
appear in the dataset and how many times each value appears. This
description is called the <strong>distribution</strong> of the variable.</p>
</div>
<div class="paragraph">
<p>The most common representation of a distribution is a <strong>histogram</strong>, which
is a graph that shows the <strong>frequency</strong> of each value. In this context,
&#8220;frequency&#8221; means the number of times the value appears.</p>
</div>
<div class="paragraph">
<p>In Python, an efficient way to compute frequencies is with a dictionary.
Given a sequence of values, <code>t</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">hist = {}
for x in t:
    hist[x] = hist.get(x, 0) + 1</code></pre>
</div>
</div>
<div class="paragraph">
<p>The result is a dictionary that maps from values to frequencies.
Alternatively, you could use the <code>Counter</code> class defined in the
<code>collections</code> module:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">from collections import Counter
counter = Counter(t)</code></pre>
</div>
</div>
<div class="paragraph">
<p>The result is a <code>Counter</code> object, which is a subclass of dictionary.</p>
</div>
<div class="paragraph">
<p>Another option is to use the pandas method <code>value_counts</code>, which we
saw in the previous chapter. But for this book I created a class, Hist,
that represents histograms and provides the methods that operate on
them.</p>
</div>
</div>
<div class="sect2">
<h3 id="_representing_histograms"><a class="anchor" href="#_representing_histograms"></a><a class="link" href="#_representing_histograms">2.2. Representing histograms</a></h3>
<div class="paragraph">
<p>The Hist constructor can take a sequence, dictionary, pandas Series, or
another Hist. You can instantiate a Hist object like this:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; import thinkstats2
&gt;&gt;&gt; hist = thinkstats2.Hist([1, 2, 2, 3, 5])
&gt;&gt;&gt; hist
Hist({1: 1, 2: 2, 3: 1, 5: 1})</pre>
</div>
</div>
<div class="paragraph">
<p>Hist objects provide <code>Freq</code>, which takes a value and returns its
frequency:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; hist.Freq(2)
2</pre>
</div>
</div>
<div class="paragraph">
<p>The bracket operator does the same thing:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; hist[2]
2</pre>
</div>
</div>
<div class="paragraph">
<p>If you look up a value that has never appeared, the frequency is 0.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; hist.Freq(4)
0</pre>
</div>
</div>
<div class="paragraph">
<p><code>Values</code> returns an unsorted list of the values in the Hist:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; hist.Values()
[1, 5, 3, 2]</pre>
</div>
</div>
<div class="paragraph">
<p>To loop through the values in order, you can use the built-in function
<code>sorted</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">for val in sorted(hist.Values()):
    print(val, hist.Freq(val))</code></pre>
</div>
</div>
<div class="paragraph">
<p>Or you can use <code>Items</code> to iterate through value-frequency pairs:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">for val, freq in hist.Items():
     print(val, freq)</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_plotting_histograms"><a class="anchor" href="#_plotting_histograms"></a><a class="link" href="#_plotting_histograms">2.3. Plotting histograms</a></h3>
<div id="first_wgt_lb_hist" class="imageblock">
<div class="content">
<img src="figs/first_wgt_lb_hist.png" alt="first wgt lb hist" height="240">
</div>
<div class="title">Figure 1. Histogram of the pound part of birth weight.</div>
</div>
<div class="paragraph">
<p>For this book I wrote a module called <code>thinkplot.py</code> that provides
functions for plotting Hists and other objects defined in
<code>thinkstats2.py</code>. It is based on <code>pyplot</code>, which is part of the
<code>matplotlib</code> package. See <a href="#code">Using the code</a> for information
about installing <code>matplotlib</code>.</p>
</div>
<div class="paragraph">
<p>To plot <code>hist</code> with <code>thinkplot</code>, try this:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; import thinkplot
&gt;&gt;&gt; thinkplot.Hist(hist)
&gt;&gt;&gt; thinkplot.Show(xlabel='value', ylabel='frequency')</pre>
</div>
</div>
<div class="paragraph">
<p>You can read the documentation for <code>thinkplot</code> at
<a href="http://greenteapress.com/thinkstats2/thinkplot.html" class="bare">http://greenteapress.com/thinkstats2/thinkplot.html</a>.</p>
</div>
<div id="first_wgt_oz_hist" class="imageblock">
<div class="content">
<img src="figs/first_wgt_oz_hist.png" alt="first wgt oz hist" height="240">
</div>
<div class="title">Figure 2. Histogram of the ounce part of birth weight.</div>
</div>
</div>
<div class="sect2">
<h3 id="_nsfg_variables"><a class="anchor" href="#_nsfg_variables"></a><a class="link" href="#_nsfg_variables">2.4. NSFG variables</a></h3>
<div class="paragraph">
<p>Now let’s get back to the data from the NSFG. The code in this chapter
is in <code>first.py</code>. For information about downloading and working with
this code, see <a href="#code">Using the code</a>.</p>
</div>
<div class="paragraph">
<p>When you start working with a new dataset, I suggest you explore the
variables you are planning to use one at a time, and a good way to start
is by looking at histograms.</p>
</div>
<div class="paragraph">
<p>In <a href="#cleaning">Section 1.6</a> we transformed <code>agepreg</code> from
centiyears to years, and combined <code>birthwgt_lb</code> and <code>birthwgt_oz</code>
into a single quantity, <code>totalwgt_lb</code>. In this section I use these
variables to demonstrate some features of histograms.</p>
</div>
<div id="first_agepreg_hist" class="imageblock">
<div class="content">
<img src="figs/first_agepreg_hist.png" alt="first agepreg hist" height="240">
</div>
<div class="title">Figure 3. Histogram of mother’s age at end of pregnancy.</div>
</div>
<div class="paragraph">
<p>I’ll start by reading the data and selecting records for live births:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    preg = nsfg.ReadFemPreg()
    live = preg[preg.outcome == 1]</code></pre>
</div>
</div>
<div class="paragraph">
<p>The expression in brackets is a boolean Series that selects rows from
the DataFrame and returns a new DataFrame. Next I generate and plot the
histogram of <code>birthwgt_lb</code> for live births.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    hist = thinkstats2.Hist(live.birthwgt_lb, label='birthwgt_lb')
    thinkplot.Hist(hist)
    thinkplot.Show(xlabel='pounds', ylabel='frequency')</code></pre>
</div>
</div>
<div class="paragraph">
<p>When the argument passed to Hist is a pandas Series, any <code>nan</code> values
are dropped. <code>label</code> is a string that appears in the legend when the
Hist is plotted.</p>
</div>
<div id="first_prglngth_hist" class="imageblock">
<div class="content">
<img src="figs/first_prglngth_hist.png" alt="first prglngth hist" height="240">
</div>
<div class="title">Figure 4. Histogram of pregnancy length in weeks.</div>
</div>
<div class="paragraph">
<p><a href="#first_wgt_lb_hist">Figure 1</a> shows the result.
The most common value, called the <strong>mode</strong>, is 7 pounds. The distribution
is approximately bell-shaped, which is the shape of the <strong>normal</strong>
distribution, also called a <strong>Gaussian</strong> distribution. But unlike a true
normal distribution, this distribution is asymmetric; it has a <strong>tail</strong>
that extends farther to the left than to the right.</p>
</div>
<div class="paragraph">
<p><a href="#first_wgt_oz_hist">Figure 2</a> shows the histogram
of <code>birthwgt_oz</code>, which is the ounces part of birth weight. In theory
we expect this distribution to be <strong>uniform</strong>; that is, all values should
have the same frequency. In fact, 0 is more common than the other
values, and 1 and 15 are less common, probably because respondents round
off birth weights that are close to an integer value.</p>
</div>
<div class="paragraph">
<p><a href="#first_agepreg_hist">Figure 3</a> shows the
histogram of <code>agepreg</code>, the mother’s age at the end of pregnancy. The
mode is 21 years. The distribution is very roughly bell-shaped, but in
this case the tail extends farther to the right than left; most mothers
are in their 20s, fewer in their 30s.</p>
</div>
<div class="paragraph">
<p><a href="#first_prglngth_hist">Figure 4</a> shows the
histogram of <code>prglngth</code>, the length of the pregnancy in weeks. By far
the most common value is 39 weeks. The left tail is longer than the
right; early babies are common, but pregnancies seldom go past 43 weeks,
and doctors often intervene if they do.</p>
</div>
</div>
<div class="sect2">
<h3 id="_outliers"><a class="anchor" href="#_outliers"></a><a class="link" href="#_outliers">2.5. Outliers</a></h3>
<div class="paragraph">
<p>Looking at histograms, it is easy to identify the most common values and
the shape of the distribution, but rare values are not always visible.</p>
</div>
<div class="paragraph">
<p>Before going on, it is a good idea to check for <strong>outliers</strong>, which are
extreme values that might be errors in measurement and recording, or
might be accurate reports of rare events.</p>
</div>
<div class="paragraph">
<p>Hist provides methods <code>Largest</code> and <code>Smallest</code>, which take an
integer <code>n</code> and return the <code>n</code> largest or smallest values from the
histogram:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    for weeks, freq in hist.Smallest(10):
        print(weeks, freq)</code></pre>
</div>
</div>
<div class="paragraph">
<p>In the list of pregnancy lengths for live births, the 10 lowest values
are <code>[0, 4, 9, 13, 17, 18, 19, 20, 21, 22]</code>. Values below 10 weeks are
certainly errors; the most likely explanation is that the outcome was
not coded correctly. Values higher than 30 weeks are probably
legitimate. Between 10 and 30 weeks, it is hard to be sure; some values
are probably errors, but some represent premature babies.</p>
</div>
<div class="paragraph">
<p>On the other end of the range, the highest values are:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>weeks  count
43     148
44     46
45     10
46     1
47     1
48     7
50     2</pre>
</div>
</div>
<div class="paragraph">
<p>Most doctors recommend induced labor if a pregnancy exceeds 42 weeks, so
some of the longer values are surprising. In particular, 50 weeks seems
medically unlikely.</p>
</div>
<div class="paragraph">
<p>The best way to handle outliers depends on &#8220;domain knowledge&#8221;; that
is, information about where the data come from and what they mean. And
it depends on what analysis you are planning to perform.</p>
</div>
<div class="paragraph">
<p>In this example, the motivating question is whether first babies tend to
be early (or late). When people ask this question, they are usually
interested in full-term pregnancies, so for this analysis I will focus
on pregnancies longer than 27 weeks.</p>
</div>
</div>
<div class="sect2">
<h3 id="_first_babies"><a class="anchor" href="#_first_babies"></a><a class="link" href="#_first_babies">2.6. First babies</a></h3>
<div class="paragraph">
<p>Now we can compare the distribution of pregnancy lengths for first
babies and others. I divided the DataFrame of live births using
<code>birthord</code>, and computed their histograms:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    firsts = live[live.birthord == 1]
    others = live[live.birthord != 1]

    first_hist = thinkstats2.Hist(firsts.prglngth, label='first')
    other_hist = thinkstats2.Hist(others.prglngth, label='other')</code></pre>
</div>
</div>
<div class="paragraph">
<p>Then I plotted their histograms on the same axis:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    width = 0.45
    thinkplot.PrePlot(2)
    thinkplot.Hist(first_hist, align='right', width=width)
    thinkplot.Hist(other_hist, align='left', width=width)
    thinkplot.Show(xlabel='weeks', ylabel='frequency',
                   xlim=[27, 46])</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>thinkplot.PrePlot</code> takes the number of histograms we are planning to
plot; it uses this information to choose an appropriate collection of
colors.</p>
</div>
<div id="first_nsfg_hist" class="imageblock">
<div class="content">
<img src="figs/first_nsfg_hist.png" alt="first nsfg hist" height="240">
</div>
<div class="title">Figure 5. Histogram of pregnancy lengths.</div>
</div>
<div class="paragraph">
<p><code>thinkplot.Hist</code> normally uses <code>align='center'</code> so that each bar is
centered over its value. For this figure, I use <code>align='right'</code> and
<code>align='left'</code> to place corresponding bars on either side of the
value.</p>
</div>
<div class="paragraph">
<p>With <code>width=0.45</code>, the total width of the two bars is 0.9, leaving
some space between each pair.</p>
</div>
<div class="paragraph">
<p>Finally, I adjust the axis to show only data between 27 and 46 weeks.
<a href="#first_nsfg_hist">Figure 5</a> shows the result.</p>
</div>
<div class="paragraph">
<p>Histograms are useful because they make the most frequent values
immediately apparent. But they are not the best choice for comparing two
distributions. In this example, there are fewer &#8220;first babies&#8221; than
&#8220;others,&#8221; so some of the apparent differences in the histograms are
due to sample sizes. In the next chapter we address this problem using
probability mass functions.</p>
</div>
</div>
<div class="sect2">
<h3 id="mean"><a class="anchor" href="#mean"></a><a class="link" href="#mean">2.7. Summarizing distributions</a></h3>
<div class="paragraph">
<p>A histogram is a complete description of the distribution of a sample;
that is, given a histogram, we could reconstruct the values in the
sample (although not their order).</p>
</div>
<div class="paragraph">
<p>If the details of the distribution are important, it might be necessary
to present a histogram. But often we want to summarize the distribution
with a few descriptive statistics.</p>
</div>
<div class="paragraph">
<p>Some of the characteristics we might want to report are:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>central tendency: Do the values tend to cluster around a particular
point?</p>
</li>
<li>
<p>modes: Is there more than one cluster?</p>
</li>
<li>
<p>spread: How much variability is there in the values?</p>
</li>
<li>
<p>tails: How quickly do the probabilities drop off as we move away from
the modes?</p>
</li>
<li>
<p>outliers: Are there extreme values far from the modes?</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Statistics designed to answer these questions are called <strong>summary
statistics</strong>. By far the most common summary statistic is the <strong>mean</strong>,
which is meant to describe the central tendency of the distribution.</p>
</div>
<div class="paragraph">
<p>If you have a sample of <code>n</code> values, \(x_i\), the mean,
\(\bar{x}\), is the sum of the values divided by the number of
values; in other words</p>
</div>
<div class="stemblock">
<div class="content">
\[\bar{x}= \frac{1}{n} \sum_i x_i\]
</div>
</div>
<div class="paragraph">
<p>The words &#8220;mean&#8221; and &#8220;average&#8221; are sometimes used interchangeably,
but I make this distinction:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The &#8220;mean&#8221; of a sample is the summary statistic computed with the
previous formula.</p>
</li>
<li>
<p>An &#8220;average&#8221; is one of several summary statistics you might choose
to describe a central tendency.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Sometimes the mean is a good description of a set of values. For
example, apples are all pretty much the same size (at least the ones
sold in supermarkets). So if I buy 6 apples and the total weight is 3
pounds, it would be a reasonable summary to say they are about a half
pound each.</p>
</div>
<div class="paragraph">
<p>But pumpkins are more diverse. Suppose I grow several varieties in my
garden, and one day I harvest three decorative pumpkins that are 1 pound
each, two pie pumpkins that are 3 pounds each, and one Atlantic
Giant pumpkin that weighs 591 pounds. The mean of this sample is 100
pounds, but if I told you &#8220;The average pumpkin in my garden is 100
pounds,&#8221; that would be misleading. In this example, there is no
meaningful average because there is no typical pumpkin.</p>
</div>
</div>
<div class="sect2">
<h3 id="_variance"><a class="anchor" href="#_variance"></a><a class="link" href="#_variance">2.8. Variance</a></h3>
<div class="paragraph">
<p>If there is no single number that summarizes pumpkin weights, we can do
a little better with two numbers: mean and <strong>variance</strong>.</p>
</div>
<div class="paragraph">
<p>Variance is a summary statistic intended to describe the variability or
spread of a distribution. The variance of a set of values is</p>
</div>
<div class="stemblock">
<div class="content">
\[S^2 = \frac{1}{n} \sum_i (x_i - \bar{x})^2\]
</div>
</div>
<div class="paragraph">
<p>The term \(x_i - \bar{x}\) is called the &#8220;deviation from the
mean,&#8221; so variance is the mean squared deviation. The square root of
variance, \(S\), is the <strong>standard deviation</strong>.</p>
</div>
<div class="paragraph">
<p>If you have prior experience, you might have seen a formula for variance
with \(n-1\) in the denominator, rather than <code>n</code>. This
statistic is used to estimate the variance in a population using a
sample. We will come back to this in <a href="#estimation">Chapter 8</a>.</p>
</div>
<div class="paragraph">
<p>Pandas data structures provides methods to compute mean, variance and
standard deviation:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    mean = live.prglngth.mean()
    var = live.prglngth.var()
    std = live.prglngth.std()</code></pre>
</div>
</div>
<div class="paragraph">
<p>For all live births, the mean pregnancy length is 38.6 weeks, the
standard deviation is 2.7 weeks, which means we should expect deviations
of 2-3 weeks to be common.</p>
</div>
<div class="paragraph">
<p>Variance of pregnancy length is 7.3, which is hard to interpret,
especially since the units are weeks\(^2\), or &#8220;square weeks.&#8221;
Variance is useful in some calculations, but it is not a good summary
statistic.</p>
</div>
</div>
<div class="sect2">
<h3 id="_effect_size"><a class="anchor" href="#_effect_size"></a><a class="link" href="#_effect_size">2.9. Effect size</a></h3>
<div class="paragraph">
<p>An <strong>effect size</strong> is a summary statistic intended to describe (wait for
it) the size of an effect. For example, to describe the difference
between two groups, one obvious choice is the difference in the means.</p>
</div>
<div class="paragraph">
<p>Mean pregnancy length for first babies is 38.601; for other babies it is
38.523. The difference is 0.078 weeks, which works out to 13 hours. As a
fraction of the typical pregnancy length, this difference is about 0.2%.</p>
</div>
<div class="paragraph">
<p>If we assume this estimate is accurate, such a difference would have no
practical consequences. In fact, without observing a large number of
pregnancies, it is unlikely that anyone would notice this difference at
all.</p>
</div>
<div class="paragraph">
<p>Another way to convey the size of the effect is to compare the
difference between groups to the variability within groups. Cohen’s
\(d\) is a statistic intended to do that; it is defined</p>
</div>
<div class="stemblock">
<div class="content">
\[d = \frac{\bar{x_1} - \bar{x_2}}{s}\]
</div>
</div>
<div class="paragraph">
<p>where \(\bar{x_1}\) and \(\bar{x_2}\) are the means of
the groups and \(s\) is the &#8220;pooled standard deviation&#8221;.
Here’s the Python code that computes Cohen’s \(d\):</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def CohenEffectSize(group1, group2):
    diff = group1.mean() - group2.mean()

    var1 = group1.var()
    var2 = group2.var()
    n1, n2 = len(group1), len(group2)

    pooled_var = (n1 * var1 + n2 * var2) / (n1 + n2)
    d = diff / math.sqrt(pooled_var)
    return d</code></pre>
</div>
</div>
<div class="paragraph">
<p>In this example, the difference in means is 0.029 standard deviations,
which is small. To put that in perspective, the difference in height
between men and women is about 1.7 standard deviations (see
<a href="https://en.wikipedia.org/wiki/Effect_size" class="bare">https://en.wikipedia.org/wiki/Effect_size</a>).</p>
</div>
</div>
<div class="sect2">
<h3 id="_reporting_results"><a class="anchor" href="#_reporting_results"></a><a class="link" href="#_reporting_results">2.10. Reporting results</a></h3>
<div class="paragraph">
<p>We have seen several ways to describe the difference in pregnancy length
(if there is one) between first babies and others. How should we report
these results?</p>
</div>
<div class="paragraph">
<p>The answer depends on who is asking the question. A scientist might be
interested in any (real) effect, no matter how small. A doctor might
only care about effects that are <strong>clinically significant</strong>; that is,
differences that affect treatment decisions. A pregnant woman might be
interested in results that are relevant to her, like the probability of
delivering early or late.</p>
</div>
<div class="paragraph">
<p>How you report results also depends on your goals. If you are trying to
demonstrate the importance of an effect, you might choose summary
statistics that emphasize differences. If you are trying to reassure a
patient, you might choose statistics that put the differences in
context.</p>
</div>
<div class="paragraph">
<p>Of course your decisions should also be guided by professional ethics.
It’s ok to be persuasive; you <em>should</em> design statistical reports and
visualizations that tell a story clearly. But you should also do your
best to make your reports honest, and to acknowledge uncertainty and
limitations.</p>
</div>
</div>
<div class="sect2">
<h3 id="_exercises_2"><a class="anchor" href="#_exercises_2"></a><a class="link" href="#_exercises_2">2.11. Exercises</a></h3>
<div class="paragraph">
<p></p>
</div>
<div class="paragraph">
<p><strong>Exercise 2.1</strong></p>
</div>
<div class="paragraph">
<p>Based on the results in this chapter, suppose you were asked to
summarize what you learned about whether first babies arrive late.</p>
</div>
<div class="paragraph">
<p>Which summary statistics would you use if you wanted to get a story on
the evening news? Which ones would you use if you wanted to reassure an
anxious patient?</p>
</div>
<div class="paragraph">
<p>Finally, imagine that you are Cecil Adams, author of <em>The Straight Dope</em>
(<a href="http://straightdope.com" class="bare">http://straightdope.com</a>), and your job is to answer the question, &#8220;Do
first babies arrive late?&#8221; Write a paragraph that uses the results in
this chapter to answer the question clearly, precisely, and honestly.</p>
</div>
<div class="paragraph">
<p><strong>Exercise 2.2</strong></p>
</div>
<div class="paragraph">
<p>In the repository you downloaded, you should find a file named
<code>chap02ex.ipynb</code>; open it. Some cells are already filled in, and you
should execute them. Other cells give you instructions for exercises.
Follow the instructions and fill in the answers.</p>
</div>
<div class="paragraph">
<p>A solution to this exercise is in <code>chap02soln.ipynb</code></p>
</div>
<div class="paragraph">
<p>In the repository you downloaded, you should find a file named
<code>chap02ex.py</code>; you can use this file as a starting place for the
following exercises. My solution is in <code>chap02soln.py</code>.</p>
</div>
<div class="paragraph">
<p><strong>Exercise 2.3</strong></p>
</div>
<div class="paragraph">
<p>The mode of a distribution is the most frequent value; see
<a href="http://wikipedia.org/wiki/Mode_(statistics" class="bare">http://wikipedia.org/wiki/Mode_(statistics</a>). Write a function called
<code>Mode</code> that takes a Hist and returns the most frequent value.</p>
</div>
<div class="paragraph">
<p>As a more challenging exercise, write a function called <code>AllModes</code>
that returns a list of value-frequency pairs in descending order of
frequency.</p>
</div>
<div class="paragraph">
<p><strong>Exercise 2.4</strong></p>
</div>
<div class="paragraph">
<p>Using the variable <code>totalwgt_lb</code>, investigate whether first babies are
lighter or heavier than others. Compute Cohen’s \(d\) to
quantify the difference between the groups. How does it compare to the
difference in pregnancy length?</p>
</div>
</div>
<div class="sect2">
<h3 id="_glossary_2"><a class="anchor" href="#_glossary_2"></a><a class="link" href="#_glossary_2">2.12. Glossary</a></h3>
<div class="ulist">
<ul>
<li>
<p><strong>distribution</strong>: The values that appear in a sample and the frequency
of each.</p>
</li>
<li>
<p><strong>histogram</strong>: A mapping from values to frequencies, or a graph that
shows this mapping.</p>
</li>
<li>
<p><strong>frequency</strong>: The number of times a value appears in a sample.</p>
</li>
<li>
<p><strong>mode</strong>: The most frequent value in a sample, or one of the most
frequent values.</p>
</li>
<li>
<p><strong>normal distribution</strong>: An idealization of a bell-shaped distribution;
also known as a Gaussian distribution.</p>
</li>
<li>
<p><strong>uniform distribution</strong>: A distribution in which all values have the
same frequency.</p>
</li>
<li>
<p><strong>tail</strong>: The part of a distribution at the high and low extremes.</p>
</li>
<li>
<p><strong>central tendency</strong>: A characteristic of a sample or population;
intuitively, it is an average or typical value.</p>
</li>
<li>
<p><strong>outlier</strong>: A value far from the central tendency.</p>
</li>
<li>
<p><strong>spread</strong>: A measure of how spread out the values in a distribution
are.</p>
</li>
<li>
<p><strong>summary statistic</strong>: A statistic that quantifies some aspect of a
distribution, like central tendency or spread.</p>
</li>
<li>
<p><strong>variance</strong>: A summary statistic often used to quantify spread.</p>
</li>
<li>
<p><strong>standard deviation</strong>: The square root of variance, also used as a
measure of spread.</p>
</li>
<li>
<p><strong>effect size</strong>: A summary statistic intended to quantify the size of an
effect like a difference between groups.</p>
</li>
<li>
<p><strong>clinically significant</strong>: A result, like a difference between groups,
that is relevant in practice.</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_probability_mass_functions"><a class="anchor" href="#_probability_mass_functions"></a><a class="link" href="#_probability_mass_functions">3. Probability mass functions</a></h2>
<div class="sectionbody">
<div class="paragraph">
<p>The code for this chapter is in <code>probability.py</code>. For information
about downloading and working with this code, see
<a href="#code">Using the code</a>.</p>
</div>
<div class="sect2">
<h3 id="_pmfs"><a class="anchor" href="#_pmfs"></a><a class="link" href="#_pmfs">3.1. Pmfs</a></h3>
<div class="paragraph">
<p>Another way to represent a distribution is a <strong>probability mass function</strong>
(PMF), which maps from each value to its probability. A <strong>probability</strong> is
a frequency expressed as a fraction of the sample size, <code>n</code>. To get
from frequencies to probabilities, we divide through by <code>n</code>, which is
called <strong>normalization</strong>.</p>
</div>
<div class="paragraph">
<p>Given a Hist, we can make a dictionary that maps from each value to its
probability:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">n = hist.Total()
d = {}
for x, freq in hist.Items():
    d[x] = freq / n</code></pre>
</div>
</div>
<div class="paragraph">
<p>Or we can use the Pmf class provided by <code>thinkstats2</code>. Like Hist, the
Pmf constructor can take a list, pandas Series, dictionary, Hist, or
another Pmf object. Here’s an example with a simple list:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; import thinkstats2
&gt;&gt;&gt; pmf = thinkstats2.Pmf([1, 2, 2, 3, 5])
&gt;&gt;&gt; pmf
Pmf({1: 0.2, 2: 0.4, 3: 0.2, 5: 0.2})</pre>
</div>
</div>
<div class="paragraph">
<p>The Pmf is normalized so total probability is 1.</p>
</div>
<div class="paragraph">
<p>Pmf and Hist objects are similar in many ways; in fact, they inherit
many of their methods from a common parent class. For example, the
methods <code>Values</code> and <code>Items</code> work the same way for both. The biggest
difference is that a Hist maps from values to integer counters; a Pmf
maps from values to floating-point probabilities.</p>
</div>
<div class="paragraph">
<p>To look up the probability associated with a value, use <code>Prob</code>:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; pmf.Prob(2)
0.4</pre>
</div>
</div>
<div class="paragraph">
<p>The bracket operator is equivalent:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; pmf[2]
0.4</pre>
</div>
</div>
<div class="paragraph">
<p>You can modify an existing Pmf by incrementing the probability
associated with a value:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; pmf.Incr(2, 0.2)
&gt;&gt;&gt; pmf.Prob(2)
0.6</pre>
</div>
</div>
<div class="paragraph">
<p>Or you can multiply a probability by a factor:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; pmf.Mult(2, 0.5)
&gt;&gt;&gt; pmf.Prob(2)
0.3</pre>
</div>
</div>
<div class="paragraph">
<p>If you modify a Pmf, the result may not be normalized; that is, the
probabilities may no longer add up to 1. To check, you can call
<code>Total</code>, which returns the sum of the probabilities:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; pmf.Total()
0.9</pre>
</div>
</div>
<div class="paragraph">
<p>To renormalize, call <code>Normalize</code>:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; pmf.Normalize()
&gt;&gt;&gt; pmf.Total()
1.0</pre>
</div>
</div>
<div class="paragraph">
<p>Pmf objects provide a <code>Copy</code> method so you can make and modify a copy
without affecting the original.</p>
</div>
<div class="paragraph">
<p>My notation in this section might seem inconsistent, but there is a
system: I use Pmf for the name of the class, <code>pmf</code> for an instance of
the class, and PMF for the mathematical concept of a probability mass
function.</p>
</div>
</div>
<div class="sect2">
<h3 id="_plotting_pmfs"><a class="anchor" href="#_plotting_pmfs"></a><a class="link" href="#_plotting_pmfs">3.2. Plotting PMFs</a></h3>
<div class="paragraph">
<p><code>thinkplot</code> provides two ways to plot Pmfs:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>To plot a Pmf as a bar graph, you can use <code>thinkplot.Hist</code>. Bar
graphs are most useful if the number of values in the Pmf is small.</p>
</li>
<li>
<p>To plot a Pmf as a step function, you can use <code>thinkplot.Pmf</code>. This
option is most useful if there are a large number of values and the Pmf
is smooth. This function also works with Hist objects.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>In addition, <code>pyplot</code> provides a function called <code>hist</code> that takes a
sequence of values, computes a histogram, and plots it. Since I use Hist
objects, I usually don’t use <code>pyplot.hist</code>.</p>
</div>
<div id="probability_nsfg_pmf" class="imageblock">
<div class="content">
<img src="figs/probability_nsfg_pmf.png" alt="probability nsfg pmf" height="288">
</div>
<div class="title">Figure 6. PMF of pregnancy lengths for first babies and others, using bar graphs and step functions.</div>
</div>
<div class="paragraph">
<p><a href="#probability_nsfg_pmf">Figure 6</a> shows PMFs of
pregnancy length for first babies and others using bar graphs (left) and
step functions (right).</p>
</div>
<div class="paragraph">
<p>By plotting the PMF instead of the histogram, we can compare the two
distributions without being mislead by the difference in sample size.
Based on this figure, first babies seem to be less likely than others to
arrive on time (week 39) and more likely to be a late (weeks 41 and 42).</p>
</div>
<div class="paragraph">
<p>Here’s the code that generates
<a href="#probability_nsfg_pmf">Figure 6</a>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    thinkplot.PrePlot(2, cols=2)
    thinkplot.Hist(first_pmf, align='right', width=width)
    thinkplot.Hist(other_pmf, align='left', width=width)
    thinkplot.Config(xlabel='weeks',
                     ylabel='probability',
                     axis=[27, 46, 0, 0.6])

    thinkplot.PrePlot(2)
    thinkplot.SubPlot(2)
    thinkplot.Pmfs([first_pmf, other_pmf])
    thinkplot.Show(xlabel='weeks',
                   axis=[27, 46, 0, 0.6])</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>PrePlot</code> takes optional parameters <code>rows</code> and <code>cols</code> to make a
grid of figures, in this case one row of two figures. The first figure
(on the left) displays the Pmfs using <code>thinkplot.Hist</code>, as we have
seen before.</p>
</div>
<div class="paragraph">
<p>The second call to <code>PrePlot</code> resets the color generator. Then
<code>SubPlot</code> switches to the second figure (on the right) and displays
the Pmfs using <code>thinkplot.Pmfs</code>. I used the <code>axis</code> option to ensure
that the two figures are on the same axes, which is generally a good
idea if you intend to compare two figures.</p>
</div>
</div>
<div class="sect2">
<h3 id="visualization"><a class="anchor" href="#visualization"></a><a class="link" href="#visualization">3.3. Other visualizations</a></h3>
<div class="paragraph">
<p>Histograms and PMFs are useful while you are exploring data and trying
to identify patterns and relationships. Once you have an idea what is
going on, a good next step is to design a visualization that makes the
patterns you have identified as clear as possible.</p>
</div>
<div class="paragraph">
<p>In the NSFG data, the biggest differences in the distributions are near
the mode. So it makes sense to zoom in on that part of the graph, and to
transform the data to emphasize differences:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    weeks = range(35, 46)
    diffs = []
    for week in weeks:
        p1 = first_pmf.Prob(week)
        p2 = other_pmf.Prob(week)
        diff = 100 * (p1 - p2)
        diffs.append(diff)

    thinkplot.Bar(weeks, diffs)</code></pre>
</div>
</div>
<div class="paragraph">
<p>In this code, <code>weeks</code> is the range of weeks; <code>diffs</code> is the
difference between the two PMFs in percentage points.
<a href="#probability_nsfg_diffs">Figure 7</a> shows the
result as a bar chart. This figure makes the pattern clearer: first
babies are less likely to be born in week 39, and somewhat more likely
to be born in weeks 41 and 42.</p>
</div>
<div id="probability_nsfg_diffs" class="imageblock">
<div class="content">
<img src="figs/probability_nsfg_diffs.png" alt="probability nsfg diffs" height="240">
</div>
<div class="title">Figure 7. Difference, in percentage points, by week.</div>
</div>
<div class="paragraph">
<p>For now we should hold this conclusion only tentatively. We used the
same dataset to identify an apparent difference and then chose a
visualization that makes the difference apparent. We can’t be sure this
effect is real; it might be due to random variation. We’ll address this
concern later.</p>
</div>
</div>
<div class="sect2">
<h3 id="_the_class_size_paradox"><a class="anchor" href="#_the_class_size_paradox"></a><a class="link" href="#_the_class_size_paradox">3.4. The class size paradox</a></h3>
<div class="paragraph">
<p>Before we go on, I want to demonstrate one kind of computation you can
do with Pmf objects; I call this example the &#8220;class size paradox.&#8221;</p>
</div>
<div class="paragraph">
<p>At many American colleges and universities, the student-to-faculty ratio
is about 10:1. But students are often surprised to discover that their
average class size is bigger than 10. There are two reasons for the
discrepancy:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Students typically take 4–5 classes per semester, but professors often
teach 1 or 2.</p>
</li>
<li>
<p>The number of students who enjoy a small class is small, but the
number of students in a large class is (ahem!) large.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The first effect is obvious, at least once it is pointed out; the second
is more subtle. Let’s look at an example. Suppose that a college offers
65 classes in a given semester, with the following distribution of
sizes:</p>
</div>
<div class="literalblock">
<div class="content">
<pre> size      count
 5- 9          8
10-14          8
15-19         14
20-24          4
25-29          6
30-34         12
35-39          8
40-44          3
45-49          2</pre>
</div>
</div>
<div class="paragraph">
<p>If you ask the Dean for the average class size, he would construct a
PMF, compute the mean, and report that the average class size is 23.7.
Here’s the code:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    d = { 7: 8, 12: 8, 17: 14, 22: 4,
          27: 6, 32: 12, 37: 8, 42: 3, 47: 2 }

    pmf = thinkstats2.Pmf(d, label='actual')
    print('mean', pmf.Mean())</code></pre>
</div>
</div>
<div class="paragraph">
<p>But if you survey a group of students, ask them how many students are in
their classes, and compute the mean, you would think the average class
was bigger. Let’s see how much bigger.</p>
</div>
<div class="paragraph">
<p>First, I compute the distribution as observed by students, where the
probability associated with each class size is &#8220;biased&#8221; by the number
of students in the class.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def BiasPmf(pmf, label):
    new_pmf = pmf.Copy(label=label)

    for x, p in pmf.Items():
        new_pmf.Mult(x, x)

    new_pmf.Normalize()
    return new_pmf</code></pre>
</div>
</div>
<div class="paragraph">
<p>For each class size, <code>x</code>, we multiply the probability by <code>x</code>, the
number of students who observe that class size. The result is a new Pmf
that represents the biased distribution.</p>
</div>
<div class="paragraph">
<p>Now we can plot the actual and observed distributions:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    biased_pmf = BiasPmf(pmf, label='observed')
    thinkplot.PrePlot(2)
    thinkplot.Pmfs([pmf, biased_pmf])
    thinkplot.Show(xlabel='class size', ylabel='PMF')</code></pre>
</div>
</div>
<div id="class_size1" class="imageblock">
<div class="content">
<img src="figs/class_size1.png" alt="class size1" height="288">
</div>
<div class="title">Figure 8. Distribution of class sizes, actual and as observed by students.</div>
</div>
<div class="paragraph">
<p><a href="#class_size1">Figure 8</a> shows the result. In the biased
distribution there are fewer small classes and more large ones. The mean
of the biased distribution is 29.1, almost 25% higher than the actual
mean.</p>
</div>
<div class="paragraph">
<p>It is also possible to invert this operation. Suppose you want to find
the distribution of class sizes at a college, but you can’t get reliable
data from the Dean. An alternative is to choose a random sample of
students and ask how many students are in their classes.</p>
</div>
<div class="paragraph">
<p>The result would be biased for the reasons we’ve just seen, but you can
use it to estimate the actual distribution. Here’s the function that
unbiases a Pmf:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def UnbiasPmf(pmf, label):
    new_pmf = pmf.Copy(label=label)

    for x, p in pmf.Items():
        new_pmf.Mult(x, 1.0/x)

    new_pmf.Normalize()
    return new_pmf</code></pre>
</div>
</div>
<div class="paragraph">
<p>It’s similar to <code>BiasPmf</code>; the only difference is that it divides each
probability by <code>x</code> instead of multiplying.</p>
</div>
</div>
<div class="sect2">
<h3 id="_dataframe_indexing"><a class="anchor" href="#_dataframe_indexing"></a><a class="link" href="#_dataframe_indexing">3.5. DataFrame indexing</a></h3>
<div class="paragraph">
<p>In <a href="#dataframe">Section 1.4</a> we read a pandas DataFrame and used it
to select and modify data columns. Now let’s look at row selection. To
start, I create a NumPy array of random numbers and use it to initialize
a DataFrame:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; import pandas
&gt;&gt;&gt; array = np.random.randn(4, 2)
&gt;&gt;&gt; df = pandas.DataFrame(array)
&gt;&gt;&gt; df
          0         1
0 -0.143510  0.616050
1 -1.489647  0.300774
2 -0.074350  0.039621
3 -1.369968  0.545897</pre>
</div>
</div>
<div class="paragraph">
<p>By default, the rows and columns are numbered starting at zero, but you
can provide column names:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; columns = ['A', 'B']
&gt;&gt;&gt; df = pandas.DataFrame(array, columns=columns)
&gt;&gt;&gt; df
          A         B
0 -0.143510  0.616050
1 -1.489647  0.300774
2 -0.074350  0.039621
3 -1.369968  0.545897</pre>
</div>
</div>
<div class="paragraph">
<p>You can also provide row names. The set of row names is called the
<strong>index</strong>; the row names themselves are called <strong>labels</strong>.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; index = ['a', 'b', 'c', 'd']
&gt;&gt;&gt; df = pandas.DataFrame(array, columns=columns, index=index)
&gt;&gt;&gt; df
          A         B
a -0.143510  0.616050
b -1.489647  0.300774
c -0.074350  0.039621
d -1.369968  0.545897</pre>
</div>
</div>
<div class="paragraph">
<p>As we saw in the previous chapter, simple indexing selects a column,
returning a Series:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; df['A']
a   -0.143510
b   -1.489647
c   -0.074350
d   -1.369968
Name: A, dtype: float64</pre>
</div>
</div>
<div class="paragraph">
<p>To select a row by label, you can use the <code>loc</code> attribute, which
returns a Series:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; df.loc['a']
A   -0.14351
B    0.61605
Name: a, dtype: float64</pre>
</div>
</div>
<div class="paragraph">
<p>If you know the integer position of a row, rather than its label, you
can use the <code>iloc</code> attribute, which also returns a Series.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; df.iloc[0]
A   -0.14351
B    0.61605
Name: a, dtype: float64</pre>
</div>
</div>
<div class="paragraph">
<p><code>loc</code> can also take a list of labels; in that case, the result is a
DataFrame.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; indices = ['a', 'c']
&gt;&gt;&gt; df.loc[indices]
         A         B
a -0.14351  0.616050
c -0.07435  0.039621</pre>
</div>
</div>
<div class="paragraph">
<p>Finally, you can use a slice to select a range of rows by label:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; df['a':'c']
          A         B
a -0.143510  0.616050
b -1.489647  0.300774
c -0.074350  0.039621</pre>
</div>
</div>
<div class="paragraph">
<p>Or by integer position:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; df[0:2]
          A         B
a -0.143510  0.616050
b -1.489647  0.300774</pre>
</div>
</div>
<div class="paragraph">
<p>The result in either case is a DataFrame, but notice that the first
result includes the end of the slice; the second doesn’t.</p>
</div>
<div class="paragraph">
<p>My advice: if your rows have labels that are not simple integers, use
the labels consistently and avoid using integer positions.</p>
</div>
</div>
<div class="sect2">
<h3 id="_exercises_3"><a class="anchor" href="#_exercises_3"></a><a class="link" href="#_exercises_3">3.6. Exercises</a></h3>
<div class="paragraph">
<p></p>
</div>
<div class="paragraph">
<p><strong>Exercise 3.1</strong></p>
</div>
<div class="paragraph">
<p>Solutions to these exercises are in <code>chap03soln.ipynb</code> and
<code>chap03soln.py</code></p>
</div>
<div class="paragraph">
<p>Something like the class size paradox appears if you survey children and
ask how many children are in their family. Families with many children
are more likely to appear in your sample, and families with no children
have no chance to be in the sample.</p>
</div>
<div class="paragraph">
<p>Use the NSFG respondent variable <code>NUMKDHH</code> to construct the actual
distribution for the number of children under 18 in the household.</p>
</div>
<div class="paragraph">
<p>Now compute the biased distribution we would see if we surveyed the
children and asked them how many children under 18 (including
themselves) are in their household.</p>
</div>
<div class="paragraph">
<p>Plot the actual and biased distributions, and compute their means. As a
starting place, you can use <code>chap03ex.ipynb</code>.</p>
</div>
<div class="paragraph">
<p><strong>Exercise 3.2</strong></p>
</div>
<div class="paragraph">
<p>In <a href="#mean">Section 2.7</a> we computed the mean of a sample by adding up
the elements and dividing by n. If you are given a PMF, you can still
compute the mean, but the process is slightly different:</p>
</div>
<div class="stemblock">
<div class="content">
\[\bar{x}= \sum_i p_i~x_i\]
</div>
</div>
<div class="paragraph">
<p>where the \(x_i\) are the unique values in the PMF and
\(p_i=PMF(x_i)\). Similarly, you can compute variance like this:</p>
</div>
<div class="stemblock">
<div class="content">
\[S^2 = \sum_i p_i~(x_i - \bar{x})^2\]
</div>
</div>
<div class="paragraph">
<p>Write functions called <code>PmfMean</code> and <code>PmfVar</code> that take a Pmf object
and compute the mean and variance. To test these methods, check that
they are consistent with the methods <code>Mean</code> and <code>Var</code> provided by
Pmf.</p>
</div>
<div class="paragraph">
<p><strong>Exercise 3.3</strong></p>
</div>
<div class="paragraph">
<p>I started with the question, &#8220;Are first babies more likely to be
late?&#8221; To address it, I computed the difference in means between groups
of babies, but I ignored the possibility that there might be a
difference between first babies and others <em>for the same woman</em>.</p>
</div>
<div class="paragraph">
<p>To address this version of the question, select respondents who have at
least two babies and compute pairwise differences. Does this formulation
of the question yield a different result?</p>
</div>
<div class="paragraph">
<p>Hint: use <code>nsfg.MakePregMap</code>.</p>
</div>
<div class="paragraph">
<p><strong>Exercise 3.4</strong></p>
</div>
<div id="relay" class="paragraph">
<p>In most foot races, everyone starts at the same time. If you are a fast
runner, you usually pass a lot of people at the beginning of the race,
but after a few miles everyone around you is going at the same speed.</p>
</div>
<div class="paragraph">
<p>When I ran a long-distance (209 miles) relay race for the first time, I
noticed an odd phenomenon: when I overtook another runner, I was usually
much faster, and when another runner overtook me, he was usually much
faster.</p>
</div>
<div class="paragraph">
<p>At first I thought that the distribution of speeds might be bimodal;
that is, there were many slow runners and many fast runners, but few at
my speed.</p>
</div>
<div class="paragraph">
<p>Then I realized that I was the victim of a bias similar to the effect of
class size. The race was unusual in two ways: it used a staggered start,
so teams started at different times; also, many teams included runners
at different levels of ability.</p>
</div>
<div class="paragraph">
<p>As a result, runners were spread out along the course with little
relationship between speed and location. When I joined the race, the
runners near me were (pretty much) a random sample of the runners in the
race.</p>
</div>
<div class="paragraph">
<p>So where does the bias come from? During my time on the course, the
chance of overtaking a runner, or being overtaken, is proportional to
the difference in our speeds. I am more likely to catch a slow runner,
and more likely to be caught by a fast runner. But runners at the same
speed are unlikely to see each other.</p>
</div>
<div class="paragraph">
<p>Write a function called <code>ObservedPmf</code> that takes a Pmf representing
the actual distribution of runners’ speeds, and the speed of a running
observer, and returns a new Pmf representing the distribution of
runners’ speeds as seen by the observer.</p>
</div>
<div class="paragraph">
<p>To test your function, you can use <code>relay.py</code>, which reads the results
from the James Joyce Ramble 10K in Dedham MA and converts the pace of
each runner to mph.</p>
</div>
<div class="paragraph">
<p>Compute the distribution of speeds you would observe if you ran a relay
race at 7.5 mph with this group of runners. A solution to this exercise
is in <code>relay_soln.py</code>.</p>
</div>
</div>
<div class="sect2">
<h3 id="_glossary_3"><a class="anchor" href="#_glossary_3"></a><a class="link" href="#_glossary_3">3.7. Glossary</a></h3>
<div class="ulist">
<ul>
<li>
<p><strong>Probability mass function (PMF)</strong>: a representation of a distribution
as a function that maps from values to probabilities.</p>
</li>
<li>
<p><strong>probability</strong>: A frequency expressed as a fraction of the sample size.</p>
</li>
<li>
<p><strong>normalization</strong>: The process of dividing a frequency by a sample size
to get a probability.</p>
</li>
<li>
<p><strong>index</strong>: In a pandas DataFrame, the index is a special column that
contains the row labels.</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="cumulative"><a class="anchor" href="#cumulative"></a><a class="link" href="#cumulative">4. Cumulative distribution functions</a></h2>
<div class="sectionbody">
<div class="paragraph">
<p>The code for this chapter is in <code>cumulative.py</code>. For information about
downloading and working with this code, see <a href="#code">Using the code</a>.</p>
</div>
<div class="sect2">
<h3 id="_the_limits_of_pmfs"><a class="anchor" href="#_the_limits_of_pmfs"></a><a class="link" href="#_the_limits_of_pmfs">4.1. The limits of PMFs</a></h3>
<div class="paragraph">
<p>PMFs work well if the number of values is small. But as the number of
values increases, the probability associated with each value gets
smaller and the effect of random noise increases.</p>
</div>
<div class="paragraph">
<p>For example, we might be interested in the distribution of birth
weights. In the NSFG data, the variable <code>totalwgt_lb</code> records weight
at birth in pounds. <a href="#nsfg_birthwgt_pmf">Figure 9</a>
shows the PMF of these values for first babies and others.</p>
</div>
<div id="nsfg_birthwgt_pmf" class="imageblock">
<div class="content">
<img src="figs/nsfg_birthwgt_pmf.png" alt="nsfg birthwgt pmf" height="240">
</div>
<div class="title">Figure 9. PMF of birth weights. This figure shows a limitation of PMFs: they are hard to compare visually.</div>
</div>
<div class="paragraph">
<p>Overall, these distributions resemble the bell shape of a normal
distribution, with many values near the mean and a few values much
higher and lower.</p>
</div>
<div class="paragraph">
<p>But parts of this figure are hard to interpret. There are many spikes
and valleys, and some apparent differences between the distributions. It
is hard to tell which of these features are meaningful. Also, it is hard
to see overall patterns; for example, which distribution do you think
has the higher mean?</p>
</div>
<div class="paragraph">
<p>These problems can be mitigated by binning the data; that is, dividing
the range of values into non-overlapping intervals and counting the
number of values in each bin. Binning can be useful, but it is tricky to
get the size of the bins right. If they are big enough to smooth out
noise, they might also smooth out useful information.</p>
</div>
<div class="paragraph">
<p>An alternative that avoids these problems is the cumulative distribution
function (CDF), which is the subject of this chapter. But before I can
explain CDFs, I have to explain percentiles.</p>
</div>
</div>
<div class="sect2">
<h3 id="_percentiles"><a class="anchor" href="#_percentiles"></a><a class="link" href="#_percentiles">4.2. Percentiles</a></h3>
<div class="paragraph">
<p>If you have taken a standardized test, you probably got your results in
the form of a raw score and a <strong>percentile rank</strong>. In this context, the
percentile rank is the fraction of people who scored lower than you (or
the same). So if you are &#8220;in the 90th percentile,&#8221; you did as well as
or better than 90% of the people who took the exam.</p>
</div>
<div class="paragraph">
<p>Here’s how you could compute the percentile rank of a value,
<code>your_score</code>, relative to the values in the sequence <code>scores</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def PercentileRank(scores, your_score):
    count = 0
    for score in scores:
        if score &lt;= your_score:
            count += 1

    percentile_rank = 100.0 * count / len(scores)
    return percentile_rank</code></pre>
</div>
</div>
<div class="paragraph">
<p>As an example, if the scores in the sequence were 55, 66, 77, 88 and 99,
and you got the 88, then your percentile rank would be <code>100 * 4 / 5</code>
which is 80.</p>
</div>
<div class="paragraph">
<p>If you are given a value, it is easy to find its percentile rank; going
the other way is slightly harder. If you are given a percentile rank and
you want to find the corresponding value, one option is to sort the
values and search for the one you want:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def Percentile(scores, percentile_rank):
    scores.sort()
    for score in scores:
        if PercentileRank(scores, score) &gt;= percentile_rank:
            return score</code></pre>
</div>
</div>
<div class="paragraph">
<p>The result of this calculation is a <strong>percentile</strong>. For example, the 50th
percentile is the value with percentile rank 50. In the distribution of
exam scores, the 50th percentile is 77.</p>
</div>
<div class="paragraph">
<p>This implementation of <code>Percentile</code> is not efficient. A better
approach is to use the percentile rank to compute the index of the
corresponding percentile:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def Percentile2(scores, percentile_rank):
    scores.sort()
    index = percentile_rank * (len(scores)-1) // 100
    return scores[index]</code></pre>
</div>
</div>
<div class="paragraph">
<p>The difference between &#8220;percentile&#8221; and &#8220;percentile rank&#8221; can be
confusing, and people do not always use the terms precisely. To
summarize, <code>PercentileRank</code> takes a value and computes its percentile
rank in a set of values; <code>Percentile</code> takes a percentile rank and
computes the corresponding value.</p>
</div>
</div>
<div class="sect2">
<h3 id="_cdfs"><a class="anchor" href="#_cdfs"></a><a class="link" href="#_cdfs">4.3. CDFs</a></h3>
<div class="paragraph">
<p>Now that we understand percentiles and percentile ranks, we are ready to
tackle the <strong>cumulative distribution function</strong> (CDF). The CDF is the
function that maps from a value to its percentile rank.</p>
</div>
<div class="paragraph">
<p>The CDF is a function of \(x\), where \(x\) is any value
that might appear in the distribution. To evaluate
\(\mathrm{CDF}(x)\) for a particular value of \(x\), we
compute the fraction of values in the distribution less than or equal to
\(x\).</p>
</div>
<div class="paragraph">
<p>Here’s what that looks like as a function that takes a sequence,
<code>sample</code>, and a value, <code>x</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def EvalCdf(sample, x):
    count = 0.0
    for value in sample:
        if value &lt;= x:
            count += 1

    prob = count / len(sample)
    return prob</code></pre>
</div>
</div>
<div class="paragraph">
<p>This function is almost identical to <code>PercentileRank</code>, except that the
result is a probability in the range 0–1 rather than a percentile rank
in the range 0–100.</p>
</div>
<div class="paragraph">
<p>As an example, suppose we collect a sample with the values
<code>[1, 2, 2, 3, 5]</code>. Here are some values from its CDF:</p>
</div>
<div class="stemblock">
<div class="content">
\[CDF(0) = 0\]
</div>
</div>
<div class="stemblock">
<div class="content">
\[CDF(1) = 0.2\]
</div>
</div>
<div class="stemblock">
<div class="content">
\[CDF(2) = 0.6\]
</div>
</div>
<div class="stemblock">
<div class="content">
\[CDF(3) = 0.8\]
</div>
</div>
<div class="stemblock">
<div class="content">
\[CDF(4) = 0.8\]
</div>
</div>
<div class="stemblock">
<div class="content">
\[CDF(5) = 1\]
</div>
</div>
<div class="paragraph">
<p>We can evaluate the CDF for any value of \(x\), not just values
that appear in the sample. If \(x\) is less than the smallest
value in the sample, \(\mathrm{CDF}(x)\) is 0. If \(x\)
is greater than the largest value, \(\mathrm{CDF}(x)\) is 1.</p>
</div>
<div id="example_cdf" class="imageblock">
<div class="content">
<img src="figs/cumulative_example_cdf.png" alt="cumulative example cdf" height="240">
</div>
<div class="title">Figure 10. Example of a CDF.</div>
</div>
<div class="paragraph">
<p><a href="#example_cdf">Figure 10</a> is a graphical representation of
this CDF. The CDF of a sample is a step function.</p>
</div>
</div>
<div class="sect2">
<h3 id="_representing_cdfs"><a class="anchor" href="#_representing_cdfs"></a><a class="link" href="#_representing_cdfs">4.4. Representing CDFs</a></h3>
<div class="paragraph">
<p><code>thinkstats2</code> provides a class named Cdf that represents CDFs. The
fundamental methods Cdf provides are:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>Prob(x)</code>: Given a value <code>x</code>, computes the probability
\(p = \mathrm{CDF}(x)\). The bracket operator is equivalent to
<code>Prob</code>.</p>
</li>
<li>
<p><code>Value(p)</code>: Given a probability <code>p</code>, computes the corresponding
value, <code>x</code>; that is, the <strong>inverse CDF</strong> of <code>p</code>.</p>
</li>
</ul>
</div>
<div id="cumulative_prglngth_cdf" class="imageblock">
<div class="content">
<img src="figs/cumulative_prglngth_cdf.png" alt="cumulative prglngth cdf" height="240">
</div>
<div class="title">Figure 11. CDF of pregnancy length.</div>
</div>
<div class="paragraph">
<p>The Cdf constructor can take as an argument a list of values, a pandas
Series, a Hist, Pmf, or another Cdf. The following code makes a Cdf for
the distribution of pregnancy lengths in the NSFG:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    live, firsts, others = first.MakeFrames()
    cdf = thinkstats2.Cdf(live.prglngth, label='prglngth')</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>thinkplot</code> provides a function named <code>Cdf</code> that plots Cdfs as
lines:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    thinkplot.Cdf(cdf)
    thinkplot.Show(xlabel='weeks', ylabel='CDF')</code></pre>
</div>
</div>
<div class="paragraph">
<p><a href="#cumulative_prglngth_cdf">Figure 11</a> shows
the result. One way to read a CDF is to look up percentiles. For
example, it looks like about 10% of pregnancies are shorter than 36
weeks, and about 90% are shorter than 41 weeks. The CDF also provides a
visual representation of the shape of the distribution. Common values
appear as steep or vertical sections of the CDF; in this example, the
mode at 39 weeks is apparent. There are few values below 30 weeks, so
the CDF in this range is flat.</p>
</div>
<div class="paragraph">
<p>It takes some time to get used to CDFs, but once you do, I think you
will find that they show more information, more clearly, than PMFs.</p>
</div>
</div>
<div class="sect2">
<h3 id="birth_weights"><a class="anchor" href="#birth_weights"></a><a class="link" href="#birth_weights">4.5. Comparing CDFs</a></h3>
<div class="paragraph">
<p>CDFs are especially useful for comparing distributions. For example,
here is the code that plots the CDF of birth weight for first babies and
others.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    first_cdf = thinkstats2.Cdf(firsts.totalwgt_lb, label='first')
    other_cdf = thinkstats2.Cdf(others.totalwgt_lb, label='other')

    thinkplot.PrePlot(2)
    thinkplot.Cdfs([first_cdf, other_cdf])
    thinkplot.Show(xlabel='weight (pounds)', ylabel='CDF')</code></pre>
</div>
</div>
<div id="cumulative_birthwgt_cdf" class="imageblock">
<div class="content">
<img src="figs/cumulative_birthwgt_cdf.png" alt="cumulative birthwgt cdf" height="240">
</div>
<div class="title">Figure 12. CDF of birth weights for first babies and others.</div>
</div>
<div class="paragraph">
<p><a href="#cumulative_birthwgt_cdf">Figure 12</a> shows
the result. Compared to
<a href="#nsfg_birthwgt_pmf">Figure 9</a>, this figure makes
the shape of the distributions, and the differences between them, much
clearer. We can see that first babies are slightly lighter throughout
the distribution, with a larger discrepancy above the mean.</p>
</div>
</div>
<div class="sect2">
<h3 id="_percentile_based_statistics"><a class="anchor" href="#_percentile_based_statistics"></a><a class="link" href="#_percentile_based_statistics">4.6. Percentile-based statistics</a></h3>
<div class="paragraph">
<p>Once you have computed a CDF, it is easy to compute percentiles and
percentile ranks. The Cdf class provides these two methods:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>PercentileRank(x)</code>: Given a value <code>x</code>, computes its percentile
rank, \(100 \cdot \mathrm{CDF}(x)\).</p>
</li>
<li>
<p><code>Percentile(p)</code>: Given a percentile rank <code>p</code>, computes the
corresponding value, <code>x</code>. Equivalent to <code>Value(p/100)</code>.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><code>Percentile</code> can be used to compute percentile-based summary
statistics. For example, the 50th percentile is the value that divides
the distribution in half, also known as the <strong>median</strong>. Like the mean, the
median is a measure of the central tendency of a distribution.</p>
</div>
<div class="paragraph">
<p>Actually, there are several definitions of &#8220;median,&#8221; each with
different properties. But <code>Percentile(50)</code> is simple and efficient to
compute.</p>
</div>
<div class="paragraph">
<p>Another percentile-based statistic is the <strong>interquartile range</strong> (IQR),
which is a measure of the spread of a distribution. The IQR is the
difference between the 75th and 25th percentiles.</p>
</div>
<div class="paragraph">
<p>More generally, percentiles are often used to summarize the shape of a
distribution. For example, the distribution of income is often reported
in &#8220;quintiles&#8221;; that is, it is split at the 20th, 40th, 60th and 80th
percentiles. Other distributions are divided into ten &#8220;deciles&#8221;.
Statistics like these that represent equally-spaced points in a CDF are
called <strong>quantiles</strong>. For more, see
<a href="https://en.wikipedia.org/wiki/Quantile" class="bare">https://en.wikipedia.org/wiki/Quantile</a>.</p>
</div>
</div>
<div class="sect2">
<h3 id="random"><a class="anchor" href="#random"></a><a class="link" href="#random">4.7. Random numbers</a></h3>
<div class="paragraph">
<p>Suppose we choose a random sample from the population of live births and
look up the percentile rank of their birth weights. Now suppose we
compute the CDF of the percentile ranks. What do you think the
distribution will look like?</p>
</div>
<div class="paragraph">
<p>Here’s how we can compute it. First, we make the Cdf of birth weights:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    weights = live.totalwgt_lb
    cdf = thinkstats2.Cdf(weights, label='totalwgt_lb')</code></pre>
</div>
</div>
<div class="paragraph">
<p>Then we generate a sample and compute the percentile rank of each value
in the sample.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    sample = np.random.choice(weights, 100, replace=True)
    ranks = [cdf.PercentileRank(x) for x in sample]</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>sample</code> is a random sample of 100 birth weights, chosen with
<strong>replacement</strong>; that is, the same value could be chosen more than once.
<code>ranks</code> is a list of percentile ranks.</p>
</div>
<div class="paragraph">
<p>Finally we make and plot the Cdf of the percentile ranks.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    rank_cdf = thinkstats2.Cdf(ranks)
    thinkplot.Cdf(rank_cdf)
    thinkplot.Show(xlabel='percentile rank', ylabel='CDF')</code></pre>
</div>
</div>
<div id="cumulative_random" class="imageblock">
<div class="content">
<img src="figs/cumulative_random.png" alt="cumulative random" height="240">
</div>
<div class="title">Figure 13. CDF of percentile ranks for a random sample of birth weights.</div>
</div>
<div class="paragraph">
<p><a href="#cumulative_random">Figure 13</a> shows the result.
The CDF is approximately a straight line, which means that the
distribution is uniform.</p>
</div>
<div class="paragraph">
<p>That outcome might be non-obvious, but it is a consequence of the way
the CDF is defined. What this figure shows is that 10% of the sample is
below the 10th percentile, 20% is below the 20th percentile, and so on,
exactly as we should expect.</p>
</div>
<div class="paragraph">
<p>So, regardless of the shape of the CDF, the distribution of percentile
ranks is uniform. This property is useful, because it is the basis of a
simple and efficient algorithm for generating random numbers with a
given CDF. Here’s how:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Choose a percentile rank uniformly from the range 0–100.</p>
</li>
<li>
<p>Use <code>Cdf.Percentile</code> to find the value in the distribution that
corresponds to the percentile rank you chose.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Cdf provides an implementation of this algorithm, called <code>Random</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python"># class Cdf:
    def Random(self):
        return self.Percentile(random.uniform(0, 100))</code></pre>
</div>
</div>
<div class="paragraph">
<p>Cdf also provides <code>Sample</code>, which takes an integer, <code>n</code>, and returns
a list of <code>n</code> values chosen at random from the Cdf.</p>
</div>
</div>
<div class="sect2">
<h3 id="_comparing_percentile_ranks"><a class="anchor" href="#_comparing_percentile_ranks"></a><a class="link" href="#_comparing_percentile_ranks">4.8. Comparing percentile ranks</a></h3>
<div class="paragraph">
<p>Percentile ranks are useful for comparing measurements across different
groups. For example, people who compete in foot races are usually
grouped by age and gender. To compare people in different age groups,
you can convert race times to percentile ranks.</p>
</div>
<div class="paragraph">
<p>A few years ago I ran the James Joyce Ramble 10K in Dedham MA; I
finished in 42:44, which was 97th in a field of 1633. I beat or tied
1537 runners out of 1633, so my percentile rank in the field is 94%.</p>
</div>
<div class="paragraph">
<p>More generally, given position and field size, we can compute percentile
rank:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def PositionToPercentile(position, field_size):
    beat = field_size - position + 1
    percentile = 100.0 * beat / field_size
    return percentile</code></pre>
</div>
</div>
<div class="paragraph">
<p>In my age group, denoted M4049 for &#8220;male between 40 and 49 years of
age&#8221;, I came in 26th out of 256. So my percentile rank in my age group
was 90%.</p>
</div>
<div class="paragraph">
<p>If I am still running in 10 years (and I hope I am), I will be in the
M5059 division. Assuming that my percentile rank in my division is the
same, how much slower should I expect to be?</p>
</div>
<div class="paragraph">
<p>I can answer that question by converting my percentile rank in M4049 to
a position in M5059. Here’s the code:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def PercentileToPosition(percentile, field_size):
    beat = percentile * field_size / 100.0
    position = field_size - beat + 1
    return position</code></pre>
</div>
</div>
<div class="paragraph">
<p>There were 171 people in M5059, so I would have to come in between 17th
and 18th place to have the same percentile rank. The finishing time of
the 17th runner in M5059 was 46:05, so that’s the time I will have to
beat to maintain my percentile rank.</p>
</div>
</div>
<div class="sect2">
<h3 id="_exercises_4"><a class="anchor" href="#_exercises_4"></a><a class="link" href="#_exercises_4">4.9. Exercises</a></h3>
<div class="paragraph">
<p></p>
</div>
<div class="paragraph">
<p>For the following exercises, you can start with <code>chap04ex.ipynb</code>. My
solution is in <code>chap04soln.ipynb</code>.</p>
</div>
<div class="paragraph">
<p><strong>Exercise 4.1</strong></p>
</div>
<div class="paragraph">
<p>How much did you weigh at birth? If you don’t know, call your mother or
someone else who knows. Using the NSFG data (all live births), compute
the distribution of birth weights and use it to find your percentile
rank. If you were a first baby, find your percentile rank in the
distribution for first babies. Otherwise use the distribution for
others. If you are in the 90th percentile or higher, call your mother
back and apologize.</p>
</div>
<div class="paragraph">
<p><strong>Exercise 4.2</strong></p>
</div>
<div class="paragraph">
<p>The numbers generated by <code>random.random</code> are supposed to be uniform
between 0 and 1; that is, every value in the range should have the same
probability.</p>
</div>
<div class="paragraph">
<p>Generate 1000 numbers from <code>random.random</code> and plot their PMF and CDF.
Is the distribution uniform?</p>
</div>
</div>
<div class="sect2">
<h3 id="_glossary_4"><a class="anchor" href="#_glossary_4"></a><a class="link" href="#_glossary_4">4.10. Glossary</a></h3>
<div class="ulist">
<ul>
<li>
<p><strong>percentile rank</strong>: The percentage of values in a distribution that are
less than or equal to a given value.</p>
</li>
<li>
<p><strong>percentile</strong>: The value associated with a given percentile rank.</p>
</li>
<li>
<p><strong>cumulative distribution function (CDF)</strong>: A function that maps from
values to their cumulative probabilities. \(\mathrm{CDF}(x)\) is
the fraction of the sample less than or equal to \(x\).</p>
</li>
<li>
<p><strong>inverse CDF</strong>: A function that maps from a cumulative probability,
\(p\), to the corresponding value.</p>
</li>
<li>
<p><strong>median</strong>: The 50th percentile, often used as a measure of central
tendency.</p>
</li>
<li>
<p><strong>interquartile range</strong>: The difference between the 75th and 25th
percentiles, used as a measure of spread.</p>
</li>
<li>
<p><strong>quantile</strong>: A sequence of values that correspond to equally spaced
percentile ranks; for example, the quartiles of a distribution are the
25th, 50th and 75th percentiles.</p>
</li>
<li>
<p><strong>replacement</strong>: A property of a sampling process. &#8220;With replacement&#8221;
means that the same value can be chosen more than once; &#8220;without
replacement&#8221; means that once a value is chosen, it is removed from the
population.</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="modeling"><a class="anchor" href="#modeling"></a><a class="link" href="#modeling">5. Modeling distributions</a></h2>
<div class="sectionbody">
<div class="paragraph">
<p>The distributions we have used so far are called <strong>empirical
distributions</strong> because they are based on empirical observations, which
are necessarily finite samples.</p>
</div>
<div class="paragraph">
<p>The alternative is an <strong>analytic distribution</strong>, which is characterized by
a CDF that is a mathematical function. Analytic distributions can be
used to model empirical distributions. In this context, a <strong>model</strong> is a
simplification that leaves out unneeded details. This chapter presents
common analytic distributions and uses them to model data from a variety
of sources.</p>
</div>
<div class="paragraph">
<p>The code for this chapter is in <code>analytic.py</code>. For information about
downloading and working with this code, see <a href="#code">Using the code</a>.</p>
</div>
<div class="sect2">
<h3 id="exponential"><a class="anchor" href="#exponential"></a><a class="link" href="#exponential">5.1. The exponential distribution</a></h3>
<div id="analytic_expo_cdf" class="imageblock">
<div class="content">
<img src="figs/analytic_expo_cdf.png" alt="analytic expo cdf" height="240">
</div>
<div class="title">Figure 14. CDFs of exponential distributions with various parameters.</div>
</div>
<div class="paragraph">
<p>I’ll start with the <strong>exponential distribution</strong> because it is relatively
simple. The CDF of the exponential distribution is</p>
</div>
<div class="stemblock">
<div class="content">
\[\mathrm{CDF}(x) = 1 - e^{-\lambda x}\]
</div>
</div>
<div class="paragraph">
<p>The parameter, \(\lambda\), determines the shape of the
distribution. <a href="#analytic_expo_cdf">Figure 14</a> shows
what this CDF looks like with \(\lambda =\) 0.5, 1, and 2.</p>
</div>
<div class="paragraph">
<p>In the real world, exponential distributions come up when we look at a
series of events and measure the times between events, called
<strong>interarrival times</strong>. If the events are equally likely to occur at any
time, the distribution of interarrival times tends to look like an
exponential distribution.</p>
</div>
<div class="paragraph">
<p>As an example, we will look at the interarrival time of births. On
December 18, 1997, 44 babies were born in a hospital in Brisbane,
Australia.<sup class="footnote">[<a id="_footnoteref_1" class="footnote" href="#_footnote_1" title="View footnote.">1</a>]</sup> The time of birth for
all 44 babies was reported in the local paper; the complete dataset is
in a file called <code>babyboom.dat</code>, in the <code>ThinkStats2</code> repository.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    df = ReadBabyBoom()
    diffs = df.minutes.diff()
    cdf = thinkstats2.Cdf(diffs, label='actual')

    thinkplot.Cdf(cdf)
    thinkplot.Show(xlabel='minutes', ylabel='CDF')</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>ReadBabyBoom</code> reads the data file and returns a DataFrame with
columns <code>time</code>, <code>sex</code>, <code>weight_g</code>, and <code>minutes</code>, where
<code>minutes</code> is time of birth converted to minutes since midnight.</p>
</div>
<div id="analytic_interarrival_cdf" class="imageblock">
<div class="content">
<img src="figs/analytic_interarrivals.png" alt="analytic interarrivals" height="240">
</div>
<div class="title">Figure 15. CDF of interarrival times (left) and CCDF on a log-y scale (right).</div>
</div>
<div class="paragraph">
<p><code>diffs</code> is the difference between consecutive birth times, and <code>cdf</code>
is the distribution of these interarrival times.
<a href="#analytic_interarrival_cdf">Figure 15</a>
(left) shows the CDF. It seems to have the general shape of an
exponential distribution, but how can we tell?</p>
</div>
<div class="paragraph">
<p>One way is to plot the <strong>complementary CDF</strong>, which is
\(1 - \mathrm{CDF}(x)\), on a log-y scale. For data from an
exponential distribution, the result is a straight line. Let’s see why
that works.</p>
</div>
<div class="paragraph">
<p>If you plot the complementary CDF (CCDF) of a dataset that you think is
exponential, you expect to see a function like:</p>
</div>
<div class="stemblock">
<div class="content">
\[y \approx e^{-\lambda x}\]
</div>
</div>
<div class="paragraph">
<p>Taking the log of both sides yields:</p>
</div>
<div class="stemblock">
<div class="content">
\[\log y \approx -\lambda x\]
</div>
</div>
<div class="paragraph">
<p>So on a log-y scale the CCDF is a straight line with slope
\(-\lambda\). Here’s how we can generate a plot like that:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    thinkplot.Cdf(cdf, complement=True)
    thinkplot.Show(xlabel='minutes',
                   ylabel='CCDF',
                   yscale='log')</code></pre>
</div>
</div>
<div class="paragraph">
<p>With the argument <code>complement=True</code>, <code>thinkplot.Cdf</code> computes the
complementary CDF before plotting. And with <code>yscale='log'</code>,
<code>thinkplot.Show</code> sets the <code>y</code> axis to a logarithmic scale.</p>
</div>
<div class="paragraph">
<p><a href="#analytic_interarrival_cdf">Figure 15</a>
(right) shows the result. It is not exactly straight, which indicates
that the exponential distribution is not a perfect model for this data.
Most likely the underlying assumption—that a birth is equally likely at
any time of day—is not exactly true. Nevertheless, it might be
reasonable to model this dataset with an exponential distribution. With
that simplification, we can summarize the distribution with a single
parameter.</p>
</div>
<div class="paragraph">
<p>The parameter, \(\lambda\), can be interpreted as a rate; that
is, the number of events that occur, on average, in a unit of time. In
this example, 44 babies are born in 24 hours, so the rate is
\(\lambda =
0.0306\) births per minute. The mean of an exponential distribution is
\(1/\lambda\), so the mean time between births is 32.7 minutes.</p>
</div>
</div>
<div class="sect2">
<h3 id="normal"><a class="anchor" href="#normal"></a><a class="link" href="#normal">5.2. The normal distribution</a></h3>
<div class="paragraph">
<p>The <strong>normal distribution</strong>, also called Gaussian, is commonly used
because it describes many phenomena, at least approximately. It turns
out that there is a good reason for its ubiquity, which we will get to
in <a href="#CLT">Section 14.4</a>.</p>
</div>
<div id="analytic_gaussian_cdf" class="imageblock">
<div class="content">
<img src="figs/analytic_gaussian_cdf.png" alt="analytic gaussian cdf" height="240">
</div>
<div class="title">Figure 16. CDF of normal distributions with a range of parameters.</div>
</div>
<div class="paragraph">
<p>The normal distribution is characterized by two parameters: the mean,
\(\mu\), and standard deviation \(\sigma\). The normal
distribution with \(\mu=0\) and \(\sigma=1\) is called
the <strong>standard normal distribution</strong>. Its CDF is defined by an integral
that does not have a closed form solution, but there are algorithms that
evaluate it efficiently. One of them is provided by SciPy:
<code>scipy.stats.norm</code> is an object that represents a normal distribution;
it provides a method, <code>cdf</code>, that evaluates the standard normal CDF:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; import scipy.stats
&gt;&gt;&gt; scipy.stats.norm.cdf(0)
0.5</pre>
</div>
</div>
<div class="paragraph">
<p>This result is correct: the median of the standard normal distribution
is 0 (the same as the mean), and half of the values fall below the
median, so \(\mathrm{CDF}(0)\) is 0.5.</p>
</div>
<div class="paragraph">
<p><code>norm.cdf</code> takes optional parameters: <code>loc</code>, which specifies the
mean, and <code>scale</code>, which specifies the standard deviation.</p>
</div>
<div class="paragraph">
<p><code>thinkstats2</code> makes this function a little easier to use by providing
<code>EvalNormalCdf</code>, which takes parameters <code>mu</code> and <code>sigma</code> and
evaluates the CDF at <code>x</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def EvalNormalCdf(x, mu=0, sigma=1):
    return scipy.stats.norm.cdf(x, loc=mu, scale=sigma)</code></pre>
</div>
</div>
<div class="paragraph">
<p><a href="#analytic_gaussian_cdf">Figure 16</a> shows CDFs
for normal distributions with a range of parameters. The sigmoid shape
of these curves is a recognizable characteristic of a normal
distribution.</p>
</div>
<div class="paragraph">
<p>In the previous chapter we looked at the distribution of birth weights
in the NSFG.
<a href="#analytic_birthwgt_model">Figure 17</a> shows
the empirical CDF of weights for all live births and the CDF of a normal
distribution with the same mean and variance.</p>
</div>
<div id="analytic_birthwgt_model" class="imageblock">
<div class="content">
<img src="figs/analytic_birthwgt_model.png" alt="analytic birthwgt model" height="240">
</div>
<div class="title">Figure 17. CDF of birth weights with a normal model.</div>
</div>
<div class="paragraph">
<p>The normal distribution is a good model for this dataset, so if we
summarize the distribution with the parameters \(\mu = 7.28\)
and \(\sigma = 1.24\), the resulting error (difference between
the model and the data) is small.</p>
</div>
<div class="paragraph">
<p>Below the 10th percentile there is a discrepancy between the data and
the model; there are more light babies than we would expect in a normal
distribution. If we are specifically interested in preterm babies, it
would be important to get this part of the distribution right, so it
might not be appropriate to use the normal model.</p>
</div>
</div>
<div class="sect2">
<h3 id="_normal_probability_plot"><a class="anchor" href="#_normal_probability_plot"></a><a class="link" href="#_normal_probability_plot">5.3. Normal probability plot</a></h3>
<div class="paragraph">
<p>For the exponential distribution, and a few others, there are simple
transformations we can use to test whether an analytic distribution is a
good model for a dataset.</p>
</div>
<div class="paragraph">
<p>For the normal distribution there is no such transformation, but there
is an alternative called a <strong>normal probability plot</strong>. There are two ways
to generate a normal probability plot: the hard way and the easy way. If
you are interested in the hard way, you can read about it at
<a href="https://en.wikipedia.org/wiki/Normal_probability_plot" class="bare">https://en.wikipedia.org/wiki/Normal_probability_plot</a>. Here’s the easy
way:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Sort the values in the sample.</p>
</li>
<li>
<p>From a standard normal distribution (\(\mu=0\) and
\(\sigma=1\)), generate a random sample with the same size as
the sample, and sort it.</p>
</li>
<li>
<p>Plot the sorted values from the sample versus the random values.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>If the distribution of the sample is approximately normal, the result is
a straight line with intercept <code>mu</code> and slope <code>sigma</code>.
<code>thinkstats2</code> provides <code>NormalProbability</code>, which takes a sample and
returns two NumPy arrays:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">xs, ys = thinkstats2.NormalProbability(sample)</code></pre>
</div>
</div>
<div id="analytic_normal_prob_example" class="imageblock">
<div class="content">
<img src="figs/analytic_normal_prob_example.png" alt="analytic normal prob example" height="240">
</div>
<div class="title">Figure 18. Normal probability plot for random samples from normal distributions.</div>
</div>
<div class="paragraph">
<p><code>ys</code> contains the sorted values from <code>sample</code>; <code>xs</code> contains the
random values from the standard normal distribution.</p>
</div>
<div class="paragraph">
<p>To test <code>NormalProbability</code> I generated some fake samples that were
actually drawn from normal distributions with various parameters.
<a href="#analytic_normal_prob_example">Figure 18</a>
shows the results. The lines are approximately straight, with values in
the tails deviating more than values near the mean.</p>
</div>
<div class="paragraph">
<p>Now let’s try it with real data. Here’s code to generate a normal
probability plot for the birth weight data from the previous section. It
plots a gray line that represents the model and a blue line that
represents the data.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def MakeNormalPlot(weights):
    mean = weights.mean()
    std = weights.std()

    xs = [-4, 4]
    fxs, fys = thinkstats2.FitLine(xs, inter=mean, slope=std)
    thinkplot.Plot(fxs, fys, color='gray', label='model')

    xs, ys = thinkstats2.NormalProbability(weights)
    thinkplot.Plot(xs, ys, label='birth weights')</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>weights</code> is a pandas Series of birth weights; <code>mean</code> and <code>std</code>
are the mean and standard deviation.</p>
</div>
<div class="paragraph">
<p><code>FitLine</code> takes a sequence of <code>xs</code>, an intercept, and a slope; it
returns <code>xs</code> and <code>ys</code> that represent a line with the given
parameters, evaluated at the values in <code>xs</code>.</p>
</div>
<div class="paragraph">
<p><code>NormalProbability</code> returns <code>xs</code> and <code>ys</code> that contain values from
the standard normal distribution and values from <code>weights</code>. If the
distribution of weights is normal, the data should match the model.</p>
</div>
<div id="analytic_birthwgt_normal" class="imageblock">
<div class="content">
<img src="figs/analytic_birthwgt_normal.png" alt="analytic birthwgt normal" height="240">
</div>
<div class="title">Figure 19. Normal probability plot of birth weights.</div>
</div>
<div class="paragraph">
<p><a href="#analytic_birthwgt_normal">Figure 19</a> shows
the results for all live births, and also for full term births
(pregnancy length greater than 36 weeks). Both curves match the model
near the mean and deviate in the tails. The heaviest babies are heavier
than what the model expects, and the lightest babies are lighter.</p>
</div>
<div class="paragraph">
<p>When we select only full term births, we remove some of the lightest
weights, which reduces the discrepancy in the lower tail of the
distribution.</p>
</div>
<div class="paragraph">
<p>This plot suggests that the normal model describes the distribution well
within a few standard deviations from the mean, but not in the tails.
Whether it is good enough for practical purposes depends on the
purposes.</p>
</div>
</div>
<div class="sect2">
<h3 id="lognormal"><a class="anchor" href="#lognormal"></a><a class="link" href="#lognormal">5.4. The lognormal distribution</a></h3>
<div class="paragraph">
<p>If the logarithms of a set of values have a normal distribution, the
values have a <strong>lognormal distribution</strong>. The CDF of the lognormal
distribution is the same as the CDF of the normal distribution, with
\(\log x\) substituted for \(x\).</p>
</div>
<div class="stemblock">
<div class="content">
\[CDF_{lognormal}(x) = CDF_{normal}(\log x)\]
</div>
</div>
<div class="paragraph">
<p>The parameters of the lognormal distribution are usually denoted
\(\mu\) and \(\sigma\). But remember that these
parameters are <em>not</em> the mean and standard deviation; the mean of a
lognormal distribution is \(\exp(\mu +\sigma^2/2)\) and the
standard deviation is ugly (see
<a href="http://wikipedia.org/wiki/Log-normal_distribution" class="bare">http://wikipedia.org/wiki/Log-normal_distribution</a>).</p>
</div>
<div id="brfss_weight" class="imageblock">
<div class="content">
<img src="figs/brfss_weight.png" alt="brfss weight" height="240">
</div>
<div class="title">Figure 20. CDF of adult weights on a linear scale (left) and log scale (right).</div>
</div>
<div class="paragraph">
<p>If a sample is approximately lognormal and you plot its CDF on a log-x
scale, it will have the characteristic shape of a normal distribution.
To test how well the sample fits a lognormal model, you can make a
normal probability plot using the log of the values in the sample.</p>
</div>
<div class="paragraph">
<p>As an example, let’s look at the distribution of adult weights, which is
approximately lognormal.<sup class="footnote">[<a id="_footnoteref_2" class="footnote" href="#_footnote_2" title="View footnote.">2</a>]</sup></p>
</div>
<div class="paragraph">
<p>The National Center for Chronic Disease Prevention and Health Promotion
conducts an annual survey as part of the Behavioral Risk Factor
Surveillance System (BRFSS).<sup class="footnote">[<a id="_footnoteref_3" class="footnote" href="#_footnote_3" title="View footnote.">3</a>]</sup> In 2008, they
interviewed 414,509 respondents and asked about their demographics,
health, and health risks. Among the data they collected are the weights
in kilograms of 398,484 respondents.</p>
</div>
<div class="paragraph">
<p>The repository for this book contains <code>CDBRFS08.ASC.gz</code>, a fixed-width
ASCII file that contains data from the BRFSS, and <code>brfss.py</code>, which
reads the file and analyzes the data.</p>
</div>
<div id="brfss_weight_normal" class="imageblock">
<div class="content">
<img src="figs/brfss_weight_normal.png" alt="brfss weight normal" height="240">
</div>
<div class="title">Figure 21. Normal probability plots for adult weight on a linear scale (left) and log scale (right).</div>
</div>
<div class="paragraph">
<p><a href="#brfss_weight">Figure 20</a> (left) shows the distribution
of adult weights on a linear scale with a normal model.
<a href="#brfss_weight">Figure 20</a> (right) shows the same
distribution on a log scale with a lognormal model. The lognormal model
is a better fit, but this representation of the data does not make the
difference particularly dramatic.</p>
</div>
<div class="paragraph">
<p><a href="#brfss_weight_normal">Figure 21</a> shows normal
probability plots for adult weights, \(w\), and for their
logarithms, \(\log_{10} w\). Now it is apparent that the data
deviate substantially from the normal model. On the other hand, the
lognormal model is a good match for the data.</p>
</div>
</div>
<div class="sect2">
<h3 id="_the_pareto_distribution"><a class="anchor" href="#_the_pareto_distribution"></a><a class="link" href="#_the_pareto_distribution">5.5. The Pareto distribution</a></h3>
<div class="paragraph">
<p>The <strong>Pareto distribution</strong> is named after the economist Vilfredo Pareto,
who used it to describe the distribution of wealth (see
<a href="http://wikipedia.org/wiki/Pareto_distribution" class="bare">http://wikipedia.org/wiki/Pareto_distribution</a>). Since then, it has been
used to describe phenomena in the natural and social sciences including
sizes of cities and towns, sand particles and meteorites, forest fires
and earthquakes.</p>
</div>
<div class="paragraph">
<p>The CDF of the Pareto distribution is:</p>
</div>
<div class="stemblock">
<div class="content">
\[CDF(x) = 1 - \left( \frac{x}{x_m} \right) ^{-\alpha}\]
</div>
</div>
<div class="paragraph">
<p>The parameters \(x_{m}\) and \(\alpha\) determine the
location and shape of the distribution. \(x_{m}\) is the minimum
possible value. <a href="#analytic_pareto_cdf">Figure 22</a>
shows CDFs of Pareto distributions with \(x_{m} = 0.5\) and
different values of \(\alpha\).</p>
</div>
<div id="analytic_pareto_cdf" class="imageblock">
<div class="content">
<img src="figs/analytic_pareto_cdf.png" alt="analytic pareto cdf" height="240">
</div>
<div class="title">Figure 22. CDFs of Pareto distributions with different parameters.</div>
</div>
<div class="paragraph">
<p>There is a simple visual test that indicates whether an empirical
distribution fits a Pareto distribution: on a log-log scale, the CCDF
looks like a straight line. Let’s see why that works.</p>
</div>
<div class="paragraph">
<p>If you plot the CCDF of a sample from a Pareto distribution on a linear
scale, you expect to see a function like:</p>
</div>
<div class="stemblock">
<div class="content">
\[y \approx \left( \frac{x}{x_m} \right) ^{-\alpha}\]
</div>
</div>
<div class="paragraph">
<p>Taking the log of both sides yields:</p>
</div>
<div class="stemblock">
<div class="content">
\[\log y \approx -\alpha (\log x - \log x_{m})\]
</div>
</div>
<div class="paragraph">
<p>So if you plot \(\log y\) versus \(\log x\), it should
look like a straight line with slope \(-\alpha\) and intercept
\(\alpha \log x_{m}\).</p>
</div>
<div class="paragraph">
<p>As an example, let’s look at the sizes of cities and towns. The
U.S. Census Bureau publishes the population of every incorporated city
and town in the United States.</p>
</div>
<div id="populations_pareto" class="imageblock">
<div class="content">
<img src="figs/populations_pareto.png" alt="populations pareto" height="240">
</div>
<div class="title">Figure 23. CCDFs of city and town populations, on a log-log scale.</div>
</div>
<div class="paragraph">
<p>I downloaded their data from
<a href="http://www.census.gov/popest/data/cities/totals/2012/SUB-EST2012-3.html" class="bare">http://www.census.gov/popest/data/cities/totals/2012/SUB-EST2012-3.html</a>;
it is in the repository for this book in a file named
<code>PEP_2012_PEPANNRES_with_ann.csv</code>. The repository also contains
<code>populations.py</code>, which reads the file and plots the distribution of
populations.</p>
</div>
<div class="paragraph">
<p><a href="#populations_pareto">Figure 23</a> shows the CCDF of
populations on a log-log scale. The largest 1% of cities and towns,
below \(10^{-2}\), fall along a straight line. So we could
conclude, as some researchers have, that the tail of this distribution
fits a Pareto model.</p>
</div>
<div class="paragraph">
<p>On the other hand, a lognormal distribution also models the data well.
<a href="#populations_normal">Figure 24</a> shows the CDF of
populations and a lognormal model (left), and a normal probability plot
(right). Both plots show good agreement between the data and the model.</p>
</div>
<div class="paragraph">
<p>Neither model is perfect. The Pareto model only applies to the largest
1% of cities, but it is a better fit for that part of the distribution.
The lognormal model is a better fit for the other 99%. Which model is
appropriate depends on which part of the distribution is relevant.</p>
</div>
<div id="populations_normal" class="imageblock">
<div class="content">
<img src="figs/populations_normal.png" alt="populations normal" height="240">
</div>
<div class="title">Figure 24. CDF of city and town populations on a log-x scale (left), and normal probability plot of log-transformed populations (right).</div>
</div>
</div>
<div class="sect2">
<h3 id="_generating_random_numbers"><a class="anchor" href="#_generating_random_numbers"></a><a class="link" href="#_generating_random_numbers">5.6. Generating random numbers</a></h3>
<div class="paragraph">
<p>Analytic CDFs can be used to generate random numbers with a given
distribution function, \(p = \mathrm{CDF}(x)\). If there is an
efficient way to compute the inverse CDF, we can generate random values
with the appropriate distribution by choosing \(p\) from a
uniform distribution between 0 and 1, then choosing
\(x = ICDF(p)\).</p>
</div>
<div class="paragraph">
<p>For example, the CDF of the exponential distribution is</p>
</div>
<div class="stemblock">
<div class="content">
\[p = 1 - e^{-\lambda x}\]
</div>
</div>
<div class="paragraph">
<p>Solving for \(x\) yields:</p>
</div>
<div class="stemblock">
<div class="content">
\[x = -\log (1 - p) / \lambda\]
</div>
</div>
<div class="paragraph">
<p>So in Python we can write</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def expovariate(lam):
    p = random.random()
    x = -math.log(1-p) / lam
    return x</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>expovariate</code> takes <code>lam</code> and returns a random value chosen from the
exponential distribution with parameter <code>lam</code>.</p>
</div>
<div class="paragraph">
<p>Two notes about this implementation: I called the parameter <code>lam</code>
because <code>lambda</code> is a Python keyword. Also, since \(\log 0\)
is undefined, we have to be a little careful. The implementation of
<code>random.random</code> can return 0 but not 1, so \(1 - p\) can be 1
but not 0, so <code>log(1-p)</code> is always defined.</p>
</div>
</div>
<div class="sect2">
<h3 id="_why_model"><a class="anchor" href="#_why_model"></a><a class="link" href="#_why_model">5.7. Why model?</a></h3>
<div class="paragraph">
<p>At the beginning of this chapter, I said that many real world phenomena
can be modeled with analytic distributions. &#8220;So,&#8221; you might ask,
&#8220;what?&#8221;</p>
</div>
<div class="paragraph">
<p>Like all models, analytic distributions are abstractions, which means
they leave out details that are considered irrelevant. For example, an
observed distribution might have measurement errors or quirks that are
specific to the sample; analytic models smooth out these idiosyncrasies.</p>
</div>
<div class="paragraph">
<p>Analytic models are also a form of data compression. When a model fits a
dataset well, a small set of parameters can summarize a large amount of
data.</p>
</div>
<div class="paragraph">
<p>It is sometimes surprising when data from a natural phenomenon fit an
analytic distribution, but these observations can provide insight into
physical systems. Sometimes we can explain why an observed distribution
has a particular form. For example, Pareto distributions are often the
result of generative processes with positive feedback (so-called
preferential attachment processes: see
<a href="http://wikipedia.org/wiki/Preferential_attachment." class="bare">http://wikipedia.org/wiki/Preferential_attachment.</a>).</p>
</div>
<div class="paragraph">
<p>Also, analytic distributions lend themselves to mathematical analysis,
as we will see in <a href="#analysis">Chapter 14</a>.</p>
</div>
<div class="paragraph">
<p>But it is important to remember that all models are imperfect. Data from
the real world never fit an analytic distribution perfectly. People
sometimes talk as if data are generated by models; for example, they
might say that the distribution of human heights is normal, or the
distribution of income is lognormal. Taken literally, these claims
cannot be true; there are always differences between the real world and
mathematical models.</p>
</div>
<div class="paragraph">
<p>Models are useful if they capture the relevant aspects of the real world
and leave out unneeded details. But what is &#8220;relevant&#8221; or &#8220;unneeded&#8221;
depends on what you are planning to use the model for.</p>
</div>
</div>
<div class="sect2">
<h3 id="_exercises_5"><a class="anchor" href="#_exercises_5"></a><a class="link" href="#_exercises_5">5.8. Exercises</a></h3>
<div class="paragraph">
<p></p>
</div>
<div class="paragraph">
<p>For the following exercises, you can start with <code>chap05ex.ipynb</code>. My
solution is in <code>chap05soln.ipynb</code>.</p>
</div>
<div class="paragraph">
<p><strong>Exercise 5.1</strong></p>
</div>
<div class="paragraph">
<p>In the BRFSS (see <a href="#lognormal">Section 5.4</a>), the
distribution of heights is roughly normal with parameters
\(\mu = 178\) cm and \(\sigma = 7.7\) cm for men, and
\(\mu = 163\) cm and \(\sigma = 7.3\) cm for women.</p>
</div>
<div class="paragraph">
<p>In order to join Blue Man Group, you have to be male between 5’10” and
6’1” (see <a href="http://bluemancasting.com" class="bare">http://bluemancasting.com</a>). What percentage of the U.S. male
population is in this range? Hint: use <code>scipy.stats.norm.cdf</code>.</p>
</div>
<div class="paragraph">
<p><strong>Exercise 5.2</strong></p>
</div>
<div class="paragraph">
<p>To get a feel for the Pareto distribution, let’s see how different the
world would be if the distribution of human height were Pareto. With the
parameters \(x_{m} = 1\) m and \(\alpha = 1.7\), we get
a distribution with a reasonable minimum, 1 m, and median, 1.5 m.</p>
</div>
<div class="paragraph">
<p>Plot this distribution. What is the mean human height in Pareto world?
What fraction of the population is shorter than the mean? If there are 7
billion people in Pareto world, how many do we expect to be taller than
1 km? How tall do we expect the tallest person to be?</p>
</div>
<div class="paragraph">
<p><strong>Exercise 5.3</strong></p>
</div>
<div id="weibull" class="paragraph">
<p>The Weibull distribution is a generalization of the exponential
distribution that comes up in failure analysis (see
<a href="http://wikipedia.org/wiki/Weibull_distribution" class="bare">http://wikipedia.org/wiki/Weibull_distribution</a>). Its CDF is</p>
</div>
<div class="stemblock">
<div class="content">
\[CDF(x) = 1 - e^{-(x / \lambda)^k}\]
</div>
</div>
<div class="paragraph">
<p>Can you find a transformation that makes a Weibull distribution look
like a straight line? What do the slope and intercept of the line
indicate?</p>
</div>
<div class="paragraph">
<p>Use <code>random.weibullvariate</code> to generate a sample from a Weibull
distribution and use it to test your transformation.</p>
</div>
<div class="paragraph">
<p><strong>Exercise 5.4</strong></p>
</div>
<div class="paragraph">
<p>For small values of \(n\), we don’t expect an empirical
distribution to fit an analytic distribution exactly. One way to
evaluate the quality of fit is to generate a sample from an analytic
distribution and see how well it matches the data.</p>
</div>
<div class="paragraph">
<p>For example, in <a href="#exponential">Section 5.1</a> we plotted the
distribution of time between births and saw that it is approximately
exponential. But the distribution is based on only 44 data points. To
see whether the data might have come from an exponential distribution,
generate 44 values from an exponential distribution with the same mean
as the data, about 33 minutes between births.</p>
</div>
<div class="paragraph">
<p>Plot the distribution of the random values and compare it to the actual
distribution. You can use <code>random.expovariate</code> to generate the values.</p>
</div>
<div class="paragraph">
<p><strong>Exercise 5.5</strong></p>
</div>
<div class="paragraph">
<p>In the repository for this book, you’ll find a set of data files called
<code>mystery0.dat</code>, <code>mystery1.dat</code>, and so on. Each contains a sequence
of random numbers generated from an analytic distribution.</p>
</div>
<div class="paragraph">
<p>You will also find <code>test_models.py</code>, a script that reads data from a
file and plots the CDF under a variety of transforms. You can run it
like this:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>$ python test_models.py mystery0.dat</pre>
</div>
</div>
<div class="paragraph">
<p>Based on these plots, you should be able to infer what kind of
distribution generated each file. If you are stumped, you can look in
<code>mystery.py</code>, which contains the code that generated the files.</p>
</div>
<div class="paragraph">
<p><strong>Exercise 5.6</strong></p>
</div>
<div id="income" class="paragraph">
<p>The distributions of wealth and income are sometimes modeled using
lognormal and Pareto distributions. To see which is better, let’s look
at some data.</p>
</div>
<div class="paragraph">
<p>The Current Population Survey (CPS) is a joint effort of the Bureau of
Labor Statistics and the Census Bureau to study income and related
variables. Data collected in 2013 is available from
<a href="http://www.census.gov/hhes/www/cpstables/032013/hhinc/toc.htm" class="bare">http://www.census.gov/hhes/www/cpstables/032013/hhinc/toc.htm</a>. I
downloaded <code>hinc06.xls</code>, which is an Excel spreadsheet with
information about household income, and converted it to <code>hinc06.csv</code>,
a CSV file you will find in the repository for this book. You will also
find <code>hinc.py</code>, which reads this file.</p>
</div>
<div class="paragraph">
<p>Extract the distribution of incomes from this dataset. Are any of the
analytic distributions in this chapter a good model of the data? A
solution to this exercise is in <code>hinc_soln.py</code>.</p>
</div>
</div>
<div class="sect2">
<h3 id="_glossary_5"><a class="anchor" href="#_glossary_5"></a><a class="link" href="#_glossary_5">5.9. Glossary</a></h3>
<div class="ulist">
<ul>
<li>
<p><strong>empirical distribution</strong>: The distribution of values in a sample.</p>
</li>
<li>
<p><strong>analytic distribution</strong>: A distribution whose CDF is an analytic
function.</p>
</li>
<li>
<p><strong>model</strong>: A useful simplification. Analytic distributions are often
good models of more complex empirical distributions.</p>
</li>
<li>
<p><strong>interarrival time</strong>: The elapsed time between two events.</p>
</li>
<li>
<p><strong>complementary CDF</strong>: A function that maps from a value, \(x\),
to the fraction of values that exceed \(x\), which is
\(1 - \mathrm{CDF}(x)\).</p>
</li>
<li>
<p><strong>standard normal distribution</strong>: The normal distribution with mean 0
and standard deviation 1.</p>
</li>
<li>
<p><strong>normal probability plot</strong>: A plot of the values in a sample versus
random values from a standard normal distribution.</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="density"><a class="anchor" href="#density"></a><a class="link" href="#density">6. Probability density functions</a></h2>
<div class="sectionbody">
<div class="paragraph">
<p>The code for this chapter is in <code>density.py</code>. For information about
downloading and working with this code, see <a href="#code">Using the code</a>.</p>
</div>
<div class="sect2">
<h3 id="_pdfs"><a class="anchor" href="#_pdfs"></a><a class="link" href="#_pdfs">6.1. PDFs</a></h3>
<div class="paragraph">
<p>The derivative of a CDF is called a <strong>probability density function</strong>, or
PDF. For example, the PDF of an exponential distribution is</p>
</div>
<div class="stemblock">
<div class="content">
\[\mathrm{PDF}_{expo}(x) = \lambda e^{-\lambda x}\]
</div>
</div>
<div class="paragraph">
<p>The PDF of a normal distribution is</p>
</div>
<div class="stemblock">
<div class="content">
\[\mathrm{PDF}_{normal}(x) = \frac{1}{\sigma \sqrt{2 \pi}}
                 \exp \left[ -\frac{1}{2}
                 \left( \frac{x - \mu}{\sigma} \right)^2 \right]\]
</div>
</div>
<div class="paragraph">
<p>Evaluating a PDF for a particular value of \(x\) is usually not
useful. The result is not a probability; it is a probability <em>density</em>.</p>
</div>
<div class="paragraph">
<p>In physics, density is mass per unit of volume; in order to get a mass,
you have to multiply by volume or, if the density is not constant, you
have to integrate over volume.</p>
</div>
<div class="paragraph">
<p>Similarly, <strong>probability density</strong> measures probability per unit of
\(x\). In order to get a probability mass, you have to integrate
over \(x\).</p>
</div>
<div class="paragraph">
<p><code>thinkstats2</code> provides a class called Pdf that represents a
probability density function. Every Pdf object provides the following
methods:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>Density</code>, which takes a value, <code>x</code>, and returns the density of
the distribution at <code>x</code>.</p>
</li>
<li>
<p><code>Render</code>, which evaluates the density at a discrete set of values
and returns a pair of sequences: the sorted values, <code>xs</code>, and their
probability densities, <code>ds</code>.</p>
</li>
<li>
<p><code>MakePmf</code>, which evaluates <code>Density</code> at a discrete set of values
and returns a normalized Pmf that approximates the Pdf.</p>
</li>
<li>
<p><code>GetLinspace</code>, which returns the default set of points used by
<code>Render</code> and <code>MakePmf</code>.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Pdf is an abstract parent class, which means you should not instantiate
it; that is, you cannot create a Pdf object. Instead, you should define
a child class that inherits from Pdf and provides definitions of
<code>Density</code> and <code>GetLinspace</code>. Pdf provides <code>Render</code> and
<code>MakePmf</code>.</p>
</div>
<div class="paragraph">
<p>For example, <code>thinkstats2</code> provides a class named <code>NormalPdf</code> that
evaluates the normal density function.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">class NormalPdf(Pdf):

    def __init__(self, mu=0, sigma=1, label=''):
        self.mu = mu
        self.sigma = sigma
        self.label = label

    def Density(self, xs):
        return scipy.stats.norm.pdf(xs, self.mu, self.sigma)

    def GetLinspace(self):
        low, high = self.mu-3*self.sigma, self.mu+3*self.sigma
        return np.linspace(low, high, 101)</code></pre>
</div>
</div>
<div class="paragraph">
<p>The NormalPdf object contains the parameters <code>mu</code> and <code>sigma</code>.
<code>Density</code> uses <code>scipy.stats.norm</code>, which is an object that
represents a normal distribution and provides <code>cdf</code> and <code>pdf</code>, among
other methods (see <a href="#normal">Section 5.2</a>).</p>
</div>
<div class="paragraph">
<p>The following example creates a NormalPdf with the mean and variance of
adult female heights, in cm, from the BRFSS (see
<a href="#lognormal">Section 5.4</a>). Then it computes the density of the
distribution at a location one standard deviation from the mean.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; mean, var = 163, 52.8
&gt;&gt;&gt; std = math.sqrt(var)
&gt;&gt;&gt; pdf = thinkstats2.NormalPdf(mean, std)
&gt;&gt;&gt; pdf.Density(mean + std)
0.0333001</pre>
</div>
</div>
<div class="paragraph">
<p>The result is about 0.03, in units of probability mass per cm. Again, a
probability density doesn’t mean much by itself. But if we plot the Pdf,
we can see the shape of the distribution:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; thinkplot.Pdf(pdf, label='normal')
&gt;&gt;&gt; thinkplot.Show()</pre>
</div>
</div>
<div class="paragraph">
<p><code>thinkplot.Pdf</code> plots the Pdf as a smooth function, as contrasted with
<code>thinkplot.Pmf</code>, which renders a Pmf as a step function.
<a href="#pdf_example">Figure 25</a> shows the result, as well as a
PDF estimated from a sample, which we’ll compute in the next section.</p>
</div>
<div class="paragraph">
<p>You can use <code>MakePmf</code> to approximate the Pdf:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; pmf = pdf.MakePmf()</pre>
</div>
</div>
<div class="paragraph">
<p>By default, the resulting Pmf contains 101 points equally spaced from
<code>mu - 3*sigma</code> to <code>mu + 3*sigma</code>. Optionally, <code>MakePmf</code> and
<code>Render</code> can take keyword arguments <code>low</code>, <code>high</code>, and <code>n</code>.</p>
</div>
<div id="pdf_example" class="imageblock">
<div class="content">
<img src="figs/pdf_example.png" alt="pdf example" height="211">
</div>
<div class="title">Figure 25. A normal PDF that models adult female height in the U.S., and the kernel density estimate of a sample with \(n=500\).</div>
</div>
</div>
<div class="sect2">
<h3 id="_kernel_density_estimation"><a class="anchor" href="#_kernel_density_estimation"></a><a class="link" href="#_kernel_density_estimation">6.2. Kernel density estimation</a></h3>
<div class="paragraph">
<p><strong>Kernel density estimation</strong> (KDE) is an algorithm that takes a sample
and finds an appropriately smooth PDF that fits the data. You can read
details at <a href="http://en.wikipedia.org/wiki/Kernel_density_estimation" class="bare">http://en.wikipedia.org/wiki/Kernel_density_estimation</a>.</p>
</div>
<div class="paragraph">
<p><code>scipy</code> provides an implementation of KDE and <code>thinkstats2</code> provides
a class called <code>EstimatedPdf</code> that uses it:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">class EstimatedPdf(Pdf):

    def __init__(self, sample):
        self.kde = scipy.stats.gaussian_kde(sample)

    def Density(self, xs):
        return self.kde.evaluate(xs)</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>__init__</code> takes a sample and computes a kernel density estimate. The
result is a <code>gaussian_kde</code> object that provides an <code>evaluate</code>
method.</p>
</div>
<div class="paragraph">
<p><code>Density</code> takes a value or sequence, calls <code>gaussian_kde.evaluate</code>,
and returns the resulting density. The word &#8220;Gaussian&#8221; appears in the
name because it uses a filter based on a Gaussian distribution to smooth
the KDE.</p>
</div>
<div class="paragraph">
<p>Here’s an example that generates a sample from a normal distribution and
then makes an EstimatedPdf to fit it:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; sample = [random.gauss(mean, std) for i in range(500)]
&gt;&gt;&gt; sample_pdf = thinkstats2.EstimatedPdf(sample)
&gt;&gt;&gt; thinkplot.Pdf(sample_pdf, label='sample KDE')</pre>
</div>
</div>
<div class="paragraph">
<p><code>sample</code> is a list of 500 random heights. <code>sample_pdf</code> is a Pdf
object that contains the estimated KDE of the sample.</p>
</div>
<div class="paragraph">
<p><a href="#pdf_example">Figure 25</a> shows the normal density
function and a KDE based on a sample of 500 random heights. The estimate
is a good match for the original distribution.</p>
</div>
<div class="paragraph">
<p>Estimating a density function with KDE is useful for several purposes:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><em>Visualization:</em> During the exploration phase of a project, CDFs are
usually the best visualization of a distribution. After you look at a
CDF, you can decide whether an estimated PDF is an appropriate model of
the distribution. If so, it can be a better choice for presenting the
distribution to an audience that is unfamiliar with CDFs.</p>
</li>
<li>
<p><em>Interpolation:</em> An estimated PDF is a way to get from a sample to a
model of the population. If you have reason to believe that the
population distribution is smooth, you can use KDE to interpolate the
density for values that don’t appear in the sample.</p>
</li>
<li>
<p><em>Simulation:</em> Simulations are often based on the distribution of a
sample. If the sample size is small, it might be appropriate to smooth
the sample distribution using KDE, which allows the simulation to
explore more possible outcomes, rather than replicating the observed
data.</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_the_distribution_framework"><a class="anchor" href="#_the_distribution_framework"></a><a class="link" href="#_the_distribution_framework">6.3. The distribution framework</a></h3>
<div id="dist_framework" class="imageblock">
<div class="content">
<img src="figs/distribution_functions.png" alt="distribution functions" height="211">
</div>
<div class="title">Figure 26. A framework that relates representations of distribution functions.</div>
</div>
<div class="paragraph">
<p>At this point we have seen PMFs, CDFs and PDFs; let’s take a minute to
review. <a href="#dist_framework">Figure 26</a> shows how these
functions relate to each other.</p>
</div>
<div class="paragraph">
<p>We started with PMFs, which represent the probabilities for a discrete
set of values. To get from a PMF to a CDF, you add up the probability
masses to get cumulative probabilities. To get from a CDF back to a PMF,
you compute differences in cumulative probabilities. We’ll see the
implementation of these operations in the next few sections.</p>
</div>
<div class="paragraph">
<p>A PDF is the derivative of a continuous CDF; or, equivalently, a CDF is
the integral of a PDF. Remember that a PDF maps from values to
probability densities; to get a probability, you have to integrate.</p>
</div>
<div class="paragraph">
<p>To get from a discrete to a continuous distribution, you can perform
various kinds of smoothing. One form of smoothing is to assume that the
data come from an analytic continuous distribution (like exponential or
normal) and to estimate the parameters of that distribution. Another
option is kernel density estimation.</p>
</div>
<div class="paragraph">
<p>The opposite of smoothing is <strong>discretizing</strong>, or quantizing. If you
evaluate a PDF at discrete points, you can generate a PMF that is an
approximation of the PDF. You can get a better approximation using
numerical integration.</p>
</div>
<div class="paragraph">
<p>To distinguish between continuous and discrete CDFs, it might be better
for a discrete CDF to be a &#8220;cumulative mass function,&#8221; but as far as I
can tell no one uses that term.</p>
</div>
</div>
<div class="sect2">
<h3 id="_hist_implementation"><a class="anchor" href="#_hist_implementation"></a><a class="link" href="#_hist_implementation">6.4. Hist implementation</a></h3>
<div class="paragraph">
<p>At this point you should know how to use the basic types provided by
<code>thinkstats2</code>: Hist, Pmf, Cdf, and Pdf. The next few sections provide
details about how they are implemented. This material might help you use
these classes more effectively, but it is not strictly necessary.</p>
</div>
<div class="paragraph">
<p>Hist and Pmf inherit from a parent class called <code>_DictWrapper</code>. The
leading underscore indicates that this class is &#8220;internal;&#8221; that is,
it should not be used by code in other modules. The name indicates what
it is: a dictionary wrapper. Its primary attribute is <code>d</code>, the
dictionary that maps from values to their frequencies.</p>
</div>
<div class="paragraph">
<p>The values can be any hashable type. The frequencies should be integers,
but can be any numeric type.</p>
</div>
<div class="paragraph">
<p><code>_DictWrapper</code> contains methods appropriate for both Hist and Pmf,
including <code>__init__</code>, <code>Values</code>, <code>Items</code> and <code>Render</code>. It also
provides modifier methods <code>Set</code>, <code>Incr</code>, <code>Mult</code>, and <code>Remove</code>.
These methods are all implemented with dictionary operations. For
example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python"># class _DictWrapper

    def Incr(self, x, term=1):
        self.d[x] = self.d.get(x, 0) + term

    def Mult(self, x, factor):
        self.d[x] = self.d.get(x, 0) * factor

    def Remove(self, x):
        del self.d[x]</code></pre>
</div>
</div>
<div class="paragraph">
<p>Hist also provides <code>Freq</code>, which looks up the frequency of a given
value.</p>
</div>
<div class="paragraph">
<p>Because Hist operators and methods are based on dictionaries, these
methods are constant time operations; that is, their run time does not
increase as the Hist gets bigger.</p>
</div>
</div>
<div class="sect2">
<h3 id="_pmf_implementation"><a class="anchor" href="#_pmf_implementation"></a><a class="link" href="#_pmf_implementation">6.5. Pmf implementation</a></h3>
<div class="paragraph">
<p>Pmf and Hist are almost the same thing, except that a Pmf maps values to
floating-point probabilities, rather than integer frequencies. If the
sum of the probabilities is 1, the Pmf is normalized.</p>
</div>
<div class="paragraph">
<p>Pmf provides <code>Normalize</code>, which computes the sum of the probabilities
and divides through by a factor:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python"># class Pmf

    def Normalize(self, fraction=1.0):
        total = self.Total()
        if total == 0.0:
            raise ValueError('Total probability is zero.')

        factor = float(fraction) / total
        for x in self.d:
            self.d[x] *= factor

        return total</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>fraction</code> determines the sum of the probabilities after normalizing;
the default value is 1. If the total probability is 0, the Pmf cannot be
normalized, so <code>Normalize</code> raises <code>ValueError</code>.</p>
</div>
<div class="paragraph">
<p>Hist and Pmf have the same constructor. It can take as an argument a
<code>dict</code>, Hist, Pmf or Cdf, a pandas Series, a list of (value,
frequency) pairs, or a sequence of values.</p>
</div>
<div class="paragraph">
<p>If you instantiate a Pmf, the result is normalized. If you instantiate a
Hist, it is not. To construct an unnormalized Pmf, you can create an
empty Pmf and modify it. The Pmf modifiers do not renormalize the Pmf.</p>
</div>
</div>
<div class="sect2">
<h3 id="_cdf_implementation"><a class="anchor" href="#_cdf_implementation"></a><a class="link" href="#_cdf_implementation">6.6. Cdf implementation</a></h3>
<div class="paragraph">
<p>A CDF maps from values to cumulative probabilities, so I could have
implemented Cdf as a <code>_DictWrapper</code>. But the values in a CDF are
ordered and the values in a <code>_DictWrapper</code> are not. Also, it is often
useful to compute the inverse CDF; that is, the map from cumulative
probability to value. So the implementaion I chose is two sorted lists.
That way I can use binary search to do a forward or inverse lookup in
logarithmic time.</p>
</div>
<div class="paragraph">
<p>The Cdf constructor can take as a parameter a sequence of values or a
pandas Series, a dictionary that maps from values to probabilities, a
sequence of (value, probability) pairs, a Hist, Pmf, or Cdf. Or if it is
given two parameters, it treats them as a sorted sequence of values and
the sequence of corresponding cumulative probabilities.</p>
</div>
<div class="paragraph">
<p>Given a sequence, pandas Series, or dictionary, the constructor makes a
Hist. Then it uses the Hist to initialize the attributes:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">        self.xs, freqs = zip(*sorted(dw.Items()))
        self.ps = np.cumsum(freqs, dtype=np.float)
        self.ps /= self.ps[-1]</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>xs</code> is the sorted list of values; <code>freqs</code> is the list of
corresponding frequencies. <code>np.cumsum</code> computes the cumulative sum of
the frequencies. Dividing through by the total frequency yields
cumulative probabilities. For <code>n</code> values, the time to construct the
Cdf is proportional to \(n \log n\).</p>
</div>
<div class="paragraph">
<p>Here is the implementation of <code>Prob</code>, which takes a value and returns
its cumulative probability:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python"># class Cdf
    def Prob(self, x):
        if x &lt; self.xs[0]:
            return 0.0
        index = bisect.bisect(self.xs, x)
        p = self.ps[index - 1]
        return p</code></pre>
</div>
</div>
<div class="paragraph">
<p>The <code>bisect</code> module provides an implementation of binary search. And
here is the implementation of <code>Value</code>, which takes a cumulative
probability and returns the corresponding value:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python"># class Cdf
    def Value(self, p):
        if p &lt; 0 or p &gt; 1:
            raise ValueError('p must be in range [0, 1]')

        index = bisect.bisect_left(self.ps, p)
        return self.xs[index]</code></pre>
</div>
</div>
<div class="paragraph">
<p>Given a Cdf, we can compute the Pmf by computing differences between
consecutive cumulative probabilities. If you call the Cdf constructor
and pass a Pmf, it computes differences by calling <code>Cdf.Items</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python"># class Cdf
    def Items(self):
        a = self.ps
        b = np.roll(a, 1)
        b[0] = 0
        return zip(self.xs, a-b)</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>np.roll</code> shifts the elements of <code>a</code> to the right, and &#8220;rolls&#8221; the
last one back to the beginning. We replace the first element of <code>b</code>
with 0 and then compute the difference <code>a-b</code>. The result is a NumPy
array of probabilities.</p>
</div>
<div class="paragraph">
<p>Cdf provides <code>Shift</code> and <code>Scale</code>, which modify the values in the
Cdf, but the probabilities should be treated as immutable.</p>
</div>
</div>
<div class="sect2">
<h3 id="_moments"><a class="anchor" href="#_moments"></a><a class="link" href="#_moments">6.7. Moments</a></h3>
<div class="paragraph">
<p>Any time you take a sample and reduce it to a single number, that number
is a statistic. The statistics we have seen so far include mean,
variance, median, and interquartile range.</p>
</div>
<div class="paragraph">
<p>A <strong>raw moment</strong> is a kind of statistic. If you have a sample of values,
\(x_i\), the \(k\)th raw moment is:</p>
</div>
<div class="stemblock">
<div class="content">
\[m'_k = \frac{1}{n} \sum_i x_i^k\]
</div>
</div>
<div class="paragraph">
<p>Or if you prefer Python notation:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def RawMoment(xs, k):
    return sum(x**k for x in xs) / len(xs)</code></pre>
</div>
</div>
<div class="paragraph">
<p>When \(k=1\) the result is the sample mean, \(\bar{x}\).
The other raw moments don’t mean much by themselves, but they are used
in some computations.</p>
</div>
<div class="paragraph">
<p>The <strong>central moments</strong> are more useful. The \(k\)th central
moment is:</p>
</div>
<div class="stemblock">
<div class="content">
\[m_k = \frac{1}{n} \sum_i (x_i - \bar{x})^k\]
</div>
</div>
<div class="paragraph">
<p>Or in Python:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def CentralMoment(xs, k):
    mean = RawMoment(xs, 1)
    return sum((x - mean)**k for x in xs) / len(xs)</code></pre>
</div>
</div>
<div class="paragraph">
<p>When \(k=2\) the result is the second central moment, which you
might recognize as variance. The definition of variance gives a hint
about why these statistics are called moments. If we attach a weight
along a ruler at each location, \(x_i\), and then spin the ruler
around the mean, the moment of inertia of the spinning weights is the
variance of the values. If you are not familiar with moment of inertia,
see <a href="http://en.wikipedia.org/wiki/Moment_of_inertia" class="bare">http://en.wikipedia.org/wiki/Moment_of_inertia</a>.</p>
</div>
<div class="paragraph">
<p>When you report moment-based statistics, it is important to think about
the units. For example, if the values \(x_i\) are in cm, the
first raw moment is also in cm. But the second moment is in
cm\(^2\), the third moment is in cm\(^3\), and so on.</p>
</div>
<div class="paragraph">
<p>Because of these units, moments are hard to interpret by themselves.
That’s why, for the second moment, it is common to report standard
deviation, which is the square root of variance, so it is in the same
units as \(x_i\).</p>
</div>
</div>
<div class="sect2">
<h3 id="_skewness"><a class="anchor" href="#_skewness"></a><a class="link" href="#_skewness">6.8. Skewness</a></h3>
<div class="paragraph">
<p><strong>Skewness</strong> is a property that describes the shape of a distribution. If
the distribution is symmetric around its central tendency, it is
unskewed. If the values extend farther to the right, it is &#8220;right
skewed&#8221; and if the values extend left, it is &#8220;left skewed.&#8221;</p>
</div>
<div class="paragraph">
<p>This use of &#8220;skewed&#8221; does not have the usual connotation of
&#8220;biased.&#8221; Skewness only describes the shape of the distribution; it
says nothing about whether the sampling process might have been biased.</p>
</div>
<div class="paragraph">
<p>Several statistics are commonly used to quantify the skewness of a
distribution. Given a sequence of values, \(x_i\), the <strong>sample
skewness</strong>, \(g_1\), can be computed like this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def StandardizedMoment(xs, k):
    var = CentralMoment(xs, 2)
    std = math.sqrt(var)
    return CentralMoment(xs, k) / std**k

def Skewness(xs):
    return StandardizedMoment(xs, 3)</code></pre>
</div>
</div>
<div class="paragraph">
<p>\(g_1\) is the third <strong>standardized moment</strong>, which means that it
has been normalized so it has no units.</p>
</div>
<div class="paragraph">
<p>Negative skewness indicates that a distribution skews left; positive
skewness indicates that a distribution skews right. The magnitude of
\(g_1\) indicates the strength of the skewness, but by itself it
is not easy to interpret.</p>
</div>
<div class="paragraph">
<p>In practice, computing sample skewness is usually not a good idea. If
there are any outliers, they have a disproportionate effect on
\(g_1\).</p>
</div>
<div class="paragraph">
<p>Another way to evaluate the asymmetry of a distribution is to look at
the relationship between the mean and median. Extreme values have more
effect on the mean than the median, so in a distribution that skews
left, the mean is less than the median. In a distribution that skews
right, the mean is greater.</p>
</div>
<div class="paragraph">
<p><strong>Pearson’s median skewness coefficient</strong> is a measure of skewness based
on the difference between the sample mean and median:</p>
</div>
<div class="stemblock">
<div class="content">
\[g_p = 3 (\bar{x}- m) / S\]
</div>
</div>
<div class="paragraph">
<p>Where \(\bar{x}\) is the sample mean, \(m\) is the
median, and \(S\) is the standard deviation. Or in Python:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def Median(xs):
    cdf = thinkstats2.Cdf(xs)
    return cdf.Value(0.5)

def PearsonMedianSkewness(xs):
    median = Median(xs)
    mean = RawMoment(xs, 1)
    var = CentralMoment(xs, 2)
    std = math.sqrt(var)
    gp = 3 * (mean - median) / std
    return gp</code></pre>
</div>
</div>
<div class="paragraph">
<p>This statistic is <strong>robust</strong>, which means that it is less vulnerable to
the effect of outliers.</p>
</div>
<div id="density_totalwgt_kde" class="imageblock">
<div class="content">
<img src="figs/density_totalwgt_kde.png" alt="density totalwgt kde" height="211">
</div>
<div class="title">Figure 27. Estimated PDF of birthweight data from the NSFG.</div>
</div>
<div class="paragraph">
<p>As an example, let’s look at the skewness of birth weights in the NSFG
pregnancy data. Here’s the code to estimate and plot the PDF:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    live, firsts, others = first.MakeFrames()
    data = live.totalwgt_lb.dropna()
    pdf = thinkstats2.EstimatedPdf(data)
    thinkplot.Pdf(pdf, label='birth weight')</code></pre>
</div>
</div>
<div class="paragraph">
<p><a href="#density_totalwgt_kde">Figure 27</a> shows the
result. The left tail appears longer than the right, so we suspect the
distribution is skewed left. The mean, 7.27 lbs, is a bit less than the
median, 7.38 lbs, so that is consistent with left skew. And both
skewness coefficients are negative: sample skewness is -0.59; Pearson’s
median skewness is -0.23.</p>
</div>
<div id="density_wtkg2_kde" class="imageblock">
<div class="content">
<img src="figs/density_wtkg2_kde.png" alt="density wtkg2 kde" height="211">
</div>
<div class="title">Figure 28. Estimated PDF of adult weight data from the BRFSS.</div>
</div>
<div class="paragraph">
<p>Now let’s compare this distribution to the distribution of adult weight
in the BRFSS. Again, here’s the code:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    df = brfss.ReadBrfss(nrows=None)
    data = df.wtkg2.dropna()
    pdf = thinkstats2.EstimatedPdf(data)
    thinkplot.Pdf(pdf, label='adult weight')</code></pre>
</div>
</div>
<div class="paragraph">
<p><a href="#density_wtkg2_kde">Figure 28</a> shows the result.
The distribution appears skewed to the right. Sure enough, the mean,
79.0, is bigger than the median, 77.3. The sample skewness is 1.1 and
Pearson’s median skewness is 0.26.</p>
</div>
<div class="paragraph">
<p>The sign of the skewness coefficient indicates whether the distribution
skews left or right, but other than that, they are hard to interpret.
Sample skewness is less robust; that is, it is more susceptible to
outliers. As a result it is less reliable when applied to skewed
distributions, exactly when it would be most relevant.</p>
</div>
<div class="paragraph">
<p>Pearson’s median skewness is based on a computed mean and variance, so
it is also susceptible to outliers, but since it does not depend on a
third moment, it is somewhat more robust.</p>
</div>
</div>
<div class="sect2">
<h3 id="_exercises_6"><a class="anchor" href="#_exercises_6"></a><a class="link" href="#_exercises_6">6.9. Exercises</a></h3>
<div class="paragraph">
<p></p>
</div>
<div class="paragraph">
<p>A solution to this exercise is in <code>chap06soln.py</code>.</p>
</div>
<div class="paragraph">
<p><strong>Exercise 6.1</strong></p>
</div>
<div class="paragraph">
<p>The distribution of income is famously skewed to the right. In this
exercise, we’ll measure how strong that skew is.</p>
</div>
<div class="paragraph">
<p>The Current Population Survey (CPS) is a joint effort of the Bureau of
Labor Statistics and the Census Bureau to study income and related
variables. Data collected in 2013 is available from
<a href="http://www.census.gov/hhes/www/cpstables/032013/hhinc/toc.htm" class="bare">http://www.census.gov/hhes/www/cpstables/032013/hhinc/toc.htm</a>. I
downloaded <code>hinc06.xls</code>, which is an Excel spreadsheet with
information about household income, and converted it to <code>hinc06.csv</code>,
a CSV file you will find in the repository for this book. You will also
find <code>hinc2.py</code>, which reads this file and transforms the data.</p>
</div>
<div class="paragraph">
<p>The dataset is in the form of a series of income ranges and the number
of respondents who fell in each range. The lowest range includes
respondents who reported annual household income &#8220;Under $5000.&#8221; The
highest range includes respondents who made &#8220;$250,000 or more.&#8221;</p>
</div>
<div class="paragraph">
<p>To estimate mean and other statistics from these data, we have to make
some assumptions about the lower and upper bounds, and how the values
are distributed in each range. <code>hinc2.py</code> provides
<code>InterpolateSample</code>, which shows one way to model this data. It takes
a DataFrame with a column, <code>income</code>, that contains the upper bound of
each range, and <code>freq</code>, which contains the number of respondents in
each frame.</p>
</div>
<div class="paragraph">
<p>It also takes <code>log_upper</code>, which is an assumed upper bound on the
highest range, expressed in <code>log10</code> dollars. The default value,
<code>log_upper=6.0</code> represents the assumption that the largest income
among the respondents is \(10^6\), or one million dollars.</p>
</div>
<div class="paragraph">
<p><code>InterpolateSample</code> generates a pseudo-sample; that is, a sample of
household incomes that yields the same number of respondents in each
range as the actual data. It assumes that incomes in each range are
equally spaced on a log10 scale.</p>
</div>
<div class="paragraph">
<p>Compute the median, mean, skewness and Pearson’s skewness of the
resulting sample. What fraction of households reports a taxable income
below the mean? How do the results depend on the assumed upper bound?</p>
</div>
</div>
<div class="sect2">
<h3 id="_glossary_6"><a class="anchor" href="#_glossary_6"></a><a class="link" href="#_glossary_6">6.10. Glossary</a></h3>
<div class="ulist">
<ul>
<li>
<p><strong>Probability density function (PDF)</strong>: The derivative of a continuous
CDF, a function that maps a value to its probability density.</p>
</li>
<li>
<p><strong>Probability density</strong>: A quantity that can be integrated over a range
of values to yield a probability. If the values are in units of cm, for
example, probability density is in units of probability per cm.</p>
</li>
<li>
<p><strong>Kernel density estimation (KDE)</strong>: An algorithm that estimates a PDF
based on a sample.</p>
</li>
<li>
<p><strong>discretize</strong>: To approximate a continuous function or distribution
with a discrete function. The opposite of smoothing.</p>
</li>
<li>
<p><strong>raw moment</strong>: A statistic based on the sum of data raised to a power.</p>
</li>
<li>
<p><strong>central moment</strong>: A statistic based on deviation from the mean, raised
to a power.</p>
</li>
<li>
<p><strong>standardized moment</strong>: A ratio of moments that has no units.</p>
</li>
<li>
<p><strong>skewness</strong>: A measure of how asymmetric a distribution is.</p>
</li>
<li>
<p><strong>sample skewness</strong>: A moment-based statistic intended to quantify the
skewness of a distribution.</p>
</li>
<li>
<p><strong>Pearson’s median skewness coefficient</strong>: A statistic intended to
quantify the skewness of a distribution based on the median, mean, and
standard deviation.</p>
</li>
<li>
<p><strong>robust</strong>: A statistic is robust if it is relatively immune to the
effect of outliers.</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_relationships_between_variables"><a class="anchor" href="#_relationships_between_variables"></a><a class="link" href="#_relationships_between_variables">7. Relationships between variables</a></h2>
<div class="sectionbody">
<div class="paragraph">
<p>So far we have only looked at one variable at a time. In this chapter we
look at relationships between variables. Two variables are related if
knowing one gives you information about the other. For example, height
and weight are related; people who are taller tend to be heavier. Of
course, it is not a perfect relationship: there are short heavy people
and tall light ones. But if you are trying to guess someone’s weight,
you will be more accurate if you know their height than if you don’t.</p>
</div>
<div class="paragraph">
<p>The code for this chapter is in <code>scatter.py</code>. For information about
downloading and working with this code, see <a href="#code">Using the code</a>.</p>
</div>
<div class="sect2">
<h3 id="_scatter_plots"><a class="anchor" href="#_scatter_plots"></a><a class="link" href="#_scatter_plots">7.1. Scatter plots</a></h3>
<div class="paragraph">
<p>The simplest way to check for a relationship between two variables is a
<strong>scatter plot</strong>, but making a good scatter plot is not always easy. As an
example, I’ll plot weight versus height for the respondents in the BRFSS
(see <a href="#lognormal">Section 5.4</a>).</p>
</div>
<div class="paragraph">
<p>Here’s the code that reads the data file and extracts height and weight:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    df = brfss.ReadBrfss(nrows=None)
    sample = thinkstats2.SampleRows(df, 5000)
    heights, weights = sample.htm3, sample.wtkg2</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>SampleRows</code> chooses a random subset of the data:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def SampleRows(df, nrows, replace=False):
    indices = np.random.choice(df.index, nrows, replace=replace)
    sample = df.loc[indices]
    return sample</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>df</code> is the DataFrame, <code>nrows</code> is the number of rows to choose, and
<code>replace</code> is a boolean indicating whether sampling should be done with
replacement; in other words, whether the same row could be chosen more
than once.</p>
</div>
<div class="paragraph">
<p><code>thinkplot</code> provides <code>Scatter</code>, which makes scatter plots:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    thinkplot.Scatter(heights, weights)
    thinkplot.Show(xlabel='Height (cm)',
                   ylabel='Weight (kg)',
                   axis=[140, 210, 20, 200])</code></pre>
</div>
</div>
<div class="paragraph">
<p>The result, in <a href="#scatter1">Figure 29</a> (left), shows the shape of the
relationship. As we expected, taller people tend to be heavier.</p>
</div>
<div id="scatter1" class="imageblock">
<div class="content">
<img src="figs/scatter1.png" alt="scatter1" height="288">
</div>
<div class="title">Figure 29. Scatter plots of weight versus height for the respondents in the BRFSS, unjittered (left), jittered (right).</div>
</div>
<div class="paragraph">
<p>But this is not the best representation of the data, because the data
are packed into columns. The problem is that the heights are rounded to
the nearest inch, converted to centimeters, and then rounded again. Some
information is lost in translation.</p>
</div>
<div class="paragraph">
<p>We can’t get that information back, but we can minimize the effect on
the scatter plot by <strong>jittering</strong> the data, which means adding random
noise to reverse the effect of rounding off. Since these measurements
were rounded to the nearest inch, they might be off by up to 0.5 inches
or 1.3 cm. Similarly, the weights might be off by 0.5 kg.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    heights = thinkstats2.Jitter(heights, 1.3)
    weights = thinkstats2.Jitter(weights, 0.5)</code></pre>
</div>
</div>
<div class="paragraph">
<p>Here’s the implementation of <code>Jitter</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def Jitter(values, jitter=0.5):
    n = len(values)
    return np.random.uniform(-jitter, +jitter, n) + values</code></pre>
</div>
</div>
<div class="paragraph">
<p>The values can be any sequence; the result is a NumPy array.</p>
</div>
<div class="paragraph">
<p><a href="#scatter1">Figure 29</a> (right) shows the result. Jittering reduces
the visual effect of rounding and makes the shape of the relationship
clearer. But in general you should only jitter data for purposes of
visualization and avoid using jittered data for analysis.</p>
</div>
<div class="paragraph">
<p>Even with jittering, this is not the best way to represent the data.
There are many overlapping points, which hides data in the dense parts
of the figure and gives disproportionate emphasis to outliers. This
effect is called <strong>saturation</strong>.</p>
</div>
<div id="scatter2" class="imageblock">
<div class="content">
<img src="figs/scatter2.png" alt="scatter2" height="288">
</div>
<div class="title">Figure 30. Scatter plot with jittering and transparency (left), hexbin plot (right).</div>
</div>
<div class="paragraph">
<p>We can solve this problem with the <code>alpha</code> parameter, which makes the
points partly transparent:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    thinkplot.Scatter(heights, weights, alpha=0.2)</code></pre>
</div>
</div>
<div class="paragraph">
<p><a href="#scatter2">Figure 30</a> (left) shows the result. Overlapping data
points look darker, so darkness is proportional to density. In this
version of the plot we can see two details that were not apparent
before: vertical clusters at several heights and a horizontal line near
90 kg or 200 pounds. Since this data is based on self-reports in pounds,
the most likely explanation is that some respondents reported rounded
values.</p>
</div>
<div class="paragraph">
<p>Using transparency works well for moderate-sized datasets, but this
figure only shows the first 5000 records in the BRFSS, out of a total of
414 509.</p>
</div>
<div class="paragraph">
<p>To handle larger datasets, another option is a hexbin plot, which
divides the graph into hexagonal bins and colors each bin according to
how many data points fall in it. <code>thinkplot</code> provides <code>HexBin</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    thinkplot.HexBin(heights, weights)</code></pre>
</div>
</div>
<div class="paragraph">
<p><a href="#scatter2">Figure 30</a> (right) shows the result. An advantage of a
hexbin is that it shows the shape of the relationship well, and it is
efficient for large datasets, both in time and in the size of the file
it generates. A drawback is that it makes the outliers invisible.</p>
</div>
<div class="paragraph">
<p>The point of this example is that it is not easy to make a scatter plot
that shows relationships clearly without introducing misleading
artifacts.</p>
</div>
</div>
<div class="sect2">
<h3 id="characterizing"><a class="anchor" href="#characterizing"></a><a class="link" href="#characterizing">7.2. Characterizing relationships</a></h3>
<div class="paragraph">
<p>Scatter plots provide a general impression of the relationship between
variables, but there are other visualizations that provide more insight
into the nature of the relationship. One option is to bin one variable
and plot percentiles of the other.</p>
</div>
<div class="paragraph">
<p>NumPy and pandas provide functions for binning data:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    df = df.dropna(subset=['htm3', 'wtkg2'])
    bins = np.arange(135, 210, 5)
    indices = np.digitize(df.htm3, bins)
    groups = df.groupby(indices)</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>dropna</code> drops rows with <code>nan</code> in any of the listed columns.
<code>arange</code> makes a NumPy array of bins from 135 to, but not including,
210, in increments of 5.</p>
</div>
<div class="paragraph">
<p><code>digitize</code> computes the index of the bin that contains each value in
<code>df.htm3</code>. The result is a NumPy array of integer indices. Values that
fall below the lowest bin are mapped to index 0. Values above the
highest bin are mapped to <code>len(bins)</code>.</p>
</div>
<div id="scatter3" class="imageblock">
<div class="content">
<img src="figs/scatter3.png" alt="scatter3" height="240">
</div>
<div class="title">Figure 31. Percentiles of weight for a range of height bins.</div>
</div>
<div class="paragraph">
<p><code>groupby</code> is a DataFrame method that returns a GroupBy object; used in
a <code>for</code> loop, <code>groups</code> iterates the names of the groups and the
DataFrames that represent them. So, for example, we can print the number
of rows in each group like this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">for i, group in groups:
    print(i, len(group))</code></pre>
</div>
</div>
<div class="paragraph">
<p>Now for each group we can compute the mean height and the CDF of weight:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    heights = [group.htm3.mean() for i, group in groups]
    cdfs = [thinkstats2.Cdf(group.wtkg2) for i, group in groups]</code></pre>
</div>
</div>
<div class="paragraph">
<p>Finally, we can plot percentiles of weight versus height:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    for percent in [75, 50, 25]:
        weights = [cdf.Percentile(percent) for cdf in cdfs]
        label = '%dth' % percent
        thinkplot.Plot(heights, weights, label=label)</code></pre>
</div>
</div>
<div class="paragraph">
<p><a href="#scatter3">Figure 31</a> shows the result. Between 140 and 200 cm the
relationship between these variables is roughly linear. This range
includes more than 99% of the data, so we don’t have to worry too much
about the extremes.</p>
</div>
</div>
<div class="sect2">
<h3 id="_correlation"><a class="anchor" href="#_correlation"></a><a class="link" href="#_correlation">7.3. Correlation</a></h3>
<div class="paragraph">
<p>A <strong>correlation</strong> is a statistic intended to quantify the strength of the
relationship between two variables.</p>
</div>
<div class="paragraph">
<p>A challenge in measuring correlation is that the variables we want to
compare are often not expressed in the same units. And even if they are
in the same units, they come from different distributions.</p>
</div>
<div class="paragraph">
<p>There are two common solutions to these problems:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Transform each value to a <strong>standard score</strong>, which is the number of
standard deviations from the mean. This transform leads to the &#8220;Pearson
product-moment correlation coefficient.&#8221;</p>
</li>
<li>
<p>Transform each value to its <strong>rank</strong>, which is its index in the sorted
list of values. This transform leads to the &#8220;Spearman rank correlation
coefficient.&#8221;</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>If \(X\) is a series of \(n\) values, \(x_i\),
we can convert to standard scores by subtracting the mean and dividing
by the standard deviation: \(z_i = (x_i - \mu) / \sigma\).</p>
</div>
<div class="paragraph">
<p>The numerator is a deviation: the distance from the mean. Dividing by
\(\sigma\) <strong>standardizes</strong> the deviation, so the values of
\(Z\) are dimensionless (no units) and their distribution has
mean 0 and variance 1.</p>
</div>
<div class="paragraph">
<p>If \(X\) is normally distributed, so is \(Z\). But if
\(X\) is skewed or has outliers, so does \(Z\); in those
cases, it is more robust to use percentile ranks. If we compute a new
variable, \(R\), so that \(r_i\) is the rank of
\(x_i\), the distribution of \(R\) is uniform from 1 to
\(n\), regardless of the distribution of \(X\).</p>
</div>
</div>
<div class="sect2">
<h3 id="_covariance"><a class="anchor" href="#_covariance"></a><a class="link" href="#_covariance">7.4. Covariance</a></h3>
<div class="paragraph">
<p><strong>Covariance</strong> is a measure of the tendency of two variables to vary
together. If we have two series, \(X\) and \(Y\), their
deviations from the mean are</p>
</div>
<div class="stemblock">
<div class="content">
\[dx_i = x_i - \bar{x}\]
</div>
</div>
<div class="stemblock">
<div class="content">
\[dy_i = y_i - \bar{y}\]
</div>
</div>
<div class="paragraph">
<p>where \(\bar{x}\) is the sample mean of \(X\) and
\(\bar{y}\) is the sample mean of \(Y\). If
\(X\) and \(Y\) vary together, their deviations tend to
have the same sign.</p>
</div>
<div class="paragraph">
<p>If we multiply them together, the product is positive when the
deviations have the same sign and negative when they have the opposite
sign. So adding up the products gives a measure of the tendency to vary
together.</p>
</div>
<div class="paragraph">
<p>Covariance is the mean of these products:</p>
</div>
<div class="stemblock">
<div class="content">
\[Cov(X,Y) = \frac{1}{n} \sum dx_i~dy_i\]
</div>
</div>
<div class="paragraph">
<p>where \(n\) is the length of the two series (they have to be the
same length).</p>
</div>
<div class="paragraph">
<p>If you have studied linear algebra, you might recognize that <code>Cov</code> is
the dot product of the deviations, divided by their length. So the
covariance is maximized if the two vectors are identical, 0 if they are
orthogonal, and negative if they point in opposite directions.
<code>thinkstats2</code> uses <code>np.dot</code> to implement <code>Cov</code> efficiently:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def Cov(xs, ys, meanx=None, meany=None):
    xs = np.asarray(xs)
    ys = np.asarray(ys)

    if meanx is None:
        meanx = np.mean(xs)
    if meany is None:
        meany = np.mean(ys)

    cov = np.dot(xs-meanx, ys-meany) / len(xs)
    return cov</code></pre>
</div>
</div>
<div class="paragraph">
<p>By default <code>Cov</code> computes deviations from the sample means, or you can
provide known means. If <code>xs</code> and <code>ys</code> are Python sequences,
<code>np.asarray</code> converts them to NumPy arrays. If they are already NumPy
arrays, <code>np.asarray</code> does nothing.</p>
</div>
<div class="paragraph">
<p>This implementation of covariance is meant to be simple for purposes of
explanation. NumPy and pandas also provide implementations of
covariance, but both of them apply a correction for small sample sizes
that we have not covered yet, and <code>np.cov</code> returns a covariance
matrix, which is more than we need for now.</p>
</div>
</div>
<div class="sect2">
<h3 id="_pearson_s_correlation"><a class="anchor" href="#_pearson_s_correlation"></a><a class="link" href="#_pearson_s_correlation">7.5. Pearson’s correlation</a></h3>
<div class="paragraph">
<p>Covariance is useful in some computations, but it is seldom reported as
a summary statistic because it is hard to interpret. Among other
problems, its units are the product of the units of \(X\) and
\(Y\). For example, the covariance of weight and height in the
BRFSS dataset is 113 kilogram-centimeters, whatever that means.</p>
</div>
<div class="paragraph">
<p>One solution to this problem is to divide the deviations by the standard
deviation, which yields standard scores, and compute the product of
standard scores:</p>
</div>
<div class="stemblock">
<div class="content">
\[p_i = \frac{(x_i - \bar{x})}{S_X} \frac{(y_i - \bar{y})}{S_Y}\]
</div>
</div>
<div class="paragraph">
<p>Where \(S_X\) and \(S_Y\) are the standard deviations of
\(X\) and \(Y\). The mean of these products is</p>
</div>
<div class="stemblock">
<div class="content">
\[\rho = \frac{1}{n} \sum p_i\]
</div>
</div>
<div class="paragraph">
<p>Or we can rewrite \(\rho\) by factoring out \(S_X\) and
\(S_Y\):</p>
</div>
<div class="stemblock">
<div class="content">
\[\rho = \frac{Cov(X,Y)}{S_X S_Y}\]
</div>
</div>
<div class="paragraph">
<p>This value is called <strong>Pearson’s correlation</strong> after Karl Pearson, an
influential early statistician. It is easy to compute and easy to
interpret. Because standard scores are dimensionless, so is
\(\rho\).</p>
</div>
<div class="paragraph">
<p>Here is the implementation in <code>thinkstats2</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def Corr(xs, ys):
    xs = np.asarray(xs)
    ys = np.asarray(ys)

    meanx, varx = MeanVar(xs)
    meany, vary = MeanVar(ys)

    corr = Cov(xs, ys, meanx, meany) / math.sqrt(varx * vary)
    return corr</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>MeanVar</code> computes mean and variance slightly more efficiently than
separate calls to <code>np.mean</code> and <code>np.var</code>.</p>
</div>
<div class="paragraph">
<p>Pearson’s correlation is always between -1 and +1 (including both). If
\(\rho\) is positive, we say that the correlation is positive,
which means that when one variable is high, the other tends to be high.
If \(\rho\) is negative, the correlation is negative, so when
one variable is high, the other is low.</p>
</div>
<div class="paragraph">
<p>The magnitude of \(\rho\) indicates the strength of the
correlation. If \(\rho\) is 1 or -1, the variables are perfectly
correlated, which means that if you know one, you can make a perfect
prediction about the other.</p>
</div>
<div class="paragraph">
<p>Most correlation in the real world is not perfect, but it is still
useful. The correlation of height and weight is 0.51, which is a strong
correlation compared to similar human-related variables.</p>
</div>
</div>
<div class="sect2">
<h3 id="_nonlinear_relationships"><a class="anchor" href="#_nonlinear_relationships"></a><a class="link" href="#_nonlinear_relationships">7.6. Nonlinear relationships</a></h3>
<div class="paragraph">
<p>If Pearson’s correlation is near 0, it is tempting to conclude that
there is no relationship between the variables, but that conclusion is
not valid. Pearson’s correlation only measures <em>linear</em> relationships.
If there’s a nonlinear relationship, \(\rho\) understates its
strength.</p>
</div>
<div id="corr_examples" class="imageblock">
<div class="content">
<img src="figs/Correlation_examples.png" alt="Correlation examples" height="240">
</div>
<div class="title">Figure 32. Examples of datasets with a range of correlations.</div>
</div>
<div class="paragraph">
<p><a href="#corr_examples">Figure 32</a> is from
<a href="http://wikipedia.org/wiki/Correlation_and_dependence" class="bare">http://wikipedia.org/wiki/Correlation_and_dependence</a>. It shows scatter
plots and correlation coefficients for several carefully constructed
datasets.</p>
</div>
<div class="paragraph">
<p>The top row shows linear relationships with a range of correlations; you
can use this row to get a sense of what different values of
\(\rho\) look like. The second row shows perfect correlations
with a range of slopes, which demonstrates that correlation is unrelated
to slope (we’ll talk about estimating slope soon). The third row shows
variables that are clearly related, but because the relationship is
nonlinear, the correlation coefficient is 0.</p>
</div>
<div class="paragraph">
<p>The moral of this story is that you should always look at a scatter plot
of your data before blindly computing a correlation coefficient.</p>
</div>
</div>
<div class="sect2">
<h3 id="_spearman_s_rank_correlation"><a class="anchor" href="#_spearman_s_rank_correlation"></a><a class="link" href="#_spearman_s_rank_correlation">7.7. Spearman’s rank correlation</a></h3>
<div class="paragraph">
<p>Pearson’s correlation works well if the relationship between variables
is linear and if the variables are roughly normal. But it is not robust
in the presence of outliers. Spearman’s rank correlation is an
alternative that mitigates the effect of outliers and skewed
distributions. To compute Spearman’s correlation, we have to compute the
<strong>rank</strong> of each value, which is its index in the sorted sample. For
example, in the sample <code>[1, 2, 5, 7]</code> the rank of the value 5 is 3,
because it appears third in the sorted list. Then we compute Pearson’s
correlation for the ranks.</p>
</div>
<div class="paragraph">
<p><code>thinkstats2</code> provides a function that computes Spearman’s rank
correlation:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def SpearmanCorr(xs, ys):
    xranks = pandas.Series(xs).rank()
    yranks = pandas.Series(ys).rank()
    return Corr(xranks, yranks)</code></pre>
</div>
</div>
<div class="paragraph">
<p>I convert the arguments to pandas Series objects so I can use <code>rank</code>,
which computes the rank for each value and returns a Series. Then I use
<code>Corr</code> to compute the correlation of the ranks.</p>
</div>
<div class="paragraph">
<p>I could also use <code>Series.corr</code> directly and specify Spearman’s method:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def SpearmanCorr(xs, ys):
    xs = pandas.Series(xs)
    ys = pandas.Series(ys)
    return xs.corr(ys, method='spearman')</code></pre>
</div>
</div>
<div class="paragraph">
<p>The Spearman rank correlation for the BRFSS data is 0.54, a little
higher than the Pearson correlation, 0.51. There are several possible
reasons for the difference, including:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>If the relationship is nonlinear, Pearson’s correlation tends to
underestimate the strength of the relationship, and</p>
</li>
<li>
<p>Pearson’s correlation can be affected (in either direction) if one of
the distributions is skewed or contains outliers. Spearman’s rank
correlation is more robust.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>In the BRFSS example, we know that the distribution of weights is
roughly lognormal; under a log transform it approximates a normal
distribution, so it has no skew. So another way to eliminate the effect
of skewness is to compute Pearson’s correlation with log-weight and
height:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    thinkstats2.Corr(df.htm3, np.log(df.wtkg2)))</code></pre>
</div>
</div>
<div class="paragraph">
<p>The result is 0.53, close to the rank correlation, 0.54. So that
suggests that skewness in the distribution of weight explains most of
the difference between Pearson’s and Spearman’s correlation.</p>
</div>
</div>
<div class="sect2">
<h3 id="_correlation_and_causation"><a class="anchor" href="#_correlation_and_causation"></a><a class="link" href="#_correlation_and_causation">7.8. Correlation and causation</a></h3>
<div class="paragraph">
<p>If variables A and B are correlated, there are three possible
explanations: A causes B, or B causes A, or some other set of factors
causes both A and B. These explanations are called &#8220;causal
relationships&#8221;.</p>
</div>
<div class="paragraph">
<p>Correlation alone does not distinguish between these explanations, so it
does not tell you which ones are true. This rule is often summarized
with the phrase &#8220;Correlation does not imply causation,&#8221; which is so
pithy it has its own Wikipedia page:
<a href="http://wikipedia.org/wiki/Correlation_does_not_imply_causation" class="bare">http://wikipedia.org/wiki/Correlation_does_not_imply_causation</a>.</p>
</div>
<div class="paragraph">
<p>So what can you do to provide evidence of causation?</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Use time. If A comes before B, then A can cause B but not the other
way around (at least according to our common understanding of
causation). The order of events can help us infer the direction of
causation, but it does not preclude the possibility that something else
causes both A and B.</p>
</li>
<li>
<p>Use randomness. If you divide a large sample into two groups at random
and compute the means of almost any variable, you expect the difference
to be small. If the groups are nearly identical in all variables but
one, you can eliminate spurious relationships.</p>
<div class="paragraph">
<p>This works even if you don’t know what the relevant variables are, but
it works even better if you do, because you can check that the groups
are identical.</p>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p>These ideas are the motivation for the <strong>randomized controlled trial</strong>, in
which subjects are assigned randomly to two (or more) groups: a
<strong>treatment group</strong> that receives some kind of intervention, like a new
medicine, and a <strong>control group</strong> that receives no intervention, or
another treatment whose effects are known.</p>
</div>
<div class="paragraph">
<p>A randomized controlled trial is the most reliable way to demonstrate a
causal relationship, and the foundation of science-based medicine (see
<a href="http://wikipedia.org/wiki/Randomized_controlled_trial" class="bare">http://wikipedia.org/wiki/Randomized_controlled_trial</a>).</p>
</div>
<div class="paragraph">
<p>Unfortunately, controlled trials are only possible in the laboratory
sciences, medicine, and a few other disciplines. In the social sciences,
controlled experiments are rare, usually because they are impossible or
unethical.</p>
</div>
<div class="paragraph">
<p>An alternative is to look for a <strong>natural experiment</strong>, where different
&#8220;treatments&#8221; are applied to groups that are otherwise similar. One
danger of natural experiments is that the groups might differ in ways
that are not apparent. You can read more about this topic at
<a href="http://wikipedia.org/wiki/Natural_experiment" class="bare">http://wikipedia.org/wiki/Natural_experiment</a>.</p>
</div>
<div class="paragraph">
<p>In some cases it is possible to infer causal relationships using
<strong>regression analysis</strong>, which is the topic of
<a href="#regression">Chapter 11</a>.</p>
</div>
</div>
<div class="sect2">
<h3 id="_exercises_7"><a class="anchor" href="#_exercises_7"></a><a class="link" href="#_exercises_7">7.9. Exercises</a></h3>
<div class="paragraph">
<p></p>
</div>
<div class="paragraph">
<p>A solution to this exercise is in <code>chap07soln.py</code>.</p>
</div>
<div class="paragraph">
<p><strong>Exercise 7.1</strong></p>
</div>
<div class="paragraph">
<p>Using data from the NSFG, make a scatter plot of birth weight versus
mother’s age. Plot percentiles of birth weight versus mother’s age.
Compute Pearson’s and Spearman’s correlations. How would you
characterize the relationship between these variables?</p>
</div>
</div>
<div class="sect2">
<h3 id="_glossary_7"><a class="anchor" href="#_glossary_7"></a><a class="link" href="#_glossary_7">7.10. Glossary</a></h3>
<div class="ulist">
<ul>
<li>
<p><strong>scatter plot</strong>: A visualization of the relationship between two
variables, showing one point for each row of data.</p>
</li>
<li>
<p><strong>jitter</strong>: Random noise added to data for purposes of visualization.</p>
</li>
<li>
<p><strong>saturation</strong>: Loss of information when multiple points are plotted on
top of each other.</p>
</li>
<li>
<p><strong>correlation</strong>: A statistic that measures the strength of the
relationship between two variables.</p>
</li>
<li>
<p><strong>standardize</strong>: To transform a set of values so that their mean is 0
and their variance is 1.</p>
</li>
<li>
<p><strong>standard score</strong>: A value that has been standardized so that it is
expressed in standard deviations from the mean.</p>
</li>
<li>
<p><strong>covariance</strong>: A measure of the tendency of two variables to vary
together.</p>
</li>
<li>
<p><strong>rank</strong>: The index where an element appears in a sorted list.</p>
</li>
<li>
<p><strong>randomized controlled trial</strong>: An experimental design in which
subjects are divided into groups at random, and different groups are
given different treatments.</p>
</li>
<li>
<p><strong>treatment group</strong>: A group in a controlled trial that receives some
kind of intervention.</p>
</li>
<li>
<p><strong>control group</strong>: A group in a controlled trial that receives no
treatment, or a treatment whose effect is known.</p>
</li>
<li>
<p><strong>natural experiment</strong>: An experimental design that takes advantage of a
natural division of subjects into groups in ways that are at least
approximately random.</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="estimation"><a class="anchor" href="#estimation"></a><a class="link" href="#estimation">8. Estimation</a></h2>
<div class="sectionbody">
<div class="paragraph">
<p>The code for this chapter is in <code>estimation.py</code>. For information about
downloading and working with this code, see <a href="#code">Using the code</a>.</p>
</div>
<div class="sect2">
<h3 id="_the_estimation_game"><a class="anchor" href="#_the_estimation_game"></a><a class="link" href="#_the_estimation_game">8.1. The estimation game</a></h3>
<div class="paragraph">
<p>Let’s play a game. I think of a distribution, and you have to guess what
it is. I’ll give you two hints: it’s a normal distribution, and here’s a
random sample drawn from it:</p>
</div>
<div class="paragraph">
<p><code>[-0.441, 1.774, -0.101, -1.138, 2.975, -2.138]</code></p>
</div>
<div class="paragraph">
<p>What do you think is the mean parameter, \(\mu\), of this
distribution?</p>
</div>
<div class="paragraph">
<p>One choice is to use the sample mean, \(\bar{x}\), as an
estimate of \(\mu\). In this example, \(\bar{x}\) is
0.155, so it would be reasonable to guess \(\mu\) = 0.155. This
process is called <strong>estimation</strong>, and the statistic we used (the sample
mean) is called an <strong>estimator</strong>.</p>
</div>
<div class="paragraph">
<p>Using the sample mean to estimate \(\mu\) is so obvious that it
is hard to imagine a reasonable alternative. But suppose we change the
game by introducing outliers.</p>
</div>
<div class="paragraph">
<p><em>I’m thinking of a distribution.</em> It’s a normal distribution, and here’s
a sample that was collected by an unreliable surveyor who occasionally
puts the decimal point in the wrong place.</p>
</div>
<div class="paragraph">
<p><code>[-0.441, 1.774, -0.101, -1.138, 2.975, -213.8]</code></p>
</div>
<div class="paragraph">
<p>Now what’s your estimate of \(\mu\)? If you use the sample mean,
your guess is -35.12. Is that the best choice? What are the
alternatives?</p>
</div>
<div class="paragraph">
<p>One option is to identify and discard outliers, then compute the sample
mean of the rest. Another option is to use the median as an estimator.</p>
</div>
<div class="paragraph">
<p>Which estimator is best depends on the circumstances (for example,
whether there are outliers) and on what the goal is. Are you trying to
minimize errors, or maximize your chance of getting the right answer?</p>
</div>
<div class="paragraph">
<p>If there are no outliers, the sample mean minimizes the <strong>mean squared
error</strong> (MSE). That is, if we play the game many times, and each time
compute the error \(\bar{x}- \mu\), the sample mean minimizes</p>
</div>
<div class="stemblock">
<div class="content">
\[MSE = \frac{1}{m} \sum (\bar{x}- \mu)^2\]
</div>
</div>
<div class="paragraph">
<p>Where \(m\) is the number of times you play the estimation game,
not to be confused with \(n\), which is the size of the sample
used to compute \(\bar{x}\).</p>
</div>
<div class="paragraph">
<p>Here is a function that simulates the estimation game and computes the
root mean squared error (RMSE), which is the square root of MSE:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def Estimate1(n=7, m=1000):
    mu = 0
    sigma = 1

    means = []
    medians = []
    for _ in range(m):
        xs = [random.gauss(mu, sigma) for i in range(n)]
        xbar = np.mean(xs)
        median = np.median(xs)
        means.append(xbar)
        medians.append(median)

    print('rmse xbar', RMSE(means, mu))
    print('rmse median', RMSE(medians, mu))</code></pre>
</div>
</div>
<div class="paragraph">
<p>Again, <code>n</code> is the size of the sample, and <code>m</code> is the number of times
we play the game. <code>means</code> is the list of estimates based on
\(\bar{x}\). <code>medians</code> is the list of medians.</p>
</div>
<div class="paragraph">
<p>Here’s the function that computes RMSE:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def RMSE(estimates, actual):
    e2 = [(estimate-actual)**2 for estimate in estimates]
    mse = np.mean(e2)
    return math.sqrt(mse)</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>estimates</code> is a list of estimates; <code>actual</code> is the actual value
being estimated. In practice, of course, we don’t know <code>actual</code>; if we
did, we wouldn’t have to estimate it. The purpose of this experiment is
to compare the performance of the two estimators.</p>
</div>
<div class="paragraph">
<p>When I ran this code, the RMSE of the sample mean was 0.41, which means
that if we use \(\bar{x}\) to estimate the mean of this
distribution, based on a sample with \(n=7\), we should expect
to be off by 0.41 on average. Using the median to estimate the mean
yields RMSE 0.53, which confirms that \(\bar{x}\) yields lower
RMSE, at least for this example.</p>
</div>
<div class="paragraph">
<p>Minimizing MSE is a nice property, but it’s not always the best
strategy. For example, suppose we are estimating the distribution of
wind speeds at a building site. If the estimate is too high, we might
overbuild the structure, increasing its cost. But if it’s too low, the
building might collapse. Because cost as a function of error is not
symmetric, minimizing MSE is not the best strategy.</p>
</div>
<div class="paragraph">
<p>As another example, suppose I roll three six-sided dice and ask you to
predict the total. If you get it exactly right, you get a prize;
otherwise you get nothing. In this case the value that minimizes MSE is
10.5, but that would be a bad guess, because the total of three dice is
never 10.5. For this game, you want an estimator that has the highest
chance of being right, which is a <strong>maximum likelihood estimator</strong> (MLE).
If you pick 10 or 11, your chance of winning is 1 in 8, and that’s the
best you can do.</p>
</div>
</div>
<div class="sect2">
<h3 id="_guess_the_variance"><a class="anchor" href="#_guess_the_variance"></a><a class="link" href="#_guess_the_variance">8.2. Guess the variance</a></h3>
<div class="paragraph">
<p><em>I’m thinking of a distribution.</em> It’s a normal distribution, and here’s
a (familiar) sample:</p>
</div>
<div class="paragraph">
<p><code>[-0.441, 1.774, -0.101, -1.138, 2.975, -2.138]</code></p>
</div>
<div class="paragraph">
<p>What do you think is the variance, \(\sigma^2\), of my
distribution? Again, the obvious choice is to use the sample variance,
\(S^2\), as an estimator.</p>
</div>
<div class="stemblock">
<div class="content">
\[S^2 = \frac{1}{n} \sum (x_i - \bar{x})^2\]
</div>
</div>
<div class="paragraph">
<p>For large samples, \(S^2\) is an adequate estimator, but for
small samples it tends to be too low. Because of this unfortunate
property, it is called a <strong>biased</strong> estimator. An estimator is <strong>unbiased</strong>
if the expected total (or mean) error, after many iterations of the
estimation game, is 0.</p>
</div>
<div class="paragraph">
<p>Fortunately, there is another simple statistic that is an unbiased
estimator of \(\sigma^2\):</p>
</div>
<div class="stemblock">
<div class="content">
\[S_{n-1}^2 = \frac{1}{n-1} \sum (x_i - \bar{x})^2\]
</div>
</div>
<div class="paragraph">
<p>For an explanation of why \(S^2\) is biased, and a proof that
\(S_{n-1}^2\) is unbiased, see
<a href="http://wikipedia.org/wiki/Bias_of_an_estimator" class="bare">http://wikipedia.org/wiki/Bias_of_an_estimator</a>.</p>
</div>
<div class="paragraph">
<p>The biggest problem with this estimator is that its name and symbol are
used inconsistently. The name &#8220;sample variance&#8221; can refer to either
\(S^2\) or \(S_{n-1}^2\), and the symbol \(S^2\)
is used for either or both.</p>
</div>
<div class="paragraph">
<p>Here is a function that simulates the estimation game and tests the
performance of \(S^2\) and \(S_{n-1}^2\):</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def Estimate2(n=7, m=1000):
    mu = 0
    sigma = 1

    estimates1 = []
    estimates2 = []
    for _ in range(m):
        xs = [random.gauss(mu, sigma) for i in range(n)]
        biased = np.var(xs)
        unbiased = np.var(xs, ddof=1)
        estimates1.append(biased)
        estimates2.append(unbiased)

    print('mean error biased', MeanError(estimates1, sigma**2))
    print('mean error unbiased', MeanError(estimates2, sigma**2))</code></pre>
</div>
</div>
<div class="paragraph">
<p>Again, <code>n</code> is the sample size and <code>m</code> is the number of times we play
the game. <code>np.var</code> computes \(S^2\) by default and
\(S_{n-1}^2\) if you provide the argument <code>ddof=1</code>, which
stands for &#8220;delta degrees of freedom.&#8221; I won’t explain that term, but
you can read about it at
<a href="http://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics" class="bare">http://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics</a>).</p>
</div>
<div class="paragraph">
<p><code>MeanError</code> computes the mean difference between the estimates and the
actual value:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def MeanError(estimates, actual):
    errors = [estimate-actual for estimate in estimates]
    return np.mean(errors)</code></pre>
</div>
</div>
<div class="paragraph">
<p>When I ran this code, the mean error for \(S^2\) was -0.13. As
expected, this biased estimator tends to be too low. For
\(S_{n-1}^2\), the mean error was 0.014, about 10 times smaller.
As <code>m</code> increases, we expect the mean error for \(S_{n-1}^2\)
to approach 0.</p>
</div>
<div class="paragraph">
<p>Properties like MSE and bias are long-term expectations based on many
iterations of the estimation game. By running simulations like the ones
in this chapter, we can compare estimators and check whether they have
desired properties.</p>
</div>
<div class="paragraph">
<p>But when you apply an estimator to real data, you just get one estimate.
It would not be meaningful to say that the estimate is unbiased; being
unbiased is a property of the estimator, not the estimate.</p>
</div>
<div class="paragraph">
<p>After you choose an estimator with appropriate properties, and use it to
generate an estimate, the next step is to characterize the uncertainty
of the estimate, which is the topic of the next section.</p>
</div>
</div>
<div class="sect2">
<h3 id="gorilla"><a class="anchor" href="#gorilla"></a><a class="link" href="#gorilla">8.3. Sampling distributions</a></h3>
<div class="paragraph">
<p>Suppose you are a scientist studying gorillas in a wildlife preserve.
You want to know the average weight of the adult female gorillas in the
preserve. To weigh them, you have to tranquilize them, which is
dangerous, expensive, and possibly harmful to the gorillas. But if it is
important to obtain this information, it might be acceptable to weigh a
sample of 9 gorillas. Let’s assume that the population of the preserve
is well known, so we can choose a representative sample of adult
females. We could use the sample mean, \(\bar{x}\), to estimate
the unknown population mean, \(\mu\).</p>
</div>
<div class="paragraph">
<p>Having weighed 9 female gorillas, you might find \(\bar{x}=90\)
kg and sample standard deviation, \(S=7.5\) kg. The sample mean
is an unbiased estimator of \(\mu\), and in the long run it
minimizes MSE. So if you report a single estimate that summarizes the
results, you would report 90 kg.</p>
</div>
<div class="paragraph">
<p>But how confident should you be in this estimate? If you only weigh
\(n=9\) gorillas out of a much larger population, you might be
unlucky and choose the 9 heaviest gorillas (or the 9 lightest ones) just
by chance. Variation in the estimate caused by random selection is
called <strong>sampling error</strong>.</p>
</div>
<div class="paragraph">
<p>To quantify sampling error, we can simulate the sampling process with
hypothetical values of \(\mu\) and \(\sigma\), and see
how much \(\bar{x}\) varies.</p>
</div>
<div class="paragraph">
<p>Since we don’t know the actual values of \(\mu\) and
\(\sigma\) in the population, we’ll use the estimates
\(\bar{x}\) and \(S\). So the question we answer is:
&#8220;If the actual values of \(\mu\) and \(\sigma\) were 90
kg and 7.5 kg, and we ran the same experiment many times, how much would
the estimated mean, \(\bar{x}\), vary?&#8221;</p>
</div>
<div class="paragraph">
<p>The following function answers that question:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def SimulateSample(mu=90, sigma=7.5, n=9, m=1000):
    means = []
    for j in range(m):
        xs = np.random.normal(mu, sigma, n)
        xbar = np.mean(xs)
        means.append(xbar)

    cdf = thinkstats2.Cdf(means)
    ci = cdf.Percentile(5), cdf.Percentile(95)
    stderr = RMSE(means, mu)</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>mu</code> and <code>sigma</code> are the <em>hypothetical</em> values of the parameters.
<code>n</code> is the sample size, the number of gorillas we measured. <code>m</code> is
the number of times we run the simulation.</p>
</div>
<div id="estimation1" class="imageblock">
<div class="content">
<img src="figs/estimation1.png" alt="estimation1" height="240">
</div>
<div class="title">Figure 33. Sampling distribution of \(\bar{x}\), with confidence interval.</div>
</div>
<div class="paragraph">
<p>In each iteration, we choose <code>n</code> values from a normal distribution
with the given parameters, and compute the sample mean, <code>xbar</code>. We run
1000 simulations and then compute the distribution, <code>cdf</code>, of the
estimates. The result is shown in <a href="#estimation1">Figure 33</a>. This
distribution is called the <strong>sampling distribution</strong> of the estimator. It
shows how much the estimates would vary if we ran the experiment over
and over.</p>
</div>
<div class="paragraph">
<p>The mean of the sampling distribution is pretty close to the
hypothetical value of \(\mu\), which means that the experiment
yields the right answer, on average. After 1000 tries, the lowest result
is 82 kg, and the highest is 98 kg. This range suggests that the
estimate might be off by as much as 8 kg.</p>
</div>
<div class="paragraph">
<p>There are two common ways to summarize the sampling distribution:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Standard error</strong> (SE) is a measure of how far we expect the estimate
to be off, on average. For each simulated experiment, we compute the
error, \(\bar{x}- \mu\), and then compute the root mean squared
error (RMSE). In this example, it is roughly 2.5 kg.</p>
</li>
<li>
<p>A <strong>confidence interval</strong> (CI) is a range that includes a given fraction
of the sampling distribution. For example, the 90% confidence interval
is the range from the 5th to the 95th percentile. In this example, the
90% CI is \((86, 94)\) kg.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Standard errors and confidence intervals are the source of much
confusion:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>People often confuse standard error and standard deviation. Remember
that standard deviation describes variability in a measured quantity; in
this example, the standard deviation of gorilla weight is 7.5 kg.
Standard error describes variability in an estimate. In this example,
the standard error of the mean, based on a sample of 9 measurements, is
2.5 kg.</p>
<div class="paragraph">
<p>One way to remember the difference is that, as sample size increases,
standard error gets smaller; standard deviation does not.</p>
</div>
</li>
<li>
<p>People often think that there is a 90% probability that the actual
parameter, \(\mu\), falls in the 90% confidence interval. Sadly,
that is not true. If you want to make a claim like that, you have to use
Bayesian methods (see my book, <em>Think Bayes</em>).</p>
<div class="paragraph">
<p>The sampling distribution answers a different question: it gives you a
sense of how reliable an estimate is by telling you how much it would
vary if you ran the experiment again.</p>
</div>
</li>
</ul>
</div>
<div class="paragraph">
<p>It is important to remember that confidence intervals and standard
errors only quantify sampling error; that is, error due to measuring
only part of the population. The sampling distribution does not account
for other sources of error, notably sampling bias and measurement error,
which are the topics of the next section.</p>
</div>
</div>
<div class="sect2">
<h3 id="_sampling_bias"><a class="anchor" href="#_sampling_bias"></a><a class="link" href="#_sampling_bias">8.4. Sampling bias</a></h3>
<div class="paragraph">
<p>Suppose that instead of the weight of gorillas in a nature preserve, you
want to know the average weight of women in the city where you live. It
is unlikely that you would be allowed to choose a representative sample
of women and weigh them.</p>
</div>
<div class="paragraph">
<p>A simple alternative would be &#8220;telephone sampling;&#8221; that is, you could
choose random numbers from the phone book, call and ask to speak to an
adult woman, and ask how much she weighs.</p>
</div>
<div class="paragraph">
<p>Telephone sampling has obvious limitations. For example, the sample is
limited to people whose telephone numbers are listed, so it eliminates
people without phones (who might be poorer than average) and people with
unlisted numbers (who might be richer). Also, if you call home
telephones during the day, you are less likely to sample people with
jobs. And if you only sample the person who answers the phone, you are
less likely to sample people who share a phone line.</p>
</div>
<div class="paragraph">
<p>If factors like income, employment, and household size are related to
weight—and it is plausible that they are—the results of your survey
would be affected one way or another. This problem is called <strong>sampling
bias</strong> because it is a property of the sampling process.</p>
</div>
<div class="paragraph">
<p>This sampling process is also vulnerable to self-selection, which is a
kind of sampling bias. Some people will refuse to answer the question,
and if the tendency to refuse is related to weight, that would affect
the results.</p>
</div>
<div class="paragraph">
<p>Finally, if you ask people how much they weigh, rather than weighing
them, the results might not be accurate. Even helpful respondents might
round up or down if they are uncomfortable with their actual weight. And
not all respondents are helpful. These inaccuracies are examples of
<strong>measurement error</strong>.</p>
</div>
<div class="paragraph">
<p>When you report an estimated quantity, it is useful to report standard
error, or a confidence interval, or both, in order to quantify sampling
error. But it is also important to remember that sampling error is only
one source of error, and often it is not the biggest.</p>
</div>
</div>
<div class="sect2">
<h3 id="_exponential_distributions"><a class="anchor" href="#_exponential_distributions"></a><a class="link" href="#_exponential_distributions">8.5. Exponential distributions</a></h3>
<div class="paragraph">
<p>Let’s play one more round of the estimation game. <em>I’m thinking of a
distribution.</em> It’s an exponential distribution, and here’s a sample:</p>
</div>
<div class="paragraph">
<p><code>[5.384, 4.493, 19.198, 2.790, 6.122, 12.844]</code></p>
</div>
<div class="paragraph">
<p>What do you think is the parameter, \(\lambda\), of this
distribution?</p>
</div>
<div class="paragraph">
<p>In general, the mean of an exponential distribution is
\(1/\lambda\), so working backwards, we might choose</p>
</div>
<div class="stemblock">
<div class="content">
\[L= 1 / \bar{x}\]
</div>
</div>
<div class="paragraph">
<p>\(L\) is an estimator of \(\lambda\). And not just any
estimator; it is also the maximum likelihood estimator (see
<a href="http://wikipedia.org/wiki/Exponential_distribution#Maximum_likelihood" class="bare">http://wikipedia.org/wiki/Exponential_distribution#Maximum_likelihood</a>).
So if you want to maximize your chance of guessing \(\lambda\)
exactly, \(L\) is the way to go.</p>
</div>
<div class="paragraph">
<p>But we know that \(\bar{x}\) is not robust in the presence of
outliers, so we expect \(L\) to have the same problem.</p>
</div>
<div class="paragraph">
<p>We can choose an alternative based on the sample median. The median of
an exponential distribution is \(\ln(2) / \lambda\), so working
backwards again, we can define an estimator</p>
</div>
<div class="stemblock">
<div class="content">
\[L_m= \ln(2) / m\]
</div>
</div>
<div class="paragraph">
<p>where \(m\) is the sample median.</p>
</div>
<div class="paragraph">
<p>To test the performance of these estimators, we can simulate the
sampling process:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def Estimate3(n=7, m=1000):
    lam = 2

    means = []
    medians = []
    for _ in range(m):
        xs = np.random.exponential(1.0/lam, n)
        L = 1 / np.mean(xs)
        Lm = math.log(2) / thinkstats2.Median(xs)
        means.append(L)
        medians.append(Lm)

    print('rmse L', RMSE(means, lam))
    print('rmse Lm', RMSE(medians, lam))
    print('mean error L', MeanError(means, lam))
    print('mean error Lm', MeanError(medians, lam))</code></pre>
</div>
</div>
<div class="paragraph">
<p>When I run this experiment with \(\lambda=2\), the RMSE of
\(L\) is 1.1. For the median-based estimator \(L_m\),
RMSE is 1.8. We can’t tell from this experiment whether \(L\)
minimizes MSE, but at least it seems better than \(L_m\).</p>
</div>
<div class="paragraph">
<p>Sadly, it seems that both estimators are biased. For \(L\) the
mean error is 0.33; for \(L_m\) it is 0.45. And neither
converges to 0 as <code>m</code> increases.</p>
</div>
<div class="paragraph">
<p>It turns out that \(\bar{x}\) is an unbiased estimator of the
mean of the distribution, \(1 / \lambda\), but \(L\) is
not an unbiased estimator of \(\lambda\).</p>
</div>
</div>
<div class="sect2">
<h3 id="_exercises_8"><a class="anchor" href="#_exercises_8"></a><a class="link" href="#_exercises_8">8.6. Exercises</a></h3>
<div class="paragraph">
<p></p>
</div>
<div class="paragraph">
<p>For the following exercises, you might want to start with a copy of
<code>estimation.py</code>. Solutions are in <code>chap08soln.py</code></p>
</div>
<div class="paragraph">
<p><strong>Exercise 8.1</strong></p>
</div>
<div class="paragraph">
<p>In this chapter we used \(\bar{x}\) and median to estimate
\(\mu\), and found that \(\bar{x}\) yields lower MSE.
Also, we used \(S^2\) and \(S_{n-1}^2\) to estimate
\(\sigma\), and found that \(S^2\) is biased and
\(S_{n-1}^2\) unbiased.</p>
</div>
<div class="paragraph">
<p>Run similar experiments to see if \(\bar{x}\) and median are
biased estimates of \(\mu\). Also check whether \(S^2\)
or \(S_{n-1}^2\) yields a lower MSE.</p>
</div>
<div class="paragraph">
<p><strong>Exercise 8.2</strong></p>
</div>
<div class="paragraph">
<p>Suppose you draw a sample with size \(n=10\) from an exponential
distribution with \(\lambda=2\). Simulate this experiment 1000
times and plot the sampling distribution of the estimate \(L\).
Compute the standard error of the estimate and the 90% confidence
interval.</p>
</div>
<div class="paragraph">
<p>Repeat the experiment with a few different values of \(n\) and
make a plot of standard error versus \(n\).</p>
</div>
<div class="paragraph">
<p><strong>Exercise 8.3</strong></p>
</div>
<div class="paragraph">
<p>In games like hockey and soccer, the time between goals is roughly
exponential. So you could estimate a team’s goal-scoring rate by
observing the number of goals they score in a game. This estimation
process is a little different from sampling the time between goals, so
let’s see how it works.</p>
</div>
<div class="paragraph">
<p>Write a function that takes a goal-scoring rate, <code>lam</code>, in goals per
game, and simulates a game by generating the time between goals until
the total time exceeds 1 game, then returns the number of goals scored.</p>
</div>
<div class="paragraph">
<p>Write another function that simulates many games, stores the estimates
of <code>lam</code>, then computes their mean error and RMSE.</p>
</div>
<div class="paragraph">
<p>Is this way of making an estimate biased? Plot the sampling distribution
of the estimates and the 90% confidence interval. What is the standard
error? What happens to sampling error for increasing values of <code>lam</code>?</p>
</div>
</div>
<div class="sect2">
<h3 id="_glossary_8"><a class="anchor" href="#_glossary_8"></a><a class="link" href="#_glossary_8">8.7. Glossary</a></h3>
<div class="ulist">
<ul>
<li>
<p><strong>estimation</strong>: The process of inferring the parameters of a
distribution from a sample.</p>
</li>
<li>
<p><strong>estimator</strong>: A statistic used to estimate a parameter.</p>
</li>
<li>
<p><strong>mean squared error (MSE)</strong>: A measure of estimation error.</p>
</li>
<li>
<p><strong>root mean squared error (RMSE)</strong>: The square root of MSE, a more
meaningful representation of typical error magnitude.</p>
</li>
<li>
<p><strong>maximum likelihood estimator (MLE)</strong>: An estimator that computes the
point estimate most likely to be correct.</p>
</li>
<li>
<p><strong>bias (of an estimator)</strong>: The tendency of an estimator to be above or
below the actual value of the parameter, when averaged over repeated
experiments.</p>
</li>
<li>
<p><strong>sampling error</strong>: Error in an estimate due to the limited size of the
sample and variation due to chance.</p>
</li>
<li>
<p><strong>sampling bias</strong>: Error in an estimate due to a sampling process that
is not representative of the population.</p>
</li>
<li>
<p><strong>measurement error</strong>: Error in an estimate due to inaccuracy collecting
or recording data.</p>
</li>
<li>
<p><strong>sampling distribution</strong>: The distribution of a statistic if an
experiment is repeated many times.</p>
</li>
<li>
<p><strong>standard error</strong>: The RMSE of an estimate, which quantifies
variability due to sampling error (but not other sources of error).</p>
</li>
<li>
<p><strong>confidence interval</strong>: An interval that represents the expected range
of an estimator if an experiment is repeated many times.</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="testing"><a class="anchor" href="#testing"></a><a class="link" href="#testing">9. Hypothesis testing</a></h2>
<div class="sectionbody">
<div class="paragraph">
<p>The code for this chapter is in <code>hypothesis.py</code>. For information about
downloading and working with this code, see <a href="#code">Using the code</a>.</p>
</div>
<div class="sect2">
<h3 id="_classical_hypothesis_testing"><a class="anchor" href="#_classical_hypothesis_testing"></a><a class="link" href="#_classical_hypothesis_testing">9.1. Classical hypothesis testing</a></h3>
<div class="paragraph">
<p>Exploring the data from the NSFG, we saw several &#8220;apparent effects,&#8221;
including differences between first babies and others. So far we have
taken these effects at face value; in this chapter, we put them to the
test.</p>
</div>
<div class="paragraph">
<p>The fundamental question we want to address is whether the effects we
see in a sample are likely to appear in the larger population. For
example, in the NSFG sample we see a difference in mean pregnancy length
for first babies and others. We would like to know if that effect
reflects a real difference for women in the U.S., or if it might appear
in the sample by chance.</p>
</div>
<div class="paragraph">
<p>There are several ways we could formulate this question, including
Fisher null hypothesis testing, Neyman-Pearson decision theory, and
Bayesian inference<sup class="footnote">[<a id="_footnoteref_4" class="footnote" href="#_footnote_4" title="View footnote.">4</a>]</sup>. What I present here is a subset of
all three that makes up most of what people use in practice, which I
will call <strong>classical hypothesis testing</strong>.</p>
</div>
<div class="paragraph">
<p>The goal of classical hypothesis testing is to answer the question,
&#8220;Given a sample and an apparent effect, what is the probability of
seeing such an effect by chance?&#8221; Here’s how we answer that question:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The first step is to quantify the size of the apparent effect by
choosing a <strong>test statistic</strong>. In the NSFG example, the apparent effect is
a difference in pregnancy length between first babies and others, so a
natural choice for the test statistic is the difference in means between
the two groups.</p>
</li>
<li>
<p>The second step is to define a <strong>null hypothesis</strong>, which is a model of
the system based on the assumption that the apparent effect is not real.
In the NSFG example the null hypothesis is that there is no difference
between first babies and others; that is, that pregnancy lengths for
both groups have the same distribution.</p>
</li>
<li>
<p>The third step is to compute a <strong>p-value</strong>, which is the probability of
seeing the apparent effect if the null hypothesis is true. In the NSFG
example, we would compute the actual difference in means, then compute
the probability of seeing a difference as big, or bigger, under the null
hypothesis.</p>
</li>
<li>
<p>The last step is to interpret the result. If the p-value is low, the
effect is said to be <strong>statistically significant</strong>, which means that it is
unlikely to have occurred by chance. In that case we infer that the
effect is more likely to appear in the larger population.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The logic of this process is similar to a proof by contradiction. To
prove a mathematical statement, A, you assume temporarily that A is
false. If that assumption leads to a contradiction, you conclude that A
must actually be true.</p>
</div>
<div class="paragraph">
<p>Similarly, to test a hypothesis like, &#8220;This effect is real,&#8221; we
assume, temporarily, that it is not. That’s the null hypothesis. Based
on that assumption, we compute the probability of the apparent effect.
That’s the p-value. If the p-value is low, we conclude that the null
hypothesis is unlikely to be true.</p>
</div>
</div>
<div class="sect2">
<h3 id="hypotest"><a class="anchor" href="#hypotest"></a><a class="link" href="#hypotest">9.2. HypothesisTest</a></h3>
<div class="paragraph">
<p><code>thinkstats2</code> provides <code>HypothesisTest</code>, a class that represents the
structure of a classical hypothesis test. Here is the definition:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">class HypothesisTest(object):

    def __init__(self, data):
        self.data = data
        self.MakeModel()
        self.actual = self.TestStatistic(data)

    def PValue(self, iters=1000):
        self.test_stats = [self.TestStatistic(self.RunModel())
                           for _ in range(iters)]

        count = sum(1 for x in self.test_stats if x &gt;= self.actual)
        return count / iters

    def TestStatistic(self, data):
        raise UnimplementedMethodException()

    def MakeModel(self):
        pass

    def RunModel(self):
        raise UnimplementedMethodException()</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>HypothesisTest</code> is an abstract parent class that provides complete
definitions for some methods and place-keepers for others. Child classes
based on <code>HypothesisTest</code> inherit <code>__init__</code> and <code>PValue</code> and
provide <code>TestStatistic</code>, <code>RunModel</code>, and optionally <code>MakeModel</code>.</p>
</div>
<div class="paragraph">
<p><code>__init__</code> takes the data in whatever form is appropriate. It calls
<code>MakeModel</code>, which builds a representation of the null hypothesis,
then passes the data to <code>TestStatistic</code>, which computes the size of
the effect in the sample.</p>
</div>
<div class="paragraph">
<p><code>PValue</code> computes the probability of the apparent effect under the
null hypothesis. It takes as a parameter <code>iters</code>, which is the number
of simulations to run. The first line generates simulated data, computes
test statistics, and stores them in <code>test_stats</code>. The result is the
fraction of elements in <code>test_stats</code> that exceed or equal the observed
test statistic, <code>self.actual</code>.</p>
</div>
<div class="paragraph">
<p>As a simple example<sup class="footnote">[<a id="_footnoteref_5" class="footnote" href="#_footnote_5" title="View footnote.">5</a>]</sup>, suppose we toss a coin 250
times and see 140 heads and 110 tails. Based on this result, we might
suspect that the coin is biased; that is, more likely to land heads. To
test this hypothesis, we compute the probability of seeing such a
difference if the coin is actually fair:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">class CoinTest(thinkstats2.HypothesisTest):

    def TestStatistic(self, data):
        heads, tails = data
        test_stat = abs(heads - tails)
        return test_stat

    def RunModel(self):
        heads, tails = self.data
        n = heads + tails
        sample = [random.choice('HT') for _ in range(n)]
        hist = thinkstats2.Hist(sample)
        data = hist['H'], hist['T']
        return data</code></pre>
</div>
</div>
<div class="paragraph">
<p>The parameter, <code>data</code>, is a pair of integers: the number of heads and
tails. The test statistic is the absolute difference between them, so
<code>self.actual</code> is 30.</p>
</div>
<div class="paragraph">
<p><code>RunModel</code> simulates coin tosses assuming that the coin is actually
fair. It generates a sample of 250 tosses, uses Hist to count the number
of heads and tails, and returns a pair of integers.</p>
</div>
<div class="paragraph">
<p>Now all we have to do is instantiate <code>CoinTest</code> and call <code>PValue</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    ct = CoinTest((140, 110))
    pvalue = ct.PValue()</code></pre>
</div>
</div>
<div class="paragraph">
<p>The result is about 0.07, which means that if the coin is fair, we
expect to see a difference as big as 30 about 7% of the time.</p>
</div>
<div class="paragraph">
<p>How should we interpret this result? By convention, 5% is the threshold
of statistical significance. If the p-value is less than 5%, the effect
is considered significant; otherwise it is not.</p>
</div>
<div class="paragraph">
<p>But the choice of 5% is arbitrary, and (as we will see later) the
p-value depends on the choice of the test statistics and the model of
the null hypothesis. So p-values should not be considered precise
measurements.</p>
</div>
<div class="paragraph">
<p>I recommend interpreting p-values according to their order of magnitude:
if the p-value is less than 1%, the effect is unlikely to be due to
chance; if it is greater than 10%, the effect can plausibly be explained
by chance. P-values between 1% and 10% should be considered borderline.
So in this example I conclude that the data do not provide strong
evidence that the coin is biased or not.</p>
</div>
</div>
<div class="sect2">
<h3 id="testdiff"><a class="anchor" href="#testdiff"></a><a class="link" href="#testdiff">9.3. Testing a difference in means</a></h3>
<div class="paragraph">
<p>One of the most common effects to test is a difference in mean between
two groups. In the NSFG data, we saw that the mean pregnancy length for
first babies is slightly longer, and the mean birth weight is slightly
smaller. Now we will see if those effects are statistically significant.</p>
</div>
<div class="paragraph">
<p>For these examples, the null hypothesis is that the distributions for
the two groups are the same. One way to model the null hypothesis is by
<strong>permutation</strong>; that is, we can take values for first babies and others
and shuffle them, treating the two groups as one big group:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">class DiffMeansPermute(thinkstats2.HypothesisTest):

    def TestStatistic(self, data):
        group1, group2 = data
        test_stat = abs(group1.mean() - group2.mean())
        return test_stat

    def MakeModel(self):
        group1, group2 = self.data
        self.n, self.m = len(group1), len(group2)
        self.pool = np.hstack((group1, group2))

    def RunModel(self):
        np.random.shuffle(self.pool)
        data = self.pool[:self.n], self.pool[self.n:]
        return data</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>data</code> is a pair of sequences, one for each group. The test statistic
is the absolute difference in the means.</p>
</div>
<div class="paragraph">
<p><code>MakeModel</code> records the sizes of the groups, <code>n</code> and <code>m</code>, and
combines the groups into one NumPy array, <code>self.pool</code>.</p>
</div>
<div class="paragraph">
<p><code>RunModel</code> simulates the null hypothesis by shuffling the pooled
values and splitting them into two groups with sizes <code>n</code> and <code>m</code>. As
always, the return value from <code>RunModel</code> has the same format as the
observed data.</p>
</div>
<div class="paragraph">
<p>To test the difference in pregnancy length, we run:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    live, firsts, others = first.MakeFrames()
    data = firsts.prglngth.values, others.prglngth.values
    ht = DiffMeansPermute(data)
    pvalue = ht.PValue()</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>MakeFrames</code> reads the NSFG data and returns DataFrames representing
all live births, first babies, and others. We extract pregnancy lengths
as NumPy arrays, pass them as data to <code>DiffMeansPermute</code>, and compute
the p-value. The result is about 0.17, which means that we expect to see
a difference as big as the observed effect about 17% of the time. So
this effect is not statistically significant.</p>
</div>
<div id="hypothesis1" class="imageblock">
<div class="content">
<img src="figs/hypothesis1.png" alt="hypothesis1" height="240">
</div>
<div class="title">Figure 34. CDF of difference in mean pregnancy length under the null hypothesis.</div>
</div>
<div class="paragraph">
<p><code>HypothesisTest</code> provides <code>PlotCdf</code>, which plots the distribution of
the test statistic and a gray line indicating the observed effect size:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    ht.PlotCdf()
    thinkplot.Show(xlabel='test statistic',
                   ylabel='CDF')</code></pre>
</div>
</div>
<div class="paragraph">
<p><a href="#hypothesis1">Figure 34</a> shows the result. The CDF intersects the
observed difference at 0.83, which is the complement of the p-value,
0.17.</p>
</div>
<div class="paragraph">
<p>If we run the same analysis with birth weight, the computed p-value is
0; after 1000 attempts, the simulation never yields an effect as big as
the observed difference, 0.12 lbs. So we would report
\(p &lt; 0.001\), and conclude that the difference in birth weight
is statistically significant.</p>
</div>
</div>
<div class="sect2">
<h3 id="_other_test_statistics"><a class="anchor" href="#_other_test_statistics"></a><a class="link" href="#_other_test_statistics">9.4. Other test statistics</a></h3>
<div class="paragraph">
<p>Choosing the best test statistic depends on what question you are trying
to address. For example, if the relevant question is whether pregnancy
lengths are different for first babies, then it makes sense to test the
absolute difference in means, as we did in the previous section.</p>
</div>
<div class="paragraph">
<p>If we had some reason to think that first babies are likely to be late,
then we would not take the absolute value of the difference; instead we
would use this test statistic:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">class DiffMeansOneSided(DiffMeansPermute):

    def TestStatistic(self, data):
        group1, group2 = data
        test_stat = group1.mean() - group2.mean()
        return test_stat</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>DiffMeansOneSided</code> inherits <code>MakeModel</code> and <code>RunModel</code> from
<code>DiffMeansPermute</code>; the only difference is that <code>TestStatistic</code> does
not take the absolute value of the difference. This kind of test is
called <strong>one-sided</strong> because it only counts one side of the distribution
of differences. The previous test, using both sides, is <strong>two-sided</strong>.</p>
</div>
<div class="paragraph">
<p>For this version of the test, the p-value is 0.09. In general the
p-value for a one-sided test is about half the p-value for a two-sided
test, depending on the shape of the distribution.</p>
</div>
<div class="paragraph">
<p>The one-sided hypothesis, that first babies are born late, is more
specific than the two-sided hypothesis, so the p-value is smaller. But
even for the stronger hypothesis, the difference is not statistically
significant.</p>
</div>
<div class="paragraph">
<p>We can use the same framework to test for a difference in standard
deviation. In <a href="#visualization">Section 3.3</a>, we saw some evidence
that first babies are more likely to be early or late, and less likely
to be on time. So we might hypothesize that the standard deviation is
higher. Here’s how we can test that:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">class DiffStdPermute(DiffMeansPermute):

    def TestStatistic(self, data):
        group1, group2 = data
        test_stat = group1.std() - group2.std()
        return test_stat</code></pre>
</div>
</div>
<div class="paragraph">
<p>This is a one-sided test because the hypothesis is that the standard
deviation for first babies is higher, not just different. The p-value is
0.09, which is not statistically significant.</p>
</div>
</div>
<div class="sect2">
<h3 id="corrtest"><a class="anchor" href="#corrtest"></a><a class="link" href="#corrtest">9.5. Testing a correlation</a></h3>
<div class="paragraph">
<p>This framework can also test correlations. For example, in the NSFG data
set, the correlation between birth weight and mother’s age is about
0.07. It seems like older mothers have heavier babies. But could this
effect be due to chance?</p>
</div>
<div class="paragraph">
<p>For the test statistic, I use Pearson’s correlation, but Spearman’s
would work as well. If we had reason to expect positive correlation, we
would do a one-sided test. But since we have no such reason, I’ll do a
two-sided test using the absolute value of correlation.</p>
</div>
<div class="paragraph">
<p>The null hypothesis is that there is no correlation between mother’s age
and birth weight. By shuffling the observed values, we can simulate a
world where the distributions of age and birth weight are the same, but
where the variables are unrelated:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">class CorrelationPermute(thinkstats2.HypothesisTest):

    def TestStatistic(self, data):
        xs, ys = data
        test_stat = abs(thinkstats2.Corr(xs, ys))
        return test_stat

    def RunModel(self):
        xs, ys = self.data
        xs = np.random.permutation(xs)
        return xs, ys</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>data</code> is a pair of sequences. <code>TestStatistic</code> computes the absolute
value of Pearson’s correlation. <code>RunModel</code> shuffles the <code>xs</code> and
returns simulated data.</p>
</div>
<div class="paragraph">
<p>Here’s the code that reads the data and runs the test:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    live, firsts, others = first.MakeFrames()
    live = live.dropna(subset=['agepreg', 'totalwgt_lb'])
    data = live.agepreg.values, live.totalwgt_lb.values
    ht = CorrelationPermute(data)
    pvalue = ht.PValue()</code></pre>
</div>
</div>
<div class="paragraph">
<p>I use <code>dropna</code> with the <code>subset</code> argument to drop rows that are
missing either of the variables we need.</p>
</div>
<div class="paragraph">
<p>The actual correlation is 0.07. The computed p-value is 0; after 1000
iterations the largest simulated correlation is 0.04. So although the
observed correlation is small, it is statistically significant.</p>
</div>
<div class="paragraph">
<p>This example is a reminder that &#8220;statistically significant&#8221; does not
always mean that an effect is important, or significant in practice. It
only means that it is unlikely to have occurred by chance.</p>
</div>
</div>
<div class="sect2">
<h3 id="casino"><a class="anchor" href="#casino"></a><a class="link" href="#casino">9.6. Testing proportions</a></h3>
<div class="paragraph">
<p>Suppose you run a casino and you suspect that a customer is using a
crooked die; that is, one that has been modified to make one of the
faces more likely than the others. You apprehend the alleged cheater and
confiscate the die, but now you have to prove that it is crooked. You
roll the die 60 times and get the following results:</p>
</div>
<table class="tableblock frame-all grid-all">
<colgroup>
<col>
<col>
<col>
<col>
<col>
<col>
<col>
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Value</th>
<th class="tableblock halign-center valign-top">1</th>
<th class="tableblock halign-center valign-top">2</th>
<th class="tableblock halign-center valign-top">3</th>
<th class="tableblock halign-center valign-top">4</th>
<th class="tableblock halign-center valign-top">5</th>
<th class="tableblock halign-center valign-top">6</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Frequency</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">8</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">9</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">19</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">5</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">8</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">11</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>On average you expect each value to appear 10 times. In this dataset,
the value 3 appears more often than expected, and the value 4 appears
less often. But are these differences statistically significant?</p>
</div>
<div class="paragraph">
<p>To test this hypothesis, we can compute the expected frequency for each
value, the difference between the expected and observed frequencies, and
the total absolute difference. In this example, we expect each side to
come up 10 times out of 60; the deviations from this expectation are -2,
-1, 9, -5, -2, and 1; so the total absolute difference is 20. How often
would we see such a difference by chance?</p>
</div>
<div class="paragraph">
<p>Here’s a version of <code>HypothesisTest</code> that answers that question:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">class DiceTest(thinkstats2.HypothesisTest):

    def TestStatistic(self, data):
        observed = data
        n = sum(observed)
        expected = np.ones(6) * n / 6
        test_stat = sum(abs(observed - expected))
        return test_stat

    def RunModel(self):
        n = sum(self.data)
        values = [1, 2, 3, 4, 5, 6]
        rolls = np.random.choice(values, n, replace=True)
        hist = thinkstats2.Hist(rolls)
        freqs = hist.Freqs(values)
        return freqs</code></pre>
</div>
</div>
<div class="paragraph">
<p>The data are represented as a list of frequencies: the observed values
are <code>[8, 9, 19, 5, 8, 11]</code>; the expected frequencies are all 10. The
test statistic is the sum of the absolute differences.</p>
</div>
<div class="paragraph">
<p>The null hypothesis is that the die is fair, so we simulate that by
drawing random samples from <code>values</code>. <code>RunModel</code> uses <code>Hist</code> to
compute and return the list of frequencies.</p>
</div>
<div class="paragraph">
<p>The p-value for this data is 0.13, which means that if the die is fair
we expect to see the observed total deviation, or more, about 13% of the
time. So the apparent effect is not statistically significant.</p>
</div>
</div>
<div class="sect2">
<h3 id="casino2"><a class="anchor" href="#casino2"></a><a class="link" href="#casino2">9.7. Chi-squared tests</a></h3>
<div class="paragraph">
<p>In the previous section we used total deviation as the test statistic.
But for testing proportions it is more common to use the chi-squared
statistic:</p>
</div>
<div class="stemblock">
<div class="content">
\[\chi^2 = \sum_i \frac{(O_i - E_i)^2}{E_i}\]
</div>
</div>
<div class="paragraph">
<p>Where \(O_i\) are the observed frequencies and \(E_i\)
are the expected frequencies. Here’s the Python code:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">class DiceChiTest(DiceTest):

    def TestStatistic(self, data):
        observed = data
        n = sum(observed)
        expected = np.ones(6) * n / 6
        test_stat = sum((observed - expected)**2 / expected)
        return test_stat</code></pre>
</div>
</div>
<div class="paragraph">
<p>Squaring the deviations (rather than taking absolute values) gives more
weight to large deviations. Dividing through by <code>expected</code>
standardizes the deviations, although in this case it has no effect
because the expected frequencies are all equal.</p>
</div>
<div class="paragraph">
<p>The p-value using the chi-squared statistic is 0.04, substantially
smaller than what we got using total deviation, 0.13. If we take the 5%
threshold seriously, we would consider this effect statistically
significant. But considering the two tests togther, I would say that the
results are borderline. I would not rule out the possibility that the
die is crooked, but I would not convict the accused cheater.</p>
</div>
<div class="paragraph">
<p>This example demonstrates an important point: the p-value depends on the
choice of test statistic and the model of the null hypothesis, and
sometimes these choices determine whether an effect is statistically
significant or not.</p>
</div>
</div>
<div class="sect2">
<h3 id="_first_babies_again"><a class="anchor" href="#_first_babies_again"></a><a class="link" href="#_first_babies_again">9.8. First babies again</a></h3>
<div class="paragraph">
<p>Earlier in this chapter we looked at pregnancy lengths for first babies
and others, and concluded that the apparent differences in mean and
standard deviation are not statistically significant. But in
<a href="#visualization">Section 3.3</a>, we saw several apparent differences in
the distribution of pregnancy length, especially in the range from 35 to
43 weeks. To see whether those differences are statistically
significant, we can use a test based on a chi-squared statistic.</p>
</div>
<div class="paragraph">
<p>The code combines elements from previous examples:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">class PregLengthTest(thinkstats2.HypothesisTest):

    def MakeModel(self):
        firsts, others = self.data
        self.n = len(firsts)
        self.pool = np.hstack((firsts, others))

        pmf = thinkstats2.Pmf(self.pool)
        self.values = range(35, 44)
        self.expected_probs = np.array(pmf.Probs(self.values))

    def RunModel(self):
        np.random.shuffle(self.pool)
        data = self.pool[:self.n], self.pool[self.n:]
        return data</code></pre>
</div>
</div>
<div class="paragraph">
<p>The data are represented as two lists of pregnancy lengths. The null
hypothesis is that both samples are drawn from the same distribution.
<code>MakeModel</code> models that distribution by pooling the two samples using
<code>hstack</code>. Then <code>RunModel</code> generates simulated data by shuffling the
pooled sample and splitting it into two parts.</p>
</div>
<div class="paragraph">
<p><code>MakeModel</code> also defines <code>values</code>, which is the range of weeks we’ll
use, and <code>expected_probs</code>, which is the probability of each value in
the pooled distribution.</p>
</div>
<div class="paragraph">
<p>Here’s the code that computes the test statistic:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python"># class PregLengthTest:

    def TestStatistic(self, data):
        firsts, others = data
        stat = self.ChiSquared(firsts) + self.ChiSquared(others)
        return stat

    def ChiSquared(self, lengths):
        hist = thinkstats2.Hist(lengths)
        observed = np.array(hist.Freqs(self.values))
        expected = self.expected_probs * len(lengths)
        stat = sum((observed - expected)**2 / expected)
        return stat</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>TestStatistic</code> computes the chi-squared statistic for first babies
and others, and adds them.</p>
</div>
<div class="paragraph">
<p><code>ChiSquared</code> takes a sequence of pregnancy lengths, computes its
histogram, and computes <code>observed</code>, which is a list of frequencies
corresponding to <code>self.values</code>. To compute the list of expected
frequencies, it multiplies the pre-computed probabilities,
<code>expected_probs</code>, by the sample size. It returns the chi-squared
statistic, <code>stat</code>.</p>
</div>
<div class="paragraph">
<p>For the NSFG data the total chi-squared statistic is 102, which doesn’t
mean much by itself. But after 1000 iterations, the largest test
statistic generated under the null hypothesis is 32. We conclude that
the observed chi-squared statistic is unlikely under the null
hypothesis, so the apparent effect is statistically significant.</p>
</div>
<div class="paragraph">
<p>This example demonstrates a limitation of chi-squared tests: they
indicate that there is a difference between the two groups, but they
don’t say anything specific about what the difference is.</p>
</div>
</div>
<div class="sect2">
<h3 id="_errors"><a class="anchor" href="#_errors"></a><a class="link" href="#_errors">9.9. Errors</a></h3>
<div class="paragraph">
<p>In classical hypothesis testing, an effect is considered statistically
significant if the p-value is below some threshold, commonly 5%. This
procedure raises two questions:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>If the effect is actually due to chance, what is the probability that
we will wrongly consider it significant? This probability is the <strong>false
positive rate</strong>.</p>
</li>
<li>
<p>If the effect is real, what is the chance that the hypothesis test
will fail? This probability is the <strong>false negative rate</strong>.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The false positive rate is relatively easy to compute: if the threshold
is 5%, the false positive rate is 5%. Here’s why:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>If there is no real effect, the null hypothesis is true, so we can
compute the distribution of the test statistic by simulating the null
hypothesis. Call this distribution \(\mathrm{CDF}_T\).</p>
</li>
<li>
<p>Each time we run an experiment, we get a test statistic,
\(t\), which is drawn from \(CDF_T\). Then we compute a
p-value, which is the probability that a random value from
\(CDF_T\) exceeds <code>t</code>, so that’s \(1 - CDF_T(t)\).</p>
</li>
<li>
<p>The p-value is less than 5% if \(CDF_T(t)\) is greater than
95%; that is, if \(t\) exceeds the 95th percentile. And how
often does a value chosen from \(CDF_T\) exceed the 95th
percentile? 5% of the time.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>So if you perform one hypothesis test with a 5% threshold, you expect a
false positive 1 time in 20.</p>
</div>
</div>
<div class="sect2">
<h3 id="power"><a class="anchor" href="#power"></a><a class="link" href="#power">9.10. Power</a></h3>
<div class="paragraph">
<p>The false negative rate is harder to compute because it depends on the
actual effect size, and normally we don’t know that. One option is to
compute a rate conditioned on a hypothetical effect size.</p>
</div>
<div class="paragraph">
<p>For example, if we assume that the observed difference between groups is
accurate, we can use the observed samples as a model of the population
and run hypothesis tests with simulated data:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def FalseNegRate(data, num_runs=100):
    group1, group2 = data
    count = 0

    for i in range(num_runs):
        sample1 = thinkstats2.Resample(group1)
        sample2 = thinkstats2.Resample(group2)

        ht = DiffMeansPermute((sample1, sample2))
        pvalue = ht.PValue(iters=101)
        if pvalue &gt; 0.05:
            count += 1

    return count / num_runs</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>FalseNegRate</code> takes data in the form of two sequences, one for each
group. Each time through the loop, it simulates an experiment by drawing
a random sample from each group and running a hypothesis test. Then it
checks the result and counts the number of false negatives.</p>
</div>
<div class="paragraph">
<p><code>Resample</code> takes a sequence and draws a sample with the same length,
with replacement:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def Resample(xs):
    return np.random.choice(xs, len(xs), replace=True)</code></pre>
</div>
</div>
<div class="paragraph">
<p>Here’s the code that tests pregnancy lengths:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    live, firsts, others = first.MakeFrames()
    data = firsts.prglngth.values, others.prglngth.values
    neg_rate = FalseNegRate(data)</code></pre>
</div>
</div>
<div class="paragraph">
<p>The result is about 70%, which means that if the actual difference in
mean pregnancy length is 0.078 weeks, we expect an experiment with this
sample size to yield a negative test 70% of the time.</p>
</div>
<div class="paragraph">
<p>This result is often presented the other way around: if the actual
difference is 0.078 weeks, we should expect a positive test only 30% of
the time. This &#8220;correct positive rate&#8221; is called the <strong>power</strong> of the
test, or sometimes &#8220;sensitivity&#8221;. It reflects the ability of the test
to detect an effect of a given size.</p>
</div>
<div class="paragraph">
<p>In this example, the test had only a 30% chance of yielding a positive
result (again, assuming that the difference is 0.078 weeks). As a rule
of thumb, a power of 80% is considered acceptable, so we would say that
this test was &#8220;underpowered.&#8221;</p>
</div>
<div class="paragraph">
<p>In general a negative hypothesis test does not imply that there is no
difference between the groups; instead it suggests that if there is a
difference, it is too small to detect with this sample size.</p>
</div>
</div>
<div class="sect2">
<h3 id="replication"><a class="anchor" href="#replication"></a><a class="link" href="#replication">9.11. Replication</a></h3>
<div class="paragraph">
<p>The hypothesis testing process I demonstrated in this chapter is not,
strictly speaking, good practice.</p>
</div>
<div class="paragraph">
<p>First, I performed multiple tests. If you run one hypothesis test, the
chance of a false positive is about 1 in 20, which might be acceptable.
But if you run 20 tests, you should expect at least one false positive,
most of the time.</p>
</div>
<div class="paragraph">
<p>Second, I used the same dataset for exploration and testing. If you
explore a large dataset, find a surprising effect, and then test whether
it is significant, you have a good chance of generating a false
positive.</p>
</div>
<div class="paragraph">
<p>To compensate for multiple tests, you can adjust the p-value threshold
(see <a href="https://en.wikipedia.org/wiki/Holm-Bonferroni_method" class="bare">https://en.wikipedia.org/wiki/Holm-Bonferroni_method</a>). Or you can
address both problems by partitioning the data, using one set for
exploration and the other for testing.</p>
</div>
<div class="paragraph">
<p>In some fields these practices are required or at least encouraged. But
it is also common to address these problems implicitly by replicating
published results. Typically the first paper to report a new result is
considered exploratory. Subsequent papers that replicate the result with
new data are considered confirmatory.</p>
</div>
<div class="paragraph">
<p>As it happens, we have an opportunity to replicate the results in this
chapter. The first edition of this book is based on Cycle 6 of the NSFG,
which was released in 2002. In October 2011, the CDC released additional
data based on interviews conducted from 2006–2010. <code>nsfg2.py</code> contains
code to read and clean this data. In the new dataset:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The difference in mean pregnancy length is 0.16 weeks and
statistically significant with \(p &lt; 0.001\) (compared to 0.078
weeks in the original dataset).</p>
</li>
<li>
<p>The difference in birth weight is 0.17 pounds with
\(p &lt; 0.001\) (compared to 0.12 lbs in the original dataset).</p>
</li>
<li>
<p>The correlation between birth weight and mother’s age is 0.08 with
\(p &lt; 0.001\) (compared to 0.07).</p>
</li>
<li>
<p>The chi-squared test is statistically significant with
\(p &lt; 0.001\) (as it was in the original).</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>In summary, all of the effects that were statistically significant in
the original dataset were replicated in the new dataset, and the
difference in pregnancy length, which was not significant in the
original, is bigger in the new dataset and significant.</p>
</div>
</div>
<div class="sect2">
<h3 id="_exercises_9"><a class="anchor" href="#_exercises_9"></a><a class="link" href="#_exercises_9">9.12. Exercises</a></h3>
<div class="paragraph">
<p></p>
</div>
<div class="paragraph">
<p>A solution to these exercises is in <code>chap09soln.py</code>.</p>
</div>
<div class="paragraph">
<p><strong>Exercise 9.1</strong></p>
</div>
<div class="paragraph">
<p>As sample size increases, the power of a hypothesis test increases,
which means it is more likely to be positive if the effect is real.
Conversely, as sample size decreases, the test is less likely to be
positive even if the effect is real.</p>
</div>
<div class="paragraph">
<p>To investigate this behavior, run the tests in this chapter with
different subsets of the NSFG data. You can use
<code>thinkstats2.SampleRows</code> to select a random subset of the rows in a
DataFrame.</p>
</div>
<div class="paragraph">
<p>What happens to the p-values of these tests as sample size decreases?
What is the smallest sample size that yields a positive test?</p>
</div>
<div class="paragraph">
<p><strong>Exercise 9.2</strong></p>
</div>
<div class="paragraph">
<p>In <a href="#testdiff">Section 9.3</a>, we simulated the null hypothesis by
permutation; that is, we treated the observed values as if they
represented the entire population, and randomly assigned the members of
the population to the two groups.</p>
</div>
<div class="paragraph">
<p>An alternative is to use the sample to estimate the distribution for the
population, then draw a random sample from that distribution. This
process is called <strong>resampling</strong>. There are several ways to implement
resampling, but one of the simplest is to draw a sample with replacement
from the observed values, as in <a href="#power">Section 9.10</a>.</p>
</div>
<div class="paragraph">
<p>Write a class named <code>DiffMeansResample</code> that inherits from
<code>DiffMeansPermute</code> and overrides <code>RunModel</code> to implement resampling,
rather than permutation.</p>
</div>
<div class="paragraph">
<p>Use this model to test the differences in pregnancy length and birth
weight. How much does the model affect the results?</p>
</div>
</div>
<div class="sect2">
<h3 id="_glossary_9"><a class="anchor" href="#_glossary_9"></a><a class="link" href="#_glossary_9">9.13. Glossary</a></h3>
<div class="ulist">
<ul>
<li>
<p><strong>hypothesis testing</strong>: The process of determining whether an apparent
effect is statistically significant.</p>
</li>
<li>
<p><strong>test statistic</strong>: A statistic used to quantify an effect size.</p>
</li>
<li>
<p><strong>null hypothesis</strong>: A model of a system based on the assumption that an
apparent effect is due to chance.</p>
</li>
<li>
<p><strong>p-value</strong>: The probability that an effect could occur by chance.</p>
</li>
<li>
<p><strong>statistically significant</strong>: An effect is statistically significant if
it is unlikely to occur by chance.</p>
</li>
<li>
<p><strong>permutation test</strong>: A way to compute p-values by generating
permutations of an observed dataset.</p>
</li>
<li>
<p><strong>resampling test</strong>: A way to compute p-values by generating samples,
with replacement, from an observed dataset.</p>
</li>
<li>
<p><strong>two-sided test</strong>: A test that asks, &#8220;What is the chance of an effect
as big as the observed effect, positive or negative?&#8221;</p>
</li>
<li>
<p><strong>one-sided test</strong>: A test that asks, &#8220;What is the chance of an effect
as big as the observed effect, and with the same sign?&#8221;</p>
</li>
<li>
<p><strong>chi-squared test</strong>: A test that uses the chi-squared statistic as the
test statistic.</p>
</li>
<li>
<p><strong>false positive</strong>: The conclusion that an effect is real when it is
not.</p>
</li>
<li>
<p><strong>false negative</strong>: The conclusion that an effect is due to chance when
it is not.</p>
</li>
<li>
<p><strong>power</strong>: The probability of a positive test if the null hypothesis is
false.</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="linear"><a class="anchor" href="#linear"></a><a class="link" href="#linear">10. Linear least squares</a></h2>
<div class="sectionbody">
<div class="paragraph">
<p>The code for this chapter is in <code>linear.py</code>. For information about
downloading and working with this code, see <a href="#code">Using the code</a>.</p>
</div>
<div class="sect2">
<h3 id="_least_squares_fit"><a class="anchor" href="#_least_squares_fit"></a><a class="link" href="#_least_squares_fit">10.1. Least squares fit</a></h3>
<div class="paragraph">
<p>Correlation coefficients measure the strength and sign of a
relationship, but not the slope. There are several ways to estimate the
slope; the most common is a <strong>linear least squares fit</strong>. A &#8220;linear fit&#8221;
is a line intended to model the relationship between variables. A
&#8220;least squares&#8221; fit is one that minimizes the mean squared error (MSE)
between the line and the data.</p>
</div>
<div class="paragraph">
<p>Suppose we have a sequence of points, <code>ys</code>, that we want to express as
a function of another sequence <code>xs</code>. If there is a linear relationship
between <code>xs</code> and <code>ys</code> with intercept <code>inter</code> and slope <code>slope</code>,
we expect each <code>y[i]</code> to be <code>inter + slope * x[i]</code>.</p>
</div>
<div class="paragraph">
<p>But unless the correlation is perfect, this prediction is only
approximate. The vertical deviation from the line, or <strong>residual</strong>, is</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">res = ys - (inter + slope * xs)</code></pre>
</div>
</div>
<div class="paragraph">
<p>The residuals might be due to random factors like measurement error, or
non-random factors that are unknown. For example, if we are trying to
predict weight as a function of height, unknown factors might include
diet, exercise, and body type.</p>
</div>
<div class="paragraph">
<p>If we get the parameters <code>inter</code> and <code>slope</code> wrong, the residuals
get bigger, so it makes intuitive sense that the parameters we want are
the ones that minimize the residuals.</p>
</div>
<div class="paragraph">
<p>We might try to minimize the absolute value of the residuals, or their
squares, or their cubes; but the most common choice is to minimize the
sum of squared residuals, <code>sum(res**2)</code>.</p>
</div>
<div class="paragraph">
<p>Why? There are three good reasons and one less important one:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Squaring has the feature of treating positive and negative residuals
the same, which is usually what we want.</p>
</li>
<li>
<p>Squaring gives more weight to large residuals, but not so much weight
that the largest residual always dominates.</p>
</li>
<li>
<p>If the residuals are uncorrelated and normally distributed with mean 0
and constant (but unknown) variance, then the least squares fit is also
the maximum likelihood estimator of <code>inter</code> and <code>slope</code>. See
<a href="https://en.wikipedia.org/wiki/Linear_regression" class="bare">https://en.wikipedia.org/wiki/Linear_regression</a>.</p>
</li>
<li>
<p>The values of <code>inter</code> and <code>slope</code> that minimize the squared
residuals can be computed efficiently.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The last reason made sense when computational efficiency was more
important than choosing the method most appropriate to the problem at
hand. That’s no longer the case, so it is worth considering whether
squared residuals are the right thing to minimize.</p>
</div>
<div class="paragraph">
<p>For example, if you are using <code>xs</code> to predict values of <code>ys</code>,
guessing too high might be better (or worse) than guessing too low. In
that case you might want to compute some cost function for each
residual, and minimize total cost, <code>sum(cost(res))</code>. However,
computing a least squares fit is quick, easy and often good enough.</p>
</div>
</div>
<div class="sect2">
<h3 id="_implementation"><a class="anchor" href="#_implementation"></a><a class="link" href="#_implementation">10.2. Implementation</a></h3>
<div class="paragraph">
<p><code>thinkstats2</code> provides simple functions that demonstrate linear least
squares:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def LeastSquares(xs, ys):
    meanx, varx = MeanVar(xs)
    meany = Mean(ys)

    slope = Cov(xs, ys, meanx, meany) / varx
    inter = meany - slope * meanx

    return inter, slope</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>LeastSquares</code> takes sequences <code>xs</code> and <code>ys</code> and returns the
estimated parameters <code>inter</code> and <code>slope</code>. For details on how it
works, see
<a href="http://wikipedia.org/wiki/Numerical_methods_for_linear_least_squares" class="bare">http://wikipedia.org/wiki/Numerical_methods_for_linear_least_squares</a>.</p>
</div>
<div class="paragraph">
<p><code>thinkstats2</code> also provides <code>FitLine</code>, which takes <code>inter</code> and
<code>slope</code> and returns the fitted line for a sequence of <code>xs</code>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def FitLine(xs, inter, slope):
    fit_xs = np.sort(xs)
    fit_ys = inter + slope * fit_xs
    return fit_xs, fit_ys</code></pre>
</div>
</div>
<div class="paragraph">
<p>We can use these functions to compute the least squares fit for birth
weight as a function of mother’s age.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    live, firsts, others = first.MakeFrames()
    live = live.dropna(subset=['agepreg', 'totalwgt_lb'])
    ages = live.agepreg
    weights = live.totalwgt_lb

    inter, slope = thinkstats2.LeastSquares(ages, weights)
    fit_xs, fit_ys = thinkstats2.FitLine(ages, inter, slope)</code></pre>
</div>
</div>
<div class="paragraph">
<p>The estimated intercept and slope are 6.8 lbs and 0.017 lbs per year.
These values are hard to interpret in this form: the intercept is the
expected weight of a baby whose mother is 0 years old, which doesn’t
make sense in context, and the slope is too small to grasp easily.</p>
</div>
<div class="paragraph">
<p>Instead of presenting the intercept at \(x=0\), it is often
helpful to present the intercept at the mean of \(x\). In this
case the mean age is about 25 years and the mean baby weight for a 25
year old mother is 7.3 pounds. The slope is 0.27 ounces per year, or
0.17 pounds per decade.</p>
</div>
<div id="linear1" class="imageblock">
<div class="content">
<img src="figs/linear1.png" alt="linear1" height="240">
</div>
<div class="title">Figure 35. Scatter plot of birth weight and mother’s age with a linear fit.</div>
</div>
<div class="paragraph">
<p><a href="#linear1">Figure 35</a> shows a scatter plot of birth weight and age
along with the fitted line. It’s a good idea to look at a figure like
this to assess whether the relationship is linear and whether the fitted
line seems like a good model of the relationship.</p>
</div>
</div>
<div class="sect2">
<h3 id="_residuals"><a class="anchor" href="#_residuals"></a><a class="link" href="#_residuals">10.3. Residuals</a></h3>
<div class="paragraph">
<p>Another useful test is to plot the residuals. <code>thinkstats2</code> provides a
function that computes residuals:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def Residuals(xs, ys, inter, slope):
    xs = np.asarray(xs)
    ys = np.asarray(ys)
    res = ys - (inter + slope * xs)
    return res</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>Residuals</code> takes sequences <code>xs</code> and <code>ys</code> and estimated parameters
<code>inter</code> and <code>slope</code>. It returns the differences between the actual
values and the fitted line.</p>
</div>
<div id="linear2" class="imageblock">
<div class="content">
<img src="figs/linear2.png" alt="linear2" height="240">
</div>
<div class="title">Figure 36. Residuals of the linear fit.</div>
</div>
<div class="paragraph">
<p>To visualize the residuals, I group respondents by age and compute
percentiles in each group, as we saw in
<a href="#characterizing">Section 7.2</a>. <a href="#linear2">Figure 36</a> shows the
25th, 50th and 75th percentiles of the residuals for each age group. The
median is near zero, as expected, and the interquartile range is about 2
pounds. So if we know the mother’s age, we can guess the baby’s weight
within a pound, about 50% of the time.</p>
</div>
<div class="paragraph">
<p>Ideally these lines should be flat, indicating that the residuals are
random, and parallel, indicating that the variance of the residuals is
the same for all age groups. In fact, the lines are close to parallel,
so that’s good; but they have some curvature, indicating that the
relationship is nonlinear. Nevertheless, the linear fit is a simple
model that is probably good enough for some purposes.</p>
</div>
</div>
<div class="sect2">
<h3 id="regest"><a class="anchor" href="#regest"></a><a class="link" href="#regest">10.4. Estimation</a></h3>
<div class="paragraph">
<p>The parameters <code>slope</code> and <code>inter</code> are estimates based on a sample;
like other estimates, they are vulnerable to sampling bias, measurement
error, and sampling error. As discussed in <a href="#estimation">Chapter 8</a>,
sampling bias is caused by non-representative sampling, measurement
error is caused by errors in collecting and recording data, and sampling
error is the result of measuring a sample rather than the entire
population.</p>
</div>
<div class="paragraph">
<p>To assess sampling error, we ask, &#8220;If we run this experiment again, how
much variability do we expect in the estimates?&#8221; We can answer this
question by running simulated experiments and computing sampling
distributions of the estimates.</p>
</div>
<div class="paragraph">
<p>I simulate the experiments by resampling the data; that is, I treat the
observed pregnancies as if they were the entire population and draw
samples, with replacement, from the observed sample.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def SamplingDistributions(live, iters=101):
    t = []
    for _ in range(iters):
        sample = thinkstats2.ResampleRows(live)
        ages = sample.agepreg
        weights = sample.totalwgt_lb
        estimates = thinkstats2.LeastSquares(ages, weights)
        t.append(estimates)

    inters, slopes = zip(*t)
    return inters, slopes</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>SamplingDistributions</code> takes a DataFrame with one row per live birth,
and <code>iters</code>, the number of experiments to simulate. It uses
<code>ResampleRows</code> to resample the observed pregnancies. We’ve already
seen <code>SampleRows</code>, which chooses random rows from a DataFrame.
<code>thinkstats2</code> also provides <code>ResampleRows</code>, which returns a sample
the same size as the original:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def ResampleRows(df):
    return SampleRows(df, len(df), replace=True)</code></pre>
</div>
</div>
<div class="paragraph">
<p>After resampling, we use the simulated sample to estimate parameters.
The result is two sequences: the estimated intercepts and estimated
slopes.</p>
</div>
<div class="paragraph">
<p>I summarize the sampling distributions by printing the standard error
and confidence interval:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def Summarize(estimates, actual=None):
    mean = thinkstats2.Mean(estimates)
    stderr = thinkstats2.Std(estimates, mu=actual)
    cdf = thinkstats2.Cdf(estimates)
    ci = cdf.ConfidenceInterval(90)
    print('mean, SE, CI', mean, stderr, ci)</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>Summarize</code> takes a sequence of estimates and the actual value. It
prints the mean of the estimates, the standard error and a 90%
confidence interval.</p>
</div>
<div class="paragraph">
<p>For the intercept, the mean estimate is 6.83, with standard error 0.07
and 90% confidence interval (6.71, 6.94). The estimated slope, in more
compact form, is 0.0174, SE 0.0028, CI (0.0126, 0.0220). There is almost
a factor of two between the low and high ends of this CI, so it should
be considered a rough estimate.</p>
</div>
<div class="paragraph">
<p>To visualize the sampling error of the estimate, we could plot all of
the fitted lines, or for a less cluttered representation, plot a 90%
confidence interval for each age. Here’s the code:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def PlotConfidenceIntervals(xs, inters, slopes,
                            percent=90, **options):
    fys_seq = []
    for inter, slope in zip(inters, slopes):
        fxs, fys = thinkstats2.FitLine(xs, inter, slope)
        fys_seq.append(fys)

    p = (100 - percent) / 2
    percents = p, 100 - p
    low, high = thinkstats2.PercentileRows(fys_seq, percents)
    thinkplot.FillBetween(fxs, low, high, **options)</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>xs</code> is the sequence of mother’s age. <code>inters</code> and <code>slopes</code> are
the estimated parameters generated by <code>SamplingDistributions</code>.
<code>percent</code> indicates which confidence interval to plot.</p>
</div>
<div class="paragraph">
<p><code>PlotConfidenceIntervals</code> generates a fitted line for each pair of
<code>inter</code> and <code>slope</code> and stores the results in a sequence,
<code>fys_seq</code>. Then it uses <code>PercentileRows</code> to select the upper and
lower percentiles of <code>y</code> for each value of <code>x</code>. For a 90% confidence
interval, it selects the 5th and 95th percentiles. <code>FillBetween</code> draws
a polygon that fills the space between two lines.</p>
</div>
<div id="linear3" class="imageblock">
<div class="content">
<img src="figs/linear3.png" alt="linear3" height="240">
</div>
<div class="title">Figure 37. 50% and 90% confidence intervals showing variability in the fitted line due to sampling error of <code>inter</code> and <code>slope</code>.</div>
</div>
<div class="paragraph">
<p><a href="#linear3">Figure 37</a> shows the 50% and 90% confidence intervals
for curves fitted to birth weight as a function of mother’s age. The
vertical width of the region represents the effect of sampling error;
the effect is smaller for values near the mean and larger for the
extremes.</p>
</div>
</div>
<div class="sect2">
<h3 id="goodness"><a class="anchor" href="#goodness"></a><a class="link" href="#goodness">10.5. Goodness of fit</a></h3>
<div class="paragraph">
<p>There are several ways to measure the quality of a linear model, or
<strong>goodness of fit</strong>. One of the simplest is the standard deviation of the
residuals.</p>
</div>
<div class="paragraph">
<p>If you use a linear model to make predictions, <code>Std(res)</code> is the root
mean squared error (RMSE) of your predictions. For example, if you use
mother’s age to guess birth weight, the RMSE of your guess would be 1.40
lbs.</p>
</div>
<div class="paragraph">
<p>If you guess birth weight without knowing the mother’s age, the RMSE of
your guess is <code>Std(ys)</code>, which is 1.41 lbs. So in this example,
knowing a mother’s age does not improve the predictions substantially.</p>
</div>
<div class="paragraph">
<p>Another way to measure goodness of fit is the <strong>coefficient of
determination</strong>, usually denoted \(R^2\) and called
&#8220;R-squared&#8221;:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def CoefDetermination(ys, res):
    return 1 - Var(res) / Var(ys)</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>Var(res)</code> is the MSE of your guesses using the model, <code>Var(ys)</code> is
the MSE without it. So their ratio is the fraction of MSE that remains
if you use the model, and \(R^2\) is the fraction of MSE the
model eliminates.</p>
</div>
<div class="paragraph">
<p>For birth weight and mother’s age, \(R^2\) is 0.0047, which
means that mother’s age predicts about half of 1% of variance in birth
weight.</p>
</div>
<div class="paragraph">
<p>There is a simple relationship between the coefficient of determination
and Pearson’s coefficient of correlation: \(R^2 = \rho^2\). For
example, if \(\rho\) is 0.8 or -0.8, \(R^2 = 0.64\).</p>
</div>
<div class="paragraph">
<p>Although \(\rho\) and \(R^2\) are often used to quantify
the strength of a relationship, they are not easy to interpret in terms
of predictive power. In my opinion, <code>Std(res)</code> is the best
representation of the quality of prediction, especially if it is
presented in relation to <code>Std(ys)</code>.</p>
</div>
<div class="paragraph">
<p>For example, when people talk about the validity of the SAT (a
standardized test used for college admission in the U.S.) they often
talk about correlations between SAT scores and other measures of
intelligence.</p>
</div>
<div class="paragraph">
<p>According to one study, there is a Pearson correlation of
\(\rho=0.72\) between total SAT scores and IQ scores, which
sounds like a strong correlation. But \(R^2 = \rho^2 = 0.52\),
so SAT scores account for only 52% of variance in IQ.</p>
</div>
<div class="paragraph">
<p>IQ scores are normalized with <code>Std(ys) = 15</code>, so</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; var_ys = 15**2
&gt;&gt;&gt; rho = 0.72
&gt;&gt;&gt; r2 = rho**2
&gt;&gt;&gt; var_res = (1 - r2) * var_ys
&gt;&gt;&gt; std_res = math.sqrt(var_res)
10.4096</pre>
</div>
</div>
<div class="paragraph">
<p>So using SAT score to predict IQ reduces RMSE from 15 points to 10.4
points. A correlation of 0.72 yields a reduction in RMSE of only 31%.</p>
</div>
<div class="paragraph">
<p>If you see a correlation that looks impressive, remember that
\(R^2\) is a better indicator of reduction in MSE, and reduction
in RMSE is a better indicator of predictive power.</p>
</div>
</div>
<div class="sect2">
<h3 id="_testing_a_linear_model"><a class="anchor" href="#_testing_a_linear_model"></a><a class="link" href="#_testing_a_linear_model">10.6. Testing a linear model</a></h3>
<div class="paragraph">
<p>The effect of mother’s age on birth weight is small, and has little
predictive power. So is it possible that the apparent relationship is
due to chance? There are several ways we might test the results of a
linear fit.</p>
</div>
<div class="paragraph">
<p>One option is to test whether the apparent reduction in MSE is due to
chance. In that case, the test statistic is \(R^2\) and the null
hypothesis is that there is no relationship between the variables. We
can simulate the null hypothesis by permutation, as in
<a href="#corrtest">Section 9.5</a>, when we tested the correlation between
mother’s age and birth weight. In fact, because
\(R^2 = \rho^2\), a one-sided test of \(R^2\) is
equivalent to a two-sided test of \(\rho\). We’ve already done
that test, and found \(p &lt; 0.001\), so we conclude that the
apparent relationship between mother’s age and birth weight is
statistically significant.</p>
</div>
<div class="paragraph">
<p>Another approach is to test whether the apparent slope is due to chance.
The null hypothesis is that the slope is actually zero; in that case we
can model the birth weights as random variations around their mean.
Here’s a HypothesisTest for this model:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">class SlopeTest(thinkstats2.HypothesisTest):

    def TestStatistic(self, data):
        ages, weights = data
        _, slope = thinkstats2.LeastSquares(ages, weights)
        return slope

    def MakeModel(self):
        _, weights = self.data
        self.ybar = weights.mean()
        self.res = weights - self.ybar

    def RunModel(self):
        ages, _ = self.data
        weights = self.ybar + np.random.permutation(self.res)
        return ages, weights</code></pre>
</div>
</div>
<div class="paragraph">
<p>The data are represented as sequences of ages and weights. The test
statistic is the slope estimated by <code>LeastSquares</code>. The model of the
null hypothesis is represented by the mean weight of all babies and the
deviations from the mean. To generate simulated data, we permute the
deviations and add them to the mean.</p>
</div>
<div class="paragraph">
<p>Here’s the code that runs the hypothesis test:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    live, firsts, others = first.MakeFrames()
    live = live.dropna(subset=['agepreg', 'totalwgt_lb'])
    ht = SlopeTest((live.agepreg, live.totalwgt_lb))
    pvalue = ht.PValue()</code></pre>
</div>
</div>
<div class="paragraph">
<p>The p-value is less than \(0.001\), so although the estimated
slope is small, it is unlikely to be due to chance.</p>
</div>
<div class="paragraph">
<p>Estimating the p-value by simulating the null hypothesis is strictly
correct, but there is a simpler alternative. Remember that we already
computed the sampling distribution of the slope, in
<a href="#regest">Section 10.4</a>. To do that, we assumed that the observed
slope was correct and simulated experiments by resampling.</p>
</div>
<div class="paragraph">
<p><a href="#linear4">Figure 38</a> shows the sampling distribution of the slope,
from <a href="#regest">Section 10.4</a>, and the distribution of slopes
generated under the null hypothesis. The sampling distribution is
centered about the estimated slope, 0.017 lbs/year, and the slopes under
the null hypothesis are centered around 0; but other than that, the
distributions are identical. The distributions are also symmetric, for
reasons we will see in <a href="#CLT">Section 14.4</a>.</p>
</div>
<div id="linear4" class="imageblock">
<div class="content">
<img src="figs/linear4.png" alt="linear4" height="240">
</div>
<div class="title">Figure 38. The sampling distribution of the estimated slope and the distribution of slopes generated under the null hypothesis. The vertical lines are at 0 and the observed slope, 0.017 lbs/year.</div>
</div>
<div class="paragraph">
<p>So we could estimate the p-value two ways:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Compute the probability that the slope under the null hypothesis
exceeds the observed slope.</p>
</li>
<li>
<p>Compute the probability that the slope in the sampling distribution
falls below 0. (If the estimated slope were negative, we would compute
the probability that the slope in the sampling distribution exceeds 0.)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The second option is easier because we normally want to compute the
sampling distribution of the parameters anyway. And it is a good
approximation unless the sample size is small <em>and</em> the distribution of
residuals is skewed. Even then, it is usually good enough, because
p-values don’t have to be precise.</p>
</div>
<div class="paragraph">
<p>Here’s the code that estimates the p-value of the slope using the
sampling distribution:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    inters, slopes = SamplingDistributions(live, iters=1001)
    slope_cdf = thinkstats2.Cdf(slopes)
    pvalue = slope_cdf[0]</code></pre>
</div>
</div>
<div class="paragraph">
<p>Again, we find \(p &lt; 0.001\).</p>
</div>
</div>
<div class="sect2">
<h3 id="weighted"><a class="anchor" href="#weighted"></a><a class="link" href="#weighted">10.7. Weighted resampling</a></h3>
<div class="paragraph">
<p>So far we have treated the NSFG data as if it were a representative
sample, but as I mentioned in <a href="#nsfg">Section 1.2</a>, it is not. The
survey deliberately oversamples several groups in order to improve the
chance of getting statistically significant results; that is, in order
to improve the power of tests involving these groups.</p>
</div>
<div class="paragraph">
<p>This survey design is useful for many purposes, but it means that we
cannot use the sample to estimate values for the general population
without accounting for the sampling process.</p>
</div>
<div class="paragraph">
<p>For each respondent, the NSFG data includes a variable called
<code>finalwgt</code>, which is the number of people in the general population
the respondent represents. This value is called a <strong>sampling weight</strong>, or
just &#8220;weight.&#8221;</p>
</div>
<div class="paragraph">
<p>As an example, if you survey 100,000 people in a country of 300 million,
each respondent represents 3,000 people. If you oversample one group by
a factor of 2, each person in the oversampled group would have a lower
weight, about 1500.</p>
</div>
<div class="paragraph">
<p>To correct for oversampling, we can use resampling; that is, we can draw
samples from the survey using probabilities proportional to sampling
weights. Then, for any quantity we want to estimate, we can generate
sampling distributions, standard errors, and confidence intervals. As an
example, I will estimate mean birth weight with and without sampling
weights.</p>
</div>
<div class="paragraph">
<p>In <a href="#regest">Section 10.4</a>, we saw <code>ResampleRows</code>, which chooses
rows from a DataFrame, giving each row the same probability. Now we need
to do the same thing using probabilities proportional to sampling
weights. <code>ResampleRowsWeighted</code> takes a DataFrame, resamples rows
according to the weights in <code>finalwgt</code>, and returns a DataFrame
containing the resampled rows:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def ResampleRowsWeighted(df, column='finalwgt'):
    weights = df[column]
    cdf = Cdf(dict(weights))
    indices = cdf.Sample(len(weights))
    sample = df.loc[indices]
    return sample</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>weights</code> is a Series; converting it to a dictionary makes a map from
the indices to the weights. In <code>cdf</code> the values are indices and the
probabilities are proportional to the weights.</p>
</div>
<div class="paragraph">
<p><code>indices</code> is a sequence of row indices; <code>sample</code> is a DataFrame that
contains the selected rows. Since we sample with replacement, the same
row might appear more than once.</p>
</div>
<div class="paragraph">
<p>Now we can compare the effect of resampling with and without weights.
Without weights, we generate the sampling distribution like this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    estimates = [ResampleRows(live).totalwgt_lb.mean()
                 for _ in range(iters)]</code></pre>
</div>
</div>
<div class="paragraph">
<p>With weights, it looks like this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    estimates = [ResampleRowsWeighted(live).totalwgt_lb.mean()
                 for _ in range(iters)]</code></pre>
</div>
</div>
<div class="paragraph">
<p>The following table summarizes the results:</p>
</div>
<table class="tableblock frame-all grid-all">
<colgroup>
<col>
<col>
<col>
<col>
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top"></th>
<th class="tableblock halign-center valign-top">mean birth weight (lbs)</th>
<th class="tableblock halign-center valign-top">standard error</th>
<th class="tableblock halign-center valign-top">90% CI</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Unweighted</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">7.27</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">0.014</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">(7.24, 7.29)</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Weighted</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">7.35</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">0.014</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">(7.32, 7.37)</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>In this example, the effect of weighting is small but non-negligible.
The difference in estimated means, with and without weighting, is about
0.08 pounds, or 1.3 ounces. This difference is substantially larger than
the standard error of the estimate, 0.014 pounds, which implies that the
difference is not due to chance.</p>
</div>
</div>
<div class="sect2">
<h3 id="_exercises_10"><a class="anchor" href="#_exercises_10"></a><a class="link" href="#_exercises_10">10.8. Exercises</a></h3>
<div class="paragraph">
<p></p>
</div>
<div class="paragraph">
<p>A solution to this exercise is in <code>chap10soln.ipynb</code></p>
</div>
<div class="paragraph">
<p><strong>Exercise 10.1</strong></p>
</div>
<div class="paragraph">
<p>Using the data from the BRFSS, compute the linear least squares fit for
log(weight) versus height. How would you best present the estimated
parameters for a model like this where one of the variables is
log-transformed? If you were trying to guess someone’s weight, how much
would it help to know their height?</p>
</div>
<div class="paragraph">
<p>Like the NSFG, the BRFSS oversamples some groups and provides a sampling
weight for each respondent. In the BRFSS data, the variable name for
these weights is <code>finalwt</code>. Use resampling, with and without weights,
to estimate the mean height of respondents in the BRFSS, the standard
error of the mean, and a 90% confidence interval. How much does correct
weighting affect the estimates?</p>
</div>
</div>
<div class="sect2">
<h3 id="_glossary_10"><a class="anchor" href="#_glossary_10"></a><a class="link" href="#_glossary_10">10.9. Glossary</a></h3>
<div class="ulist">
<ul>
<li>
<p><strong>linear fit</strong>: a line intended to model the relationship between
variables.</p>
</li>
<li>
<p><strong>least squares fit</strong>: A model of a dataset that minimizes the sum of
squares of the residuals.</p>
</li>
<li>
<p><strong>residual</strong>: The deviation of an actual value from a model.</p>
</li>
<li>
<p><strong>goodness of fit</strong>: A measure of how well a model fits data.</p>
</li>
<li>
<p><strong>coefficient of determination</strong>: A statistic intended to quantify
goodness of fit.</p>
</li>
<li>
<p><strong>sampling weight</strong>: A value associated with an observation in a sample
that indicates what part of the population it represents.</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="regression"><a class="anchor" href="#regression"></a><a class="link" href="#regression">11. Regression</a></h2>
<div class="sectionbody">
<div class="paragraph">
<p>The linear least squares fit in the previous chapter is an example of
<strong>regression</strong>, which is the more general problem of fitting any kind of
model to any kind of data. This use of the term &#8220;regression&#8221; is a
historical accident; it is only indirectly related to the original
meaning of the word.</p>
</div>
<div class="paragraph">
<p>The goal of regression analysis is to describe the relationship between
one set of variables, called the <strong>dependent variables</strong>, and another set
of variables, called independent or <strong>explanatory variables</strong>.</p>
</div>
<div class="paragraph">
<p>In the previous chapter we used mother’s age as an explanatory variable
to predict birth weight as a dependent variable. When there is only one
dependent and one explanatory variable, that’s <strong>simple regression</strong>. In
this chapter, we move on to <strong>multiple regression</strong>, with more than one
explanatory variable. If there is more than one dependent variable,
that’s multivariate regression.</p>
</div>
<div class="paragraph">
<p>If the relationship between the dependent and explanatory variable is
linear, that’s <strong>linear regression</strong>. For example, if the dependent
variable is \(y\) and the explanatory variables are
\(x_1\) and \(x_2\), we would write the following linear
regression model:</p>
</div>
<div class="stemblock">
<div class="content">
\[y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \varepsilon\]
</div>
</div>
<div class="paragraph">
<p>where \(\beta_0\) is the intercept, \(\beta_1\) is the
parameter associated with \(x_1\), \(\beta_2\) is the
parameter associated with \(x_2\), and \(\varepsilon\)
is the residual due to random variation or other unknown factors.</p>
</div>
<div class="paragraph">
<p>Given a sequence of values for \(y\) and sequences for
\(x_1\) and \(x_2\), we can find the parameters,
\(\beta_0\), \(\beta_1\), and \(\beta_2\), that
minimize the sum of \(\varepsilon^2\). This process is called
<strong>ordinary least squares</strong>. The computation is similar to
<code>thinkstats2.LeastSquare</code>, but generalized to deal with more than one
explanatory variable. You can find the details at
<a href="https://en.wikipedia.org/wiki/Ordinary_least_squares" class="bare">https://en.wikipedia.org/wiki/Ordinary_least_squares</a></p>
</div>
<div class="paragraph">
<p>The code for this chapter is in <code>regression.py</code>. For information about
downloading and working with this code, see <a href="#code">Using the code</a>.</p>
</div>
<div class="sect2">
<h3 id="statsmodels"><a class="anchor" href="#statsmodels"></a><a class="link" href="#statsmodels">11.1. StatsModels</a></h3>
<div class="paragraph">
<p>In the previous chapter I presented <code>thinkstats2.LeastSquares</code>, an
implementation of simple linear regression intended to be easy to read.
For multiple regression we’ll switch to StatsModels, a Python package
that provides several forms of regression and other analyses. If you are
using Anaconda, you already have StatsModels; otherwise you might have
to install it.</p>
</div>
<div class="paragraph">
<p>As an example, I’ll run the model from the previous chapter with
StatsModels:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    import statsmodels.formula.api as smf

    live, firsts, others = first.MakeFrames()
    formula = 'totalwgt_lb ~ agepreg'
    model = smf.ols(formula, data=live)
    results = model.fit()</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>statsmodels</code> provides two interfaces (APIs); the &#8220;formula&#8221; API uses
strings to identify the dependent and explanatory variables. It uses a
syntax called <code>patsy</code>; in this example, the <code>~</code> operator separates
the dependent variable on the left from the explanatory variables on the
right.</p>
</div>
<div class="paragraph">
<p><code>smf.ols</code> takes the formula string and the DataFrame, <code>live</code>, and
returns an OLS object that represents the model. The name <code>ols</code> stands
for &#8220;ordinary least squares.&#8221;</p>
</div>
<div class="paragraph">
<p>The <code>fit</code> method fits the model to the data and returns a
RegressionResults object that contains the results.</p>
</div>
<div class="paragraph">
<p>The results are also available as attributes. <code>params</code> is a Series
that maps from variable names to their parameters, so we can get the
intercept and slope like this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    inter = results.params['Intercept']
    slope = results.params['agepreg']</code></pre>
</div>
</div>
<div class="paragraph">
<p>The estimated parameters are 6.83 and 0.0175, the same as from
<code>LeastSquares</code>.</p>
</div>
<div class="paragraph">
<p><code>pvalues</code> is a Series that maps from variable names to the associated
p-values, so we can check whether the estimated slope is statistically
significant:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    slope_pvalue = results.pvalues['agepreg']</code></pre>
</div>
</div>
<div class="paragraph">
<p>The p-value associated with <code>agepreg</code> is <code>5.7e-11</code>, which is less
than \(0.001\), as expected.</p>
</div>
<div class="paragraph">
<p><code>results.rsquared</code> contains \(R^2\), which is
\(0.0047\). <code>results</code> also provides <code>f_pvalue</code>, which is the
p-value associated with the model as a whole, similar to testing whether
\(R^2\) is statistically significant.</p>
</div>
<div class="paragraph">
<p>And <code>results</code> provides <code>resid</code>, a sequence of residuals, and
<code>fittedvalues</code>, a sequence of fitted values corresponding to
<code>agepreg</code>.</p>
</div>
<div class="paragraph">
<p>The results object provides <code>summary()</code>, which represents the results
in a readable format.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    print(results.summary())</code></pre>
</div>
</div>
<div class="paragraph">
<p>But it prints a lot of information that is not relevant (yet), so I use
a simpler function called <code>SummarizeResults</code>. Here are the results of
this model:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>Intercept       6.83    (0)
agepreg         0.0175  (5.72e-11)
R^2 0.004738
Std(ys) 1.408
Std(res) 1.405</pre>
</div>
</div>
<div class="paragraph">
<p><code>Std(ys)</code> is the standard deviation of the dependent variable, which
is the RMSE if you have to guess birth weights without the benefit of
any explanatory variables. <code>Std(res)</code> is the standard deviation of the
residuals, which is the RMSE if your guesses are informed by the
mother’s age. As we have already seen, knowing the mother’s age provides
no substantial improvement to the predictions.</p>
</div>
</div>
<div class="sect2">
<h3 id="multiple"><a class="anchor" href="#multiple"></a><a class="link" href="#multiple">11.2. Multiple regression</a></h3>
<div class="paragraph">
<p>In <a href="#birth_weights">Section 4.5</a> we saw that first babies tend to be
lighter than others, and this effect is statistically significant. But
it is a strange result because there is no obvious mechanism that would
cause first babies to be lighter. So we might wonder whether this
relationship is <strong>spurious</strong>.</p>
</div>
<div class="paragraph">
<p>In fact, there is a possible explanation for this effect. We have seen
that birth weight depends on mother’s age, and we might expect that
mothers of first babies are younger than others.</p>
</div>
<div class="paragraph">
<p>With a few calculations we can check whether this explanation is
plausible. Then we’ll use multiple regression to investigate more
carefully. First, let’s see how big the difference in weight is:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">diff_weight = firsts.totalwgt_lb.mean() - others.totalwgt_lb.mean()</code></pre>
</div>
</div>
<div class="paragraph">
<p>First babies are 0.125 lbs lighter, or 2 ounces. And the difference in
ages:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">diff_age = firsts.agepreg.mean() - others.agepreg.mean()</code></pre>
</div>
</div>
<div class="paragraph">
<p>The mothers of first babies are 3.59 years younger. Running the linear
model again, we get the change in birth weight as a function of age:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">results = smf.ols('totalwgt_lb ~ agepreg', data=live).fit()
slope = results.params['agepreg']</code></pre>
</div>
</div>
<div class="paragraph">
<p>The slope is 0.0175 pounds per year. If we multiply the slope by the
difference in ages, we get the expected difference in birth weight for
first babies and others, due to mother’s age:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">slope * diff_age</code></pre>
</div>
</div>
<div class="paragraph">
<p>The result is 0.063, just about half of the observed difference. So we
conclude, tentatively, that the observed difference in birth weight can
be partly explained by the difference in mother’s age.</p>
</div>
<div class="paragraph">
<p>Using multiple regression, we can explore these relationships more
systematically.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    live['isfirst'] = live.birthord == 1
    formula = 'totalwgt_lb ~ isfirst'
    results = smf.ols(formula, data=live).fit()</code></pre>
</div>
</div>
<div class="paragraph">
<p>The first line creates a new column named <code>isfirst</code> that is True for
first babies and false otherwise. Then we fit a model using <code>isfirst</code>
as an explanatory variable.</p>
</div>
<div class="paragraph">
<p>Here are the results:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>Intercept         7.33   (0)
isfirst[T.True]  -0.125  (2.55e-05)
R^2 0.00196</pre>
</div>
</div>
<div class="paragraph">
<p>Because <code>isfirst</code> is a boolean, <code>ols</code> treats it as a <strong>categorical
variable</strong>, which means that the values fall into categories, like True
and False, and should not be treated as numbers. The estimated parameter
is the effect on birth weight when <code>isfirst</code> is true, so the result,
-0.125 lbs, is the difference in birth weight between first babies and
others.</p>
</div>
<div class="paragraph">
<p>The slope and the intercept are statistically significant, which means
that they were unlikely to occur by chance, but the the \(R^2\)
value for this model is small, which means that <code>isfirst</code> doesn’t
account for a substantial part of the variation in birth weight.</p>
</div>
<div class="paragraph">
<p>The results are similar with <code>agepreg</code>:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>Intercept       6.83    (0)
agepreg         0.0175  (5.72e-11)
R^2 0.004738</pre>
</div>
</div>
<div class="paragraph">
<p>Again, the parameters are statistically significant, but \(R^2\)
is low.</p>
</div>
<div class="paragraph">
<p>These models confirm results we have already seen. But now we can fit a
single model that includes both variables. With the formula
<code>totalwgt_lb ~ isfirst + agepreg</code>, we get:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>Intercept        6.91    (0)
isfirst[T.True] -0.0698  (0.0253)
agepreg          0.0154  (3.93e-08)
R^2 0.005289</pre>
</div>
</div>
<div class="paragraph">
<p>In the combined model, the parameter for <code>isfirst</code> is smaller by about
half, which means that part of the apparent effect of <code>isfirst</code> is
actually accounted for by <code>agepreg</code>. And the p-value for <code>isfirst</code>
is about 2.5%, which is on the border of statistical significance.</p>
</div>
<div class="paragraph">
<p>\(R^2\) for this model is a little higher, which indicates that
the two variables together account for more variation in birth weight
than either alone (but not by much).</p>
</div>
</div>
<div class="sect2">
<h3 id="nonlinear"><a class="anchor" href="#nonlinear"></a><a class="link" href="#nonlinear">11.3. Nonlinear relationships</a></h3>
<div class="paragraph">
<p>Remembering that the contribution of <code>agepreg</code> might be nonlinear, we
might consider adding a variable to capture more of this relationship.
One option is to create a column, <code>agepreg2</code>, that contains the
squares of the ages:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    live['agepreg2'] = live.agepreg**2
    formula = 'totalwgt_lb ~ isfirst + agepreg + agepreg2'</code></pre>
</div>
</div>
<div class="paragraph">
<p>Now by estimating parameters for <code>agepreg</code> and <code>agepreg2</code>, we are
effectively fitting a parabola:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>Intercept        5.69     (1.38e-86)
isfirst[T.True] -0.0504   (0.109)
agepreg          0.112    (3.23e-07)
agepreg2        -0.00185  (8.8e-06)
R^2 0.007462</pre>
</div>
</div>
<div class="paragraph">
<p>The parameter of <code>agepreg2</code> is negative, so the parabola curves
downward, which is consistent with the shape of the lines in
<a href="#linear2">Figure 36</a>.</p>
</div>
<div class="paragraph">
<p>The quadratic model of <code>agepreg</code> accounts for more of the variability
in birth weight; the parameter for <code>isfirst</code> is smaller in this model,
and no longer statistically significant.</p>
</div>
<div class="paragraph">
<p>Using computed variables like <code>agepreg2</code> is a common way to fit
polynomials and other functions to data. This process is still
considered linear regression, because the dependent variable is a linear
function of the explanatory variables, regardless of whether some
variables are nonlinear functions of others.</p>
</div>
<div class="paragraph">
<p>The following table summarizes the results of these regressions:</p>
</div>
<table class="tableblock frame-all grid-all">
<colgroup>
<col>
<col>
<col>
<col>
<col>
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top"></th>
<th class="tableblock halign-center valign-top">isfirst</th>
<th class="tableblock halign-center valign-top">agepreg</th>
<th class="tableblock halign-center valign-top">agepreg2</th>
<th class="tableblock halign-center valign-top">\(R^2\)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Model 1</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">-0.125 *</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">–</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">–</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">0.002</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Model 2</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">–</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">0.0175 *</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">–</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">0.0047</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Model 3</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">-0.0698 (0.025)</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">0.0154 *</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">–</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">0.0053</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Model 4</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">-0.0504 (0.11)</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">0.112 *</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">-0.00185 *</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">0.0075</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>The columns in this table are the explanatory variables and the
coefficient of determination, \(R^2\). Each entry is an
estimated parameter and either a p-value in parentheses or an asterisk
to indicate a p-value less that 0.001.</p>
</div>
<div class="paragraph">
<p>We conclude that the apparent difference in birth weight is explained,
at least in part, by the difference in mother’s age. When we include
mother’s age in the model, the effect of <code>isfirst</code> gets smaller, and
the remaining effect might be due to chance.</p>
</div>
<div class="paragraph">
<p>In this example, mother’s age acts as a <strong>control variable</strong>; including
<code>agepreg</code> in the model &#8220;controls for&#8221; the difference in age between
first-time mothers and others, making it possible to isolate the effect
(if any) of <code>isfirst</code>.</p>
</div>
</div>
<div class="sect2">
<h3 id="mining"><a class="anchor" href="#mining"></a><a class="link" href="#mining">11.4. Data mining</a></h3>
<div class="paragraph">
<p>So far we have used regression models for explanation; for example, in
the previous section we discovered that an apparent difference in birth
weight is actually due to a difference in mother’s age. But the
\(R^2\) values of those models is very low, which means that
they have little predictive power. In this section we’ll try to do
better.</p>
</div>
<div class="paragraph">
<p>Suppose one of your co-workers is expecting a baby and there is an
office pool to guess the baby’s birth weight (if you are not familiar
with betting pools, see <a href="https://en.wikipedia.org/wiki/Betting_pool" class="bare">https://en.wikipedia.org/wiki/Betting_pool</a>).</p>
</div>
<div class="paragraph">
<p>Now suppose that you <em>really</em> want to win the pool. What could you do to
improve your chances? Well, the NSFG dataset includes 244 variables
about each pregnancy and another 3087 variables about each respondent.
Maybe some of those variables have predictive power. To find out which
ones are most useful, why not try them all?</p>
</div>
<div class="paragraph">
<p>Testing the variables in the pregnancy table is easy, but in order to
use the variables in the respondent table, we have to match up each
pregnancy with a respondent. In theory we could iterate through the rows
of the pregnancy table, use the <code>caseid</code> to find the corresponding
respondent, and copy the values from the correspondent table into the
pregnancy table. But that would be slow.</p>
</div>
<div class="paragraph">
<p>A better option is to recognize this process as a <strong>join</strong> operation as
defined in SQL and other relational database languages (see
<a href="https://en.wikipedia.org/wiki/Join_(SQL)" class="bare">https://en.wikipedia.org/wiki/Join_(SQL)</a>). Join is implemented as a
DataFrame method, so we can perform the operation like this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    live = live[live.prglngth&gt;30]
    resp = chap01soln.ReadFemResp()
    resp.index = resp.caseid
    join = live.join(resp, on='caseid', rsuffix='_r')</code></pre>
</div>
</div>
<div class="paragraph">
<p>The first line selects records for pregnancies longer than 30 weeks,
assuming that the office pool is formed several weeks before the due
date.</p>
</div>
<div class="paragraph">
<p>The next line reads the respondent file. The result is a DataFrame with
integer indices; in order to look up respondents efficiently, I replace
<code>resp.index</code> with <code>resp.caseid</code>.</p>
</div>
<div class="paragraph">
<p>The <code>join</code> method is invoked on <code>live</code>, which is considered the
&#8220;left&#8221; table, and passed <code>resp</code>, which is the &#8220;right&#8221; table. The
keyword argument <code>on</code> indicates the variable used to match up rows
from the two tables.</p>
</div>
<div class="paragraph">
<p>In this example some column names appear in both tables, so we have to
provide <code>rsuffix</code>, which is a string that will be appended to the
names of overlapping columns from the right table. For example, both
tables have a column named <code>race</code> that encodes the race of the
respondent. The result of the join contains two columns named <code>race</code>
and <code>race_r</code>.</p>
</div>
<div class="paragraph">
<p>The pandas implementation is fast. Joining the NSFG tables takes less
than a second on an ordinary desktop computer. Now we can start testing
variables.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    t = []
    for name in join.columns:
        try:
            if join[name].var() &lt; 1e-7:
                continue

            formula = 'totalwgt_lb ~ agepreg + ' + name
            model = smf.ols(formula, data=join)
            if model.nobs &lt; len(join)/2:
                continue

            results = model.fit()
        except (ValueError, TypeError):
            continue

        t.append((results.rsquared, name))</code></pre>
</div>
</div>
<div class="paragraph">
<p>For each variable we construct a model, compute \(R^2\), and
append the results to a list. The models all include <code>agepreg</code>, since
we already know that it has some predictive power.</p>
</div>
<div class="paragraph">
<p>I check that each explanatory variable has some variability; otherwise
the results of the regression are unreliable. I also check the number of
observations for each model. Variables that contain a large number of
`nan`s are not good candidates for prediction.</p>
</div>
<div class="paragraph">
<p>For most of these variables, we haven’t done any cleaning. Some of them
are encoded in ways that don’t work very well for linear regression. As
a result, we might overlook some variables that would be useful if they
were cleaned properly. But maybe we will find some good candidates.</p>
</div>
</div>
<div class="sect2">
<h3 id="_prediction"><a class="anchor" href="#_prediction"></a><a class="link" href="#_prediction">11.5. Prediction</a></h3>
<div class="paragraph">
<p>The next step is to sort the results and select the variables that yield
the highest values of \(R^2\).</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    t.sort(reverse=True)
    for mse, name in t[:30]:
        print(name, mse)</code></pre>
</div>
</div>
<div class="paragraph">
<p>The first variable on the list is <code>totalwgt_lb</code>, followed by
<code>birthwgt_lb</code>. Obviously, we can’t use birth weight to predict birth
weight.</p>
</div>
<div class="paragraph">
<p>Similarly <code>prglngth</code> has useful predictive power, but for the office
pool we assume pregnancy length (and the related variables) are not
known yet.</p>
</div>
<div class="paragraph">
<p>The first useful predictive variable is <code>babysex</code> which indicates
whether the baby is male or female. In the NSFG dataset, boys are about
0.3 lbs heavier. So, assuming that the sex of the baby is known, we can
use it for prediction.</p>
</div>
<div class="paragraph">
<p>Next is <code>race</code>, which indicates whether the respondent is white,
black, or other. As an explanatory variable, race can be problematic. In
datasets like the NSFG, race is correlated with many other variables,
including income and other socioeconomic factors. In a regression model,
race acts as a <strong>proxy variable</strong>, so apparent correlations with race are
often caused, at least in part, by other factors.</p>
</div>
<div class="paragraph">
<p>The next variable on the list is <code>nbrnaliv</code>, which indicates whether
the pregnancy yielded multiple births. Twins and triplets tend to be
smaller than other babies, so if we know whether our hypothetical
co-worker is expecting twins, that would help.</p>
</div>
<div class="paragraph">
<p>Next on the list is <code>paydu</code>, which indicates whether the respondent
owns her home. It is one of several income-related variables that turn
out to be predictive. In datasets like the NSFG, income and wealth are
correlated with just about everything. In this example, income is
related to diet, health, health care, and other factors likely to affect
birth weight.</p>
</div>
<div class="paragraph">
<p>Some of the other variables on the list are things that would not be
known until later, like <code>bfeedwks</code>, the number of weeks the baby was
breast fed. We can’t use these variables for prediction, but you might
want to speculate on reasons <code>bfeedwks</code> might be correlated with birth
weight.</p>
</div>
<div class="paragraph">
<p>Sometimes you start with a theory and use data to test it. Other times
you start with data and go looking for possible theories. The second
approach, which this section demonstrates, is called <strong>data mining</strong>. An
advantage of data mining is that it can discover unexpected patterns. A
hazard is that many of the patterns it discovers are either random or
spurious.</p>
</div>
<div class="paragraph">
<p>Having identified potential explanatory variables, I tested a few models
and settled on this one:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    formula = ('totalwgt_lb ~ agepreg + C(race) + babysex==1 + '
               'nbrnaliv&gt;1 + paydu==1 + totincr')
    results = smf.ols(formula, data=join).fit()</code></pre>
</div>
</div>
<div class="paragraph">
<p>This formula uses some syntax we have not seen yet: <code>C(race)</code> tells
the formula parser (Patsy) to treat race as a categorical variable, even
though it is encoded numerically.</p>
</div>
<div class="paragraph">
<p>The encoding for <code>babysex</code> is 1 for male, 2 for female; writing
<code>babysex==1</code> converts it to boolean, True for male and false for
female.</p>
</div>
<div class="paragraph">
<p>Similarly <code>nbrnaliv&gt;1</code> is True for multiple births and <code>paydu==1</code> is
True for respondents who own their houses.</p>
</div>
<div class="paragraph">
<p><code>totincr</code> is encoded numerically from 1-14, with each increment
representing about $5000 in annual income. So we can treat these values
as numerical, expressed in units of $5000.</p>
</div>
<div class="paragraph">
<p>Here are the results of the model:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>Intercept               6.63    (0)
C(race)[T.2]            0.357   (5.43e-29)
C(race)[T.3]            0.266   (2.33e-07)
babysex == 1[T.True]    0.295   (5.39e-29)
nbrnaliv &gt; 1[T.True]   -1.38    (5.1e-37)
paydu == 1[T.True]      0.12    (0.000114)
agepreg                 0.00741 (0.0035)
totincr                 0.0122  (0.00188)</pre>
</div>
</div>
<div class="paragraph">
<p>The estimated parameters for race are larger than I expected, especially
since we control for income. The encoding is 1 for black, 2 for white,
and 3 for other. Babies of black mothers are lighter than babies of
other races by 0.27–0.36 lbs.</p>
</div>
<div class="paragraph">
<p>As we’ve already seen, boys are heavier by about 0.3 lbs; twins and
other multiplets are lighter by 1.4 lbs.</p>
</div>
<div class="paragraph">
<p>People who own their homes have heavier babies by about 0.12 lbs, even
when we control for income. The parameter for mother’s age is smaller
than what we saw in <a href="#multiple">Section 11.2</a>, which suggests that
some of the other variables are correlated with age, probably including
<code>paydu</code> and <code>totincr</code>.</p>
</div>
<div class="paragraph">
<p>All of these variables are statistically significant, some with very low
p-values, but \(R^2\) is only 0.06, still quite small. RMSE
without using the model is 1.27 lbs; with the model it drops to 1.23. So
your chance of winning the pool is not substantially improved. Sorry!</p>
</div>
</div>
<div class="sect2">
<h3 id="_logistic_regression"><a class="anchor" href="#_logistic_regression"></a><a class="link" href="#_logistic_regression">11.6. Logistic regression</a></h3>
<div class="paragraph">
<p>In the previous examples, some of the explanatory variables were
numerical and some categorical (including boolean). But the dependent
variable was always numerical.</p>
</div>
<div class="paragraph">
<p>Linear regression can be generalized to handle other kinds of dependent
variables. If the dependent variable is boolean, the generalized model
is called <strong>logistic regression</strong>. If the dependent variable is an integer
count, it’s called <strong>Poisson regression</strong>.</p>
</div>
<div class="paragraph">
<p>As an example of logistic regression, let’s consider a variation on the
office pool scenario. Suppose a friend of yours is pregnant and you want
to predict whether the baby is a boy or a girl. You could use data from
the NSFG to find factors that affect the &#8220;sex ratio&#8221;, which is
conventionally defined to be the probability of having a boy.</p>
</div>
<div class="paragraph">
<p>If you encode the dependent variable numerically, for example 0 for a
girl and 1 for a boy, you could apply ordinary least squares, but there
would be problems. The linear model might be something like this:</p>
</div>
<div class="stemblock">
<div class="content">
\[y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \varepsilon\]
</div>
</div>
<div class="paragraph">
<p>Where \(y\) is the dependent variable, and \(x_1\) and
\(x_2\) are explanatory variables. Then we could find the
parameters that minimize the residuals.</p>
</div>
<div class="paragraph">
<p>The problem with this approach is that it produces predictions that are
hard to interpret. Given estimated parameters and values for
\(x_1\) and \(x_2\), the model might predict
\(y=0.5\), but the only meaningful values of \(y\) are 0
and 1.</p>
</div>
<div class="paragraph">
<p>It is tempting to interpret a result like that as a probability; for
example, we might say that a respondent with particular values of
\(x_1\) and \(x_2\) has a 50% chance of having a boy.
But it is also possible for this model to predict \(y=1.1\) or
\(y=-0.1\), and those are not valid probabilities.</p>
</div>
<div class="paragraph">
<p>Logistic regression avoids this problem by expressing predictions in
terms of <strong>odds</strong> rather than probabilities. If you are not familiar with
odds, &#8220;odds in favor&#8221; of an event is the ratio of the probability it
will occur to the probability that it will not.</p>
</div>
<div class="paragraph">
<p>So if I think my team has a 75% chance of winning, I would say that the
odds in their favor are three to one, because the chance of winning is
three times the chance of losing.</p>
</div>
<div class="paragraph">
<p>Odds and probabilities are different representations of the same
information. Given a probability, you can compute the odds like this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    o = p / (1-p)</code></pre>
</div>
</div>
<div class="paragraph">
<p>Given odds in favor, you can convert to probability like this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    p = o / (o+1)</code></pre>
</div>
</div>
<div class="paragraph">
<p>Logistic regression is based on the following model:</p>
</div>
<div class="stemblock">
<div class="content">
\[\log o = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \varepsilon\]
</div>
</div>
<div class="paragraph">
<p>Where \(o\) is the odds in favor of a particular outcome; in the
example, \(o\) would be the odds of having a boy.</p>
</div>
<div class="paragraph">
<p>Suppose we have estimated the parameters \(\beta_0\),
\(\beta_1\), and \(\beta_2\) (I’ll explain how in a
minute). And suppose we are given values for \(x_1\) and
\(x_2\). We can compute the predicted value of
\(\log o\), and then convert to a probability:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    o = np.exp(log_o)
    p = o / (o+1)</code></pre>
</div>
</div>
<div class="paragraph">
<p>So in the office pool scenario we could compute the predictive
probability of having a boy. But how do we estimate the parameters?</p>
</div>
</div>
<div class="sect2">
<h3 id="_estimating_parameters"><a class="anchor" href="#_estimating_parameters"></a><a class="link" href="#_estimating_parameters">11.7. Estimating parameters</a></h3>
<div class="paragraph">
<p>Unlike linear regression, logistic regression does not have a closed
form solution, so it is solved by guessing an initial solution and
improving it iteratively.</p>
</div>
<div class="paragraph">
<p>The usual goal is to find the maximum-likelihood estimate (MLE), which
is the set of parameters that maximizes the likelihood of the data. For
example, suppose we have the following data:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; y = np.array([0, 1, 0, 1])
&gt;&gt;&gt; x1 = np.array([0, 0, 0, 1])
&gt;&gt;&gt; x2 = np.array([0, 1, 1, 1])</pre>
</div>
</div>
<div class="paragraph">
<p>And we start with the initial guesses \(\beta_0=-1.5\),
\(\beta_1=2.8\), and \(\beta_2=1.1\):</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; beta = [-1.5, 2.8, 1.1]</pre>
</div>
</div>
<div class="paragraph">
<p>Then for each row we can compute <code>log_o</code>:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; log_o = beta[0] + beta[1] * x1 + beta[2] * x2
[-1.5 -0.4 -0.4  2.4]</pre>
</div>
</div>
<div class="paragraph">
<p>And convert from log odds to probabilities:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; o = np.exp(log_o)
[  0.223   0.670   0.670  11.02  ]

&gt;&gt;&gt; p = o / (o+1)
[ 0.182  0.401  0.401  0.916 ]</pre>
</div>
</div>
<div class="paragraph">
<p>Notice that when <code>log_o</code> is greater than 0, <code>o</code> is greater than 1
and <code>p</code> is greater than 0.5.</p>
</div>
<div class="paragraph">
<p>The likelihood of an outcome is <code>p</code> when <code>y==1</code> and <code>1-p</code> when
<code>y==0</code>. For example, if we think the probability of a boy is 0.8 and
the outcome is a boy, the likelihood is 0.8; if the outcome is a girl,
the likelihood is 0.2. We can compute that like this:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; likes = y * p + (1-y) * (1-p)
[ 0.817  0.401  0.598  0.916 ]</pre>
</div>
</div>
<div class="paragraph">
<p>The overall likelihood of the data is the product of <code>likes</code>:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; like = np.prod(likes)
0.18</pre>
</div>
</div>
<div class="paragraph">
<p>For these values of <code>beta</code>, the likelihood of the data is 0.18. The
goal of logistic regression is to find parameters that maximize this
likelihood. To do that, most statistics packages use an iterative solver
like Newton’s method (see
<a href="https://en.wikipedia.org/wiki/Logistic_regression#Model_fitting" class="bare">https://en.wikipedia.org/wiki/Logistic_regression#Model_fitting</a>).</p>
</div>
</div>
<div class="sect2">
<h3 id="implementation"><a class="anchor" href="#implementation"></a><a class="link" href="#implementation">11.8. Implementation</a></h3>
<div class="paragraph">
<p>StatsModels provides an implementation of logistic regression called
<code>logit</code>, named for the function that converts from probability to log
odds. To demonstrate its use, I’ll look for variables that affect the
sex ratio.</p>
</div>
<div class="paragraph">
<p>Again, I load the NSFG data and select pregnancies longer than 30 weeks:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    live, firsts, others = first.MakeFrames()
    df = live[live.prglngth&gt;30]</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>logit</code> requires the dependent variable to be binary (rather than
boolean), so I create a new column named <code>boy</code>, using <code>astype(int)</code>
to convert to binary integers:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    df['boy'] = (df.babysex==1).astype(int)</code></pre>
</div>
</div>
<div class="paragraph">
<p>Factors that have been found to affect sex ratio include parents’ age,
birth order, race, and social status. We can use logistic regression to
see if these effects appear in the NSFG data. I’ll start with the
mother’s age:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    import statsmodels.formula.api as smf

    model = smf.logit('boy ~ agepreg', data=df)
    results = model.fit()
    SummarizeResults(results)</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>logit</code> takes the same arguments as <code>ols</code>, a formula in Patsy syntax
and a DataFrame. The result is a Logit object that represents the model.
It contains attributes called <code>endog</code> and <code>exog</code> that contain the
<strong>endogenous variable</strong>, another name for the dependent variable, and the
<strong>exogenous variables</strong>, another name for the explanatory variables. Since
they are NumPy arrays, it is sometimes convenient to convert them to
DataFrames:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    endog = pandas.DataFrame(model.endog, columns=[model.endog_names])
    exog = pandas.DataFrame(model.exog, columns=model.exog_names)</code></pre>
</div>
</div>
<div class="paragraph">
<p>The result of <code>model.fit</code> is a BinaryResults object, which is similar
to the RegressionResults object we got from <code>ols</code>. Here is a summary
of the results:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>Intercept   0.00579   (0.953)
agepreg     0.00105   (0.783)
R^2 6.144e-06</pre>
</div>
</div>
<div class="paragraph">
<p>The parameter of <code>agepreg</code> is positive, which suggests that older
mothers are more likely to have boys, but the p-value is 0.783, which
means that the apparent effect could easily be due to chance.</p>
</div>
<div class="paragraph">
<p>The coefficient of determination, \(R^2\), does not apply to
logistic regression, but there are several alternatives that are used as
&#8220;pseudo \(R^2\) values.&#8221; These values can be useful for
comparing models. For example, here’s a model that includes several
factors believed to be associated with sex ratio:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    formula = 'boy ~ agepreg + hpagelb + birthord + C(race)'
    model = smf.logit(formula, data=df)
    results = model.fit()</code></pre>
</div>
</div>
<div class="paragraph">
<p>Along with mother’s age, this model includes father’s age at birth
(<code>hpagelb</code>), birth order (<code>birthord</code>), and race as a categorical
variable. Here are the results:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>Intercept      -0.0301     (0.772)
C(race)[T.2]   -0.0224     (0.66)
C(race)[T.3]   -0.000457   (0.996)
agepreg        -0.00267    (0.629)
hpagelb         0.0047     (0.266)
birthord        0.00501    (0.821)
R^2 0.000144</pre>
</div>
</div>
<div class="paragraph">
<p>None of the estimated parameters are statistically significant. The
pseudo-\(R^2\) value is a little higher, but that could be due
to chance.</p>
</div>
</div>
<div class="sect2">
<h3 id="_accuracy"><a class="anchor" href="#_accuracy"></a><a class="link" href="#_accuracy">11.9. Accuracy</a></h3>
<div class="paragraph">
<p>In the office pool scenario, we are most interested in the accuracy of
the model: the number of successful predictions, compared with what we
would expect by chance.</p>
</div>
<div class="paragraph">
<p>In the NSFG data, there are more boys than girls, so the baseline
strategy is to guess &#8220;boy&#8221; every time. The accuracy of this strategy
is just the fraction of boys:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    actual = endog['boy']
    baseline = actual.mean()</code></pre>
</div>
</div>
<div class="paragraph">
<p>Since <code>actual</code> is encoded in binary integers, the mean is the fraction
of boys, which is 0.507.</p>
</div>
<div class="paragraph">
<p>Here’s how we compute the accuracy of the model:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    predict = (results.predict() &gt;= 0.5)
    true_pos = predict * actual
    true_neg = (1 - predict) * (1 - actual)</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>results.predict</code> returns a NumPy array of probabilities, which we
round off to 0 or 1. Multiplying by <code>actual</code> yields 1 if we predict a
boy and get it right, 0 otherwise. So, <code>true_pos</code> indicates &#8220;true
positives&#8221;.</p>
</div>
<div class="paragraph">
<p>Similarly, <code>true_neg</code> indicates the cases where we guess &#8220;girl&#8221; and
get it right. Accuracy is the fraction of correct guesses:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    acc = (sum(true_pos) + sum(true_neg)) / len(actual)</code></pre>
</div>
</div>
<div class="paragraph">
<p>The result is 0.512, slightly better than the baseline, 0.507. But, you
should not take this result too seriously. We used the same data to
build and test the model, so the model may not have predictive power on
new data.</p>
</div>
<div class="paragraph">
<p>Nevertheless, let’s use the model to make a prediction for the office
pool. Suppose your friend is 35 years old and white, her husband is 39,
and they are expecting their third child:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    columns = ['agepreg', 'hpagelb', 'birthord', 'race']
    new = pandas.DataFrame([[35, 39, 3, 2]], columns=columns)
    y = results.predict(new)</code></pre>
</div>
</div>
<div class="paragraph">
<p>To invoke <code>results.predict</code> for a new case, you have to construct a
DataFrame with a column for each variable in the model. The result in
this case is 0.52, so you should guess &#8220;boy.&#8221; But if the model
improves your chances of winning, the difference is very small.</p>
</div>
</div>
<div class="sect2">
<h3 id="_exercises_11"><a class="anchor" href="#_exercises_11"></a><a class="link" href="#_exercises_11">11.10. Exercises</a></h3>
<div class="paragraph">
<p></p>
</div>
<div class="paragraph">
<p>My solution to these exercises is in <code>chap11soln.ipynb</code>.</p>
</div>
<div class="paragraph">
<p><strong>Exercise 11.1</strong></p>
</div>
<div class="paragraph">
<p>Suppose one of your co-workers is expecting a baby and you are
participating in an office pool to predict the date of birth. Assuming
that bets are placed during the 30th week of pregnancy, what variables
could you use to make the best prediction? You should limit yourself to
variables that are known before the birth, and likely to be available to
the people in the pool.</p>
</div>
<div class="paragraph">
<p><strong>Exercise 11.2</strong></p>
</div>
<div class="paragraph">
<p>The Trivers-Willard hypothesis suggests that for many mammals the sex
ratio depends on &#8220;maternal condition&#8221;; that is, factors like the
mother’s age, size, health, and social status. See
<a href="https://en.wikipedia.org/wiki/Trivers-Willard_hypothesis" class="bare">https://en.wikipedia.org/wiki/Trivers-Willard_hypothesis</a></p>
</div>
<div class="paragraph">
<p>Some studies have shown this effect among humans, but results are mixed.
In this chapter we tested some variables related to these factors, but
didn’t find any with a statistically significant effect on sex ratio.</p>
</div>
<div class="paragraph">
<p>As an exercise, use a data mining approach to test the other variables
in the pregnancy and respondent files. Can you find any factors with a
substantial effect?</p>
</div>
<div class="paragraph">
<p><strong>Exercise 11.3</strong></p>
</div>
<div class="paragraph">
<p>If the quantity you want to predict is a count, you can use Poisson
regression, which is implemented in StatsModels with a function called
<code>poisson</code>. It works the same way as <code>ols</code> and <code>logit</code>. As an
exercise, let’s use it to predict how many children a woman has born; in
the NSFG dataset, this variable is called <code>numbabes</code>.</p>
</div>
<div class="paragraph">
<p>Suppose you meet a woman who is 35 years old, black, and a college
graduate whose annual household income exceeds $75,000. How many
children would you predict she has born?</p>
</div>
<div class="paragraph">
<p><strong>Exercise 11.4</strong></p>
</div>
<div class="paragraph">
<p>If the quantity you want to predict is categorical, you can use
multinomial logistic regression, which is implemented in StatsModels
with a function called <code>mnlogit</code>. As an exercise, let’s use it to
guess whether a woman is married, cohabitating, widowed, divorced,
separated, or never married; in the NSFG dataset, marital status is
encoded in a variable called <code>rmarital</code>.</p>
</div>
<div class="paragraph">
<p>Suppose you meet a woman who is 25 years old, white, and a high school
graduate whose annual household income is about $45,000. What is the
probability that she is married, cohabitating, etc?</p>
</div>
</div>
<div class="sect2">
<h3 id="_glossary_11"><a class="anchor" href="#_glossary_11"></a><a class="link" href="#_glossary_11">11.11. Glossary</a></h3>
<div class="ulist">
<ul>
<li>
<p><strong>regression</strong>: One of several related processes for estimating
parameters that fit a model to data.</p>
</li>
<li>
<p><strong>dependent variables</strong>: The variables in a regression model we would
like to predict. Also known as endogenous variables.</p>
</li>
<li>
<p><strong>explanatory variables</strong>: The variables used to predict or explain the
dependent variables. Also known as independent, or exogenous, variables.</p>
</li>
<li>
<p><strong>simple regression</strong>: A regression with only one dependent and one
explanatory variable.</p>
</li>
<li>
<p><strong>multiple regression</strong>: A regression with multiple explanatory
variables, but only one dependent variable.</p>
</li>
<li>
<p><strong>linear regression</strong>: A regression based on a linear model.</p>
</li>
<li>
<p><strong>ordinary least squares</strong>: A linear regression that estimates
parameters by minimizing the squared error of the residuals.</p>
</li>
<li>
<p><strong>spurious relationship</strong>: A relationship between two variables that is
caused by a statistical artifact or a factor, not included in the model,
that is related to both variables.</p>
</li>
<li>
<p><strong>control variable</strong>: A variable included in a regression to eliminate
or &#8220;control for&#8221; a spurious relationship.</p>
</li>
<li>
<p><strong>proxy variable</strong>: A variable that contributes information to a
regression model indirectly because of a relationship with another
factor, so it acts as a proxy for that factor.</p>
</li>
<li>
<p><strong>categorical variable</strong>: A variable that can have one of a discrete set
of unordered values.</p>
</li>
<li>
<p><strong>join</strong>: An operation that combines data from two DataFrames using a
key to match up rows in the two frames.</p>
</li>
<li>
<p><strong>data mining</strong>: An approach to finding relationships between variables
by testing a large number of models.</p>
</li>
<li>
<p><strong>logistic regression</strong>: A form of regression used when the dependent
variable is boolean.</p>
</li>
<li>
<p><strong>Poisson regression</strong>: A form of regression used when the dependent
variable is a non-negative integer, usually a count.</p>
</li>
<li>
<p><strong>odds</strong>: An alternative way of representing a probability,
\(p\), as the ratio of the probability and its complement,
\(p / (1-p)\).</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_time_series_analysis"><a class="anchor" href="#_time_series_analysis"></a><a class="link" href="#_time_series_analysis">12. Time series analysis</a></h2>
<div class="sectionbody">
<div class="paragraph">
<p>A <strong>time series</strong> is a sequence of measurements from a system that varies
in time. One famous example is the &#8220;hockey stick graph&#8221; that shows
global average temperature over time (see
<a href="https://en.wikipedia.org/wiki/Hockey_stick_graph" class="bare">https://en.wikipedia.org/wiki/Hockey_stick_graph</a>).</p>
</div>
<div class="paragraph">
<p>The example I work with in this chapter comes from Zachary M. Jones, a
researcher in political science who studies the black market for
cannabis in the U.S. (<a href="http://zmjones.com/marijuana" class="bare">http://zmjones.com/marijuana</a>). He collected data
from a web site called &#8220;Price of Weed&#8221; that crowdsources market
information by asking participants to report the price, quantity,
quality, and location of cannabis transactions
(<a href="http://www.priceofweed.com/" class="bare">http://www.priceofweed.com/</a>). The goal of his project is to investigate
the effect of policy decisions, like legalization, on markets. I find
this project appealing because it is an example that uses data to
address important political questions, like drug policy.</p>
</div>
<div class="paragraph">
<p>I hope you will find this chapter interesting, but I’ll take this
opportunity to reiterate the importance of maintaining a professional
attitude to data analysis. Whether and which drugs should be illegal are
important and difficult public policy questions; our decisions should be
informed by accurate data reported honestly.</p>
</div>
<div class="paragraph">
<p>The code for this chapter is in <code>timeseries.py</code>. For information about
downloading and working with this code, see <a href="#code">Using the code</a>.</p>
</div>
<div class="sect2">
<h3 id="_importing_and_cleaning"><a class="anchor" href="#_importing_and_cleaning"></a><a class="link" href="#_importing_and_cleaning">12.1. Importing and cleaning</a></h3>
<div class="paragraph">
<p>The data I downloaded from Mr. Jones’s site is in the repository for
this book. The following code reads it into a pandas DataFrame:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    transactions = pandas.read_csv('mj-clean.csv', parse_dates=[5])</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>parse_dates</code> tells <code>read_csv</code> to interpret values in column 5 as
dates and convert them to NumPy <code>datetime64</code> objects.</p>
</div>
<div class="paragraph">
<p>The DataFrame has a row for each reported transaction and the following
columns:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>city: string city name.</p>
</li>
<li>
<p>state: two-letter state abbreviation.</p>
</li>
<li>
<p>price: price paid in dollars.</p>
</li>
<li>
<p>amount: quantity purchased in grams.</p>
</li>
<li>
<p>quality: high, medium, or low quality, as reported by the purchaser.</p>
</li>
<li>
<p>date: date of report, presumed to be shortly after date of purchase.</p>
</li>
<li>
<p>ppg: price per gram, in dollars.</p>
</li>
<li>
<p>state.name: string state name.</p>
</li>
<li>
<p>lat: approximate latitude of the transaction, based on city name.</p>
</li>
<li>
<p>lon: approximate longitude of the transaction.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Each transaction is an event in time, so we could treat this dataset as
a time series. But the events are not equally spaced in time; the number
of transactions reported each day varies from 0 to several hundred. Many
methods used to analyze time series require the measurements to be
equally spaced, or at least things are simpler if they are.</p>
</div>
<div class="paragraph">
<p>In order to demonstrate these methods, I divide the dataset into groups
by reported quality, and then transform each group into an equally
spaced series by computing the mean daily price per gram.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def GroupByQualityAndDay(transactions):
    groups = transactions.groupby('quality')
    dailies = {}
    for name, group in groups:
        dailies[name] = GroupByDay(group)

    return dailies</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>groupby</code> is a DataFrame method that returns a GroupBy object,
<code>groups</code>; used in a for loop, it iterates the names of the groups and
the DataFrames that represent them. Since the values of <code>quality</code> are
<code>low</code>, <code>medium</code>, and <code>high</code>, we get three groups with those names.</p>
</div>
<div class="paragraph">
<p>The loop iterates through the groups and calls <code>GroupByDay</code>, which
computes the daily average price and returns a new DataFrame:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def GroupByDay(transactions, func=np.mean):
    grouped = transactions[['date', 'ppg']].groupby('date')
    daily = grouped.aggregate(func)

    daily['date'] = daily.index
    start = daily.date[0]
    one_year = np.timedelta64(1, 'Y')
    daily['years'] = (daily.date - start) / one_year

    return daily</code></pre>
</div>
</div>
<div class="paragraph">
<p>The parameter, <code>transactions</code>, is a DataFrame that contains columns
<code>date</code> and <code>ppg</code>. We select these two columns, then group by
<code>date</code>.</p>
</div>
<div class="paragraph">
<p>The result, <code>grouped</code>, is a map from each date to a DataFrame that
contains prices reported on that date. <code>aggregate</code> is a GroupBy method
that iterates through the groups and applies a function to each column
of the group; in this case there is only one column, <code>ppg</code>. So the
result of <code>aggregate</code> is a DataFrame with one row for each date and
one column, <code>ppg</code>.</p>
</div>
<div class="paragraph">
<p>Dates in these DataFrames are stored as NumPy <code>datetime64</code> objects,
which are represented as 64-bit integers in nanoseconds. For some of the
analyses coming up, it will be convenient to work with time in more
human-friendly units, like years. So <code>GroupByDay</code> adds a column named
<code>date</code> by copying the <code>index</code>, then adds <code>years</code>, which contains
the number of years since the first transaction as a floating-point
number.</p>
</div>
<div class="paragraph">
<p>The resulting DataFrame has columns <code>ppg</code>, <code>date</code>, and <code>years</code>.</p>
</div>
</div>
<div class="sect2">
<h3 id="_plotting"><a class="anchor" href="#_plotting"></a><a class="link" href="#_plotting">12.2. Plotting</a></h3>
<div class="paragraph">
<p>The result from <code>GroupByQualityAndDay</code> is a map from each quality to a
DataFrame of daily prices. Here’s the code I use to plot the three time
series:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    thinkplot.PrePlot(rows=3)
    for i, (name, daily) in enumerate(dailies.items()):
        thinkplot.SubPlot(i+1)
        title = 'price per gram ($)' if i==0 else ''
        thinkplot.Config(ylim=[0, 20], title=title)
        thinkplot.Scatter(daily.index, daily.ppg, s=10, label=name)
        if i == 2:
            pyplot.xticks(rotation=30)
        else:
            thinkplot.Config(xticks=[])</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>PrePlot</code> with <code>rows=3</code> means that we are planning to make three
subplots laid out in three rows. The loop iterates through the
DataFrames and creates a scatter plot for each. It is common to plot
time series with line segments between the points, but in this case
there are many data points and prices are highly variable, so adding
lines would not help.</p>
</div>
<div class="paragraph">
<p>Since the labels on the x-axis are dates, I use <code>pyplot.xticks</code> to
rotate the &#8220;ticks&#8221; 30 degrees, making them more readable.</p>
</div>
<div id="timeseries1" class="imageblock">
<div class="content">
<img src="figs/timeseries1.png" alt="timeseries1" width="336">
</div>
<div class="title">Figure 39. Time series of daily price per gram for high, medium, and low quality cannabis.</div>
</div>
<div class="paragraph">
<p><a href="#timeseries1">Figure 39</a> shows the result. One apparent feature in
these plots is a gap around November 2013. It’s possible that data
collection was not active during this time, or the data might not be
available. We will consider ways to deal with this missing data later.</p>
</div>
<div class="paragraph">
<p>Visually, it looks like the price of high quality cannabis is declining
during this period, and the price of medium quality is increasing. The
price of low quality might also be increasing, but it is harder to tell,
since it seems to be more volatile. Keep in mind that quality data is
reported by volunteers, so trends over time might reflect changes in how
participants apply these labels.</p>
</div>
</div>
<div class="sect2">
<h3 id="timeregress"><a class="anchor" href="#timeregress"></a><a class="link" href="#timeregress">12.3. Linear regression</a></h3>
<div class="paragraph">
<p>Although there are methods specific to time series analysis, for many
problems a simple way to get started is by applying general-purpose
tools like linear regression. The following function takes a DataFrame
of daily prices and computes a least squares fit, returning the model
and results objects from StatsModels:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def RunLinearModel(daily):
    model = smf.ols('ppg ~ years', data=daily)
    results = model.fit()
    return model, results</code></pre>
</div>
</div>
<div class="paragraph">
<p>Then we can iterate through the qualities and fit a model to each:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    for name, daily in dailies.items():
        model, results = RunLinearModel(daily)
        print(name)
        regression.SummarizeResults(results)</code></pre>
</div>
</div>
<div class="paragraph">
<p>Here are the results:</p>
</div>
<table class="tableblock frame-all grid-all">
<colgroup>
<col>
<col>
<col>
<col>
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">quality</th>
<th class="tableblock halign-left valign-top">intercept</th>
<th class="tableblock halign-left valign-top">slope</th>
<th class="tableblock halign-center valign-top">\(R^2\)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">high</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">13.450</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">-0.708</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">0.444</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">medium</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">8.879</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">0.283</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">0.050</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">low</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">5.362</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">0.568</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">0.030</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>The estimated slopes indicate that the price of high quality cannabis
dropped by about 71 cents per year during the observed interval; for
medium quality it increased by 28 cents per year, and for low quality it
increased by 57 cents per year. These estimates are all statistically
significant with very small p-values.</p>
</div>
<div class="paragraph">
<p>The \(R^2\) value for high quality cannabis is 0.44, which means
that time as an explanatory variable accounts for 44% of the observed
variability in price. For the other qualities, the change in price is
smaller, and variability in prices is higher, so the values of
\(R^2\) are smaller (but still statistically significant).</p>
</div>
<div class="paragraph">
<p>The following code plots the observed prices and the fitted values:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def PlotFittedValues(model, results, label=''):
    years = model.exog[:,1]
    values = model.endog
    thinkplot.Scatter(years, values, s=15, label=label)
    thinkplot.Plot(years, results.fittedvalues, label='model')</code></pre>
</div>
</div>
<div class="paragraph">
<p>As we saw in <a href="#implementation">Section 11.8</a>, <code>model</code> contains
<code>exog</code> and <code>endog</code>, NumPy arrays with the exogenous (explanatory)
and endogenous (dependent) variables.</p>
</div>
<div id="timeseries2" class="imageblock">
<div class="content">
<img src="figs/timeseries2.png" alt="timeseries2" height="240">
</div>
<div class="title">Figure 40. Time series of daily price per gram for high quality cannabis, and a linear least squares fit.</div>
</div>
<div class="paragraph">
<p><code>PlotFittedValues</code> makes a scatter plot of the data points and a line
plot of the fitted values. <a href="#timeseries2">Figure 40</a> shows the
results for high quality cannabis. The model seems like a good linear
fit for the data; nevertheless, linear regression is not the most
appropriate choice for this data:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>First, there is no reason to expect the long-term trend to be a line
or any other simple function. In general, prices are determined by
supply and demand, both of which vary over time in unpredictable ways.</p>
</li>
<li>
<p>Second, the linear regression model gives equal weight to all data,
recent and past. For purposes of prediction, we should probably give
more weight to recent data.</p>
</li>
<li>
<p>Finally, one of the assumptions of linear regression is that the
residuals are uncorrelated noise. With time series data, this assumption
is often false because successive values are correlated.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The next section presents an alternative that is more appropriate for
time series data.</p>
</div>
</div>
<div class="sect2">
<h3 id="_moving_averages"><a class="anchor" href="#_moving_averages"></a><a class="link" href="#_moving_averages">12.4. Moving averages</a></h3>
<div class="paragraph">
<p>Most time series analysis is based on the modeling assumption that the
observed series is the sum of three components:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Trend: A smooth function that captures persistent changes.</p>
</li>
<li>
<p>Seasonality: Periodic variation, possibly including daily, weekly,
monthly, or yearly cycles.</p>
</li>
<li>
<p>Noise: Random variation around the long-term trend.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Regression is one way to extract the trend from a series, as we saw in
the previous section. But if the trend is not a simple function, a good
alternative is a <strong>moving average</strong>. A moving average divides the series
into overlapping regions, called <strong>windows</strong>, and computes the average of
the values in each window.</p>
</div>
<div class="paragraph">
<p>One of the simplest moving averages is the <strong>rolling mean</strong>, which
computes the mean of the values in each window. For example, if the
window size is 3, the rolling mean computes the mean of values 0 through
2, 1 through 3, 2 through 4, etc.</p>
</div>
<div class="paragraph">
<p>pandas provides <code>rolling_mean</code>, which takes a Series and a window size
and returns a new Series.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; series = np.arange(10)
array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

&gt;&gt;&gt; pandas.rolling_mean(series, 3)
array([ nan,  nan,   1,   2,   3,   4,   5,   6,   7,   8])</pre>
</div>
</div>
<div class="paragraph">
<p>The first two values are <code>nan</code>; the next value is the mean of the
first three elements, 0, 1, and 2. The next value is the mean of 1, 2,
and 3. And so on.</p>
</div>
<div class="paragraph">
<p>Before we can apply <code>rolling_mean</code> to the cannabis data, we have to
deal with missing values. There are a few days in the observed interval
with no reported transactions for one or more quality categories, and a
period in 2013 when data collection was not active.</p>
</div>
<div class="paragraph">
<p>In the DataFrames we have used so far, these dates are absent; the index
skips days with no data. For the analysis that follows, we need to
represent this missing data explicitly. We can do that by &#8220;reindexing&#8221;
the DataFrame:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    dates = pandas.date_range(daily.index.min(), daily.index.max())
    reindexed = daily.reindex(dates)</code></pre>
</div>
</div>
<div class="paragraph">
<p>The first line computes a date range that includes every day from the
beginning to the end of the observed interval. The second line creates a
new DataFrame with all of the data from <code>daily</code>, but including rows
for all dates, filled with <code>nan</code>.</p>
</div>
<div class="paragraph">
<p>Now we can plot the rolling mean like this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    roll_mean = pandas.rolling_mean(reindexed.ppg, 30)
    thinkplot.Plot(roll_mean.index, roll_mean)</code></pre>
</div>
</div>
<div class="paragraph">
<p>The window size is 30, so each value in <code>roll_mean</code> is the mean of 30
values from <code>reindexed.ppg</code>.</p>
</div>
<div id="timeseries10" class="imageblock">
<div class="content">
<img src="figs/timeseries10.png" alt="timeseries10" height="240">
</div>
<div class="title">Figure 41. Daily price and a rolling mean (left) and exponentially-weighted moving average (right).</div>
</div>
<div class="paragraph">
<p><a href="#timeseries10">Figure 41</a> (left) shows the result. The rolling
mean seems to do a good job of smoothing out the noise and extracting
the trend. The first 29 values are <code>nan</code>, and wherever there’s a
missing value, it’s followed by another 29 `nan`s. There are ways to
fill in these gaps, but they are a minor nuisance.</p>
</div>
<div class="paragraph">
<p>An alternative is the <strong>exponentially-weighted moving average</strong> (EWMA),
which has two advantages. First, as the name suggests, it computes a
weighted average where the most recent value has the highest weight and
the weights for previous values drop off exponentially. Second, the
pandas implementation of EWMA handles missing values better.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    ewma = pandas.ewma(reindexed.ppg, span=30)
    thinkplot.Plot(ewma.index, ewma)</code></pre>
</div>
</div>
<div class="paragraph">
<p>The <strong>span</strong> parameter corresponds roughly to the window size of a moving
average; it controls how fast the weights drop off, so it determines the
number of points that make a non-negligible contribution to each
average.</p>
</div>
<div class="paragraph">
<p><a href="#timeseries10">Figure 41</a> (right) shows the EWMA for the same
data. It is similar to the rolling mean, where they are both defined,
but it has no missing values, which makes it easier to work with. The
values are noisy at the beginning of the time series, because they are
based on fewer data points.</p>
</div>
</div>
<div class="sect2">
<h3 id="_missing_values"><a class="anchor" href="#_missing_values"></a><a class="link" href="#_missing_values">12.5. Missing values</a></h3>
<div class="paragraph">
<p>Now that we have characterized the trend of the time series, the next
step is to investigate seasonality, which is periodic behavior. Time
series data based on human behavior often exhibits daily, weekly,
monthly, or yearly cycles. In the next section I present methods to test
for seasonality, but they don’t work well with missing data, so we have
to solve that problem first.</p>
</div>
<div class="paragraph">
<p>A simple and common way to fill missing data is to use a moving average.
The Series method <code>fillna</code> does just what we want:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    reindexed.ppg.fillna(ewma, inplace=True)</code></pre>
</div>
</div>
<div class="paragraph">
<p>Wherever <code>reindexed.ppg</code> is <code>nan</code>, <code>fillna</code> replaces it with the
corresponding value from <code>ewma</code>. The <code>inplace</code> flag tells <code>fillna</code>
to modify the existing Series rather than create a new one.</p>
</div>
<div class="paragraph">
<p>A drawback of this method is that it understates the noise in the
series. We can solve that problem by adding in resampled residuals:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    resid = (reindexed.ppg - ewma).dropna()
    fake_data = ewma + thinkstats2.Resample(resid, len(reindexed))
    reindexed.ppg.fillna(fake_data, inplace=True)</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>resid</code> contains the residual values, not including days when <code>ppg</code>
is <code>nan</code>. <code>fake_data</code> contains the sum of the moving average and a
random sample of residuals. Finally, <code>fillna</code> replaces <code>nan</code> with
values from <code>fake_data</code>.</p>
</div>
<div id="timeseries8" class="imageblock">
<div class="content">
<img src="figs/timeseries8.png" alt="timeseries8" height="240">
</div>
<div class="title">Figure 42. Daily price with filled data.</div>
</div>
<div class="paragraph">
<p><a href="#timeseries8">Figure 42</a> shows the result. The filled data is
visually similar to the actual values. Since the resampled residuals are
random, the results are different every time; later we’ll see how to
characterize the error created by missing values.</p>
</div>
</div>
<div class="sect2">
<h3 id="_serial_correlation"><a class="anchor" href="#_serial_correlation"></a><a class="link" href="#_serial_correlation">12.6. Serial correlation</a></h3>
<div class="paragraph">
<p>As prices vary from day to day, you might expect to see patterns. If the
price is high on Monday, you might expect it to be high for a few more
days; and if it’s low, you might expect it to stay low. A pattern like
this is called <strong>serial correlation</strong>, because each value is correlated
with the next one in the series.</p>
</div>
<div class="paragraph">
<p>To compute serial correlation, we can shift the time series by an
interval called a <strong>lag</strong>, and then compute the correlation of the shifted
series with the original:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def SerialCorr(series, lag=1):
    xs = series[lag:]
    ys = series.shift(lag)[lag:]
    corr = thinkstats2.Corr(xs, ys)
    return corr</code></pre>
</div>
</div>
<div class="paragraph">
<p>After the shift, the first <code>lag</code> values are <code>nan</code>, so I use a slice
to remove them before computing <code>Corr</code>.</p>
</div>
<div class="paragraph">
<p>If we apply <code>SerialCorr</code> to the raw price data with lag 1, we find
serial correlation 0.48 for the high quality category, 0.16 for medium
and 0.10 for low. In any time series with a long-term trend, we expect
to see strong serial correlations; for example, if prices are falling,
we expect to see values above the mean in the first half of the series
and values below the mean in the second half.</p>
</div>
<div class="paragraph">
<p>It is more interesting to see if the correlation persists if you
subtract away the trend. For example, we can compute the residual of the
EWMA and then compute its serial correlation:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    ewma = pandas.ewma(reindexed.ppg, span=30)
    resid = reindexed.ppg - ewma
    corr = SerialCorr(resid, 1)</code></pre>
</div>
</div>
<div class="paragraph">
<p>With lag=1, the serial correlations for the de-trended data are -0.022
for high quality, -0.015 for medium, and 0.036 for low. These values are
small, indicating that there is little or no one-day serial correlation
in this series.</p>
</div>
<div class="paragraph">
<p>To check for weekly, monthly, and yearly seasonality, I ran the analysis
again with different lags. Here are the results:</p>
</div>
<table class="tableblock frame-all grid-all">
<colgroup>
<col>
<col>
<col>
<col>
</colgroup>
<thead>
<tr>
<th class="tableblock halign-center valign-top">lag</th>
<th class="tableblock halign-center valign-top">high</th>
<th class="tableblock halign-center valign-top">medium</th>
<th class="tableblock halign-center valign-top">low</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-center valign-top"><p class="tableblock">1</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">-0.029</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">-0.014</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">0.034</p></td>
</tr>
<tr>
<td class="tableblock halign-center valign-top"><p class="tableblock">7</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">0.02</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">-0.042</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">-0.0097</p></td>
</tr>
<tr>
<td class="tableblock halign-center valign-top"><p class="tableblock">30</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">0.014</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">-0.0064</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">-0.013</p></td>
</tr>
<tr>
<td class="tableblock halign-center valign-top"><p class="tableblock">365</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">0.045</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">0.015</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">0.033</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>In the next section we’ll test whether these correlations are
statistically significant (they are not), but at this point we can
tentatively conclude that there are no substantial seasonal patterns in
these series, at least not with these lags.</p>
</div>
</div>
<div class="sect2">
<h3 id="_autocorrelation"><a class="anchor" href="#_autocorrelation"></a><a class="link" href="#_autocorrelation">12.7. Autocorrelation</a></h3>
<div class="paragraph">
<p>If you think a series might have some serial correlation, but you don’t
know which lags to test, you can test them all! The <strong>autocorrelation
function</strong> is a function that maps from lag to the serial correlation
with the given lag. &#8220;Autocorrelation&#8221; is another name for serial
correlation, used more often when the lag is not 1.</p>
</div>
<div class="paragraph">
<p>StatsModels, which we used for linear regression in
<a href="#statsmodels">Section 11.1</a>, also provides functions for time series
analysis, including <code>acf</code>, which computes the autocorrelation
function:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    import statsmodels.tsa.stattools as smtsa
    acf = smtsa.acf(filled.resid, nlags=365, unbiased=True)</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>acf</code> computes serial correlations with lags from 0 through <code>nlags</code>.
The <code>unbiased</code> flag tells <code>acf</code> to correct the estimates for the
sample size. The result is an array of correlations. If we select daily
prices for high quality, and extract correlations for lags 1, 7, 30, and
365, we can confirm that <code>acf</code> and <code>SerialCorr</code> yield approximately
the same results:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; acf[0], acf[1], acf[7], acf[30], acf[365]
1.000, -0.029, 0.020, 0.014, 0.044</pre>
</div>
</div>
<div class="paragraph">
<p>With <code>lag=0</code>, <code>acf</code> computes the correlation of the series with
itself, which is always 1.</p>
</div>
<div id="timeseries9" class="imageblock">
<div class="content">
<img src="figs/timeseries9.png" alt="timeseries9" height="240">
</div>
<div class="title">Figure 43. Autocorrelation function for daily prices (left), and daily prices with a simulated weekly seasonality (right).</div>
</div>
<div class="paragraph">
<p><a href="#timeseries9">Figure 43</a> (left) shows autocorrelation functions
for the three quality categories, with <code>nlags=40</code>. The gray region
shows the normal variability we would expect if there is no actual
autocorrelation; anything that falls outside this range is statistically
significant, with a p-value less than 5%. Since the false positive rate
is 5%, and we are computing 120 correlations (40 lags for each of 3
times series), we expect to see about 6 points outside this region. In
fact, there are 7. We conclude that there are no autocorrelations in
these series that could not be explained by chance.</p>
</div>
<div class="paragraph">
<p>I computed the gray regions by resampling the residuals. You can see my
code in <code>timeseries.py</code>; the function is called
<code>SimulateAutocorrelation</code>.</p>
</div>
<div class="paragraph">
<p>To see what the autocorrelation function looks like when there is a
seasonal component, I generated simulated data by adding a weekly cycle.
Assuming that demand for cannabis is higher on weekends, we might expect
the price to be higher. To simulate this effect, I select dates that
fall on Friday or Saturday and add a random amount to the price, chosen
from a uniform distribution from $0 to $2.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def AddWeeklySeasonality(daily):
    frisat = (daily.index.dayofweek==4) | (daily.index.dayofweek==5)
    fake = daily.copy()
    fake.ppg[frisat] += np.random.uniform(0, 2, frisat.sum())
    return fake</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>frisat</code> is a boolean Series, <code>True</code> if the day of the week is
Friday or Saturday. <code>fake</code> is a new DataFrame, initially a copy of
<code>daily</code>, which we modify by adding random values to <code>ppg</code>.
<code>frisat.sum()</code> is the total number of Fridays and Saturdays, which is
the number of random values we have to generate.</p>
</div>
<div class="paragraph">
<p><a href="#timeseries9">Figure 43</a> (right) shows autocorrelation functions
for prices with this simulated seasonality. As expected, the
correlations are highest when the lag is a multiple of 7. For high and
medium quality, the new correlations are statistically significant. For
low quality they are not, because residuals in this category are large;
the effect would have to be bigger to be visible through the noise.</p>
</div>
</div>
<div class="sect2">
<h3 id="_prediction_2"><a class="anchor" href="#_prediction_2"></a><a class="link" href="#_prediction_2">12.8. Prediction</a></h3>
<div class="paragraph">
<p>Time series analysis can be used to investigate, and sometimes explain,
the behavior of systems that vary in time. It can also make predictions.</p>
</div>
<div class="paragraph">
<p>The linear regressions we used in <a href="#timeregress">Section 12.3</a> can be
used for prediction. The RegressionResults class provides <code>predict</code>,
which takes a DataFrame containing the explanatory variables and returns
a sequence of predictions. Here’s the code:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def GenerateSimplePrediction(results, years):
    n = len(years)
    inter = np.ones(n)
    d = dict(Intercept=inter, years=years)
    predict_df = pandas.DataFrame(d)
    predict = results.predict(predict_df)
    return predict</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>results</code> is a RegressionResults object; <code>years</code> is the sequence of
time values we want predictions for. The function constructs a
DataFrame, passes it to <code>predict</code>, and returns the result.</p>
</div>
<div class="paragraph">
<p>If all we want is a single, best-guess prediction, we’re done. But for
most purposes it is important to quantify error. In other words, we want
to know how accurate the prediction is likely to be.</p>
</div>
<div class="paragraph">
<p>There are three sources of error we should take into account:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Sampling error: The prediction is based on estimated parameters, which
depend on random variation in the sample. If we run the experiment
again, we expect the estimates to vary.</p>
</li>
<li>
<p>Random variation: Even if the estimated parameters are perfect, the
observed data varies randomly around the long-term trend, and we expect
this variation to continue in the future.</p>
</li>
<li>
<p>Modeling error: We have already seen evidence that the long-term trend
is not linear, so predictions based on a linear model will eventually
fail.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Another source of error to consider is unexpected future events.
Agricultural prices are affected by weather, and all prices are affected
by politics and law. As I write this, cannabis is legal in two states
and legal for medical purposes in 20 more. If more states legalize it,
the price is likely to go down. But if the federal government cracks
down, the price might go up.</p>
</div>
<div class="paragraph">
<p>Modeling errors and unexpected future events are hard to quantify.
Sampling error and random variation are easier to deal with, so we’ll do
that first.</p>
</div>
<div class="paragraph">
<p>To quantify sampling error, I use resampling, as we did in
<a href="#regest">Section 10.4</a>. As always, the goal is to use the actual
observations to simulate what would happen if we ran the experiment
again. The simulations are based on the assumption that the estimated
parameters are correct, but the random residuals could have been
different. Here is a function that runs the simulations:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def SimulateResults(daily, iters=101):
    model, results = RunLinearModel(daily)
    fake = daily.copy()

    result_seq = []
    for i in range(iters):
        fake.ppg = results.fittedvalues + Resample(results.resid)
        _, fake_results = RunLinearModel(fake)
        result_seq.append(fake_results)

    return result_seq</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>daily</code> is a DataFrame containing the observed prices; <code>iters</code> is
the number of simulations to run.</p>
</div>
<div class="paragraph">
<p><code>SimulateResults</code> uses <code>RunLinearModel</code>, from
<a href="#timeregress">Section 12.3</a>, to estimate the slope and intercept of
the observed values.</p>
</div>
<div class="paragraph">
<p>Each time through the loop, it generates a &#8220;fake&#8221; dataset by
resampling the residuals and adding them to the fitted values. Then it
runs a linear model on the fake data and stores the RegressionResults
object.</p>
</div>
<div class="paragraph">
<p>The next step is to use the simulated results to generate predictions:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def GeneratePredictions(result_seq, years, add_resid=False):
    n = len(years)
    d = dict(Intercept=np.ones(n), years=years, years2=years**2)
    predict_df = pandas.DataFrame(d)

    predict_seq = []
    for fake_results in result_seq:
        predict = fake_results.predict(predict_df)
        if add_resid:
            predict += thinkstats2.Resample(fake_results.resid, n)
        predict_seq.append(predict)

    return predict_seq</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>GeneratePredictions</code> takes the sequence of results from the previous
step, as well as <code>years</code>, which is a sequence of floats that specifies
the interval to generate predictions for, and <code>add_resid</code>, which
indicates whether it should add resampled residuals to the straight-line
prediction. <code>GeneratePredictions</code> iterates through the sequence of
RegressionResults and generates a sequence of predictions.</p>
</div>
<div id="timeseries4" class="imageblock">
<div class="content">
<img src="figs/timeseries4.png" alt="timeseries4" height="240">
</div>
<div class="title">Figure 44. Predictions based on linear fits, showing variation due to sampling error and prediction error.</div>
</div>
<div class="paragraph">
<p>Finally, here’s the code that plots a 90% confidence interval for the
predictions:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def PlotPredictions(daily, years, iters=101, percent=90):
    result_seq = SimulateResults(daily, iters=iters)
    p = (100 - percent) / 2
    percents = p, 100-p

    predict_seq = GeneratePredictions(result_seq, years, True)
    low, high = thinkstats2.PercentileRows(predict_seq, percents)
    thinkplot.FillBetween(years, low, high, alpha=0.3, color='gray')

    predict_seq = GeneratePredictions(result_seq, years, False)
    low, high = thinkstats2.PercentileRows(predict_seq, percents)
    thinkplot.FillBetween(years, low, high, alpha=0.5, color='gray')</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>PlotPredictions</code> calls <code>GeneratePredictions</code> twice: once with
<code>add_resid=True</code> and again with <code>add_resid=False</code>. It uses
<code>PercentileRows</code> to select the 5th and 95th percentiles for each year,
then plots a gray region between these bounds.</p>
</div>
<div class="paragraph">
<p><a href="#timeseries4">Figure 44</a> shows the result. The dark gray region
represents a 90% confidence interval for the sampling error; that is,
uncertainty about the estimated slope and intercept due to sampling.</p>
</div>
<div class="paragraph">
<p>The lighter region shows a 90% confidence interval for prediction error,
which is the sum of sampling error and random variation.</p>
</div>
<div class="paragraph">
<p>These regions quantify sampling error and random variation, but not
modeling error. In general modeling error is hard to quantify, but in
this case we can address at least one source of error, unpredictable
external events.</p>
</div>
<div class="paragraph">
<p>The regression model is based on the assumption that the system is
<strong>stationary</strong>; that is, that the parameters of the model don’t change
over time. Specifically, it assumes that the slope and intercept are
constant, as well as the distribution of residuals.</p>
</div>
<div class="paragraph">
<p>But looking at the moving averages in <a href="#timeseries10">Figure 41</a>,
it seems like the slope changes at least once during the observed
interval, and the variance of the residuals seems bigger in the first
half than the second.</p>
</div>
<div class="paragraph">
<p>As a result, the parameters we get depend on the interval we observe. To
see how much effect this has on the predictions, we can extend
<code>SimulateResults</code> to use intervals of observation with different start
and end dates. My implementation is in <code>timeseries.py</code>.</p>
</div>
<div id="timeseries5" class="imageblock">
<div class="content">
<img src="figs/timeseries5.png" alt="timeseries5" height="240">
</div>
<div class="title">Figure 45. Predictions based on linear fits, showing variation due to the interval of observation.</div>
</div>
<div class="paragraph">
<p><a href="#timeseries5">Figure 45</a> shows the result for the medium quality
category. The lightest gray area shows a confidence interval that
includes uncertainty due to sampling error, random variation, and
variation in the interval of observation.</p>
</div>
<div class="paragraph">
<p>The model based on the entire interval has positive slope, indicating
that prices were increasing. But the most recent interval shows signs of
decreasing prices, so models based on the most recent data have negative
slope. As a result, the widest predictive interval includes the
possibility of decreasing prices over the next year.</p>
</div>
</div>
<div class="sect2">
<h3 id="_further_reading"><a class="anchor" href="#_further_reading"></a><a class="link" href="#_further_reading">12.9. Further reading</a></h3>
<div class="paragraph">
<p>Time series analysis is a big topic; this chapter has only scratched the
surface. An important tool for working with time series data is
autoregression, which I did not cover here, mostly because it turns out
not to be useful for the example data I worked with.</p>
</div>
<div class="paragraph">
<p>But once you have learned the material in this chapter, you are well
prepared to learn about autoregression. One resource I recommend is
Philipp Janert’s book, <em>Data Analysis with Open Source Tools</em>, O’Reilly
Media, 2011. His chapter on time series analysis picks up where this one
leaves off.</p>
</div>
</div>
<div class="sect2">
<h3 id="_exercises_12"><a class="anchor" href="#_exercises_12"></a><a class="link" href="#_exercises_12">12.10. Exercises</a></h3>
<div class="paragraph">
<p></p>
</div>
<div class="paragraph">
<p>My solution to these exercises is in <code>chap12soln.py</code>.</p>
</div>
<div class="paragraph">
<p><strong>Exercise 12.1</strong></p>
</div>
<div class="paragraph">
<p>The linear model I used in this chapter has the obvious drawback that it
is linear, and there is no reason to expect prices to change linearly
over time. We can add flexibility to the model by adding a quadratic
term, as we did in <a href="#nonlinear">Section 11.3</a>.</p>
</div>
<div class="paragraph">
<p>Use a quadratic model to fit the time series of daily prices, and use
the model to generate predictions. You will have to write a version of
<code>RunLinearModel</code> that runs that quadratic model, but after that you
should be able to reuse code in <code>timeseries.py</code> to generate
predictions.</p>
</div>
<div class="paragraph">
<p><strong>Exercise 12.2</strong></p>
</div>
<div class="paragraph">
<p>Write a definition for a class named <code>SerialCorrelationTest</code> that
extends <code>HypothesisTest</code> from <a href="#hypotest">Section 9.2</a>. It should
take a series and a lag as data, compute the serial correlation of the
series with the given lag, and then compute the p-value of the observed
correlation.</p>
</div>
<div class="paragraph">
<p>Use this class to test whether the serial correlation in raw price data
is statistically significant. Also test the residuals of the linear
model and (if you did the previous exercise), the quadratic model.</p>
</div>
<div class="paragraph">
<p><strong>Exercise 12.3</strong></p>
</div>
<div class="paragraph">
<p>There are several ways to extend the EWMA model to generate predictions.
One of the simplest is something like this:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Compute the EWMA of the time series and use the last point as an
intercept, <code>inter</code>.</p>
</li>
<li>
<p>Compute the EWMA of differences between successive elements in the
time series and use the last point as a slope, <code>slope</code>.</p>
</li>
<li>
<p>To predict values at future times, compute <code>inter + slope * dt</code>,
where <code>dt</code> is the difference between the time of the prediction and
the time of the last observation.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>Use this method to generate predictions for a year after the last
observation. A few hints:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Use <code>timeseries.FillMissing</code> to fill in missing values before
running this analysis. That way the time between consecutive elements is
consistent.</p>
</li>
<li>
<p>Use <code>Series.diff</code> to compute differences between successive
elements.</p>
</li>
<li>
<p>Use <code>reindex</code> to extend the DataFrame index into the future.</p>
</li>
<li>
<p>Use <code>fillna</code> to put your predicted values into the DataFrame.</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_glossary_12"><a class="anchor" href="#_glossary_12"></a><a class="link" href="#_glossary_12">12.11. Glossary</a></h3>
<div class="ulist">
<ul>
<li>
<p><strong>time series</strong>: A dataset where each value is associated with a
timestamp, often a series of measurements and the times they were
collected.</p>
</li>
<li>
<p><strong>window</strong>: A sequence of consecutive values in a time series, often
used to compute a moving average.</p>
</li>
<li>
<p><strong>moving average</strong>: One of several statistics intended to estimate the
underlying trend in a time series by computing averages (of some kind)
for a series of overlapping windows.</p>
</li>
<li>
<p><strong>rolling mean</strong>: A moving average based on the mean value in each
window.</p>
</li>
<li>
<p><strong>exponentially-weighted moving average (EWMA)</strong>: A moving average based
on a weighted mean that gives the highest weight to the most recent
values, and exponentially decreasing weights to earlier values.</p>
</li>
<li>
<p><strong>span</strong>: A parameter of EWMA that determines how quickly the weights
decrease.</p>
</li>
<li>
<p><strong>serial correlation</strong>: Correlation between a time series and a shifted
or lagged version of itself.</p>
</li>
<li>
<p><strong>lag</strong>: The size of the shift in a serial correlation or
autocorrelation.</p>
</li>
<li>
<p><strong>autocorrelation</strong>: A more general term for a serial correlation with
any amount of lag.</p>
</li>
<li>
<p><strong>autocorrelation function</strong>: A function that maps from lag to serial
correlation.</p>
</li>
<li>
<p><strong>stationary</strong>: A model is stationary if the parameters and the
distribution of residuals does not change over time.</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_survival_analysis"><a class="anchor" href="#_survival_analysis"></a><a class="link" href="#_survival_analysis">13. Survival analysis</a></h2>
<div class="sectionbody">
<div class="paragraph">
<p><strong>Survival analysis</strong> is a way to describe how long things last. It is
often used to study human lifetimes, but it also applies to &#8220;survival&#8221;
of mechanical and electronic components, or more generally to intervals
in time before an event.</p>
</div>
<div class="paragraph">
<p>If someone you know has been diagnosed with a life-threatening disease,
you might have seen a &#8220;5-year survival rate,&#8221; which is the probability
of surviving five years after diagnosis. That estimate and related
statistics are the result of survival analysis.</p>
</div>
<div class="paragraph">
<p>The code in this chapter is in <code>survival.py</code>. For information about
downloading and working with this code, see <a href="#code">Using the code</a>.</p>
</div>
<div class="sect2">
<h3 id="survival"><a class="anchor" href="#survival"></a><a class="link" href="#survival">13.1. Survival curves</a></h3>
<div class="paragraph">
<p>The fundamental concept in survival analysis is the <strong>survival curve</strong>,
\(S(t)\), which is a function that maps from a duration,
\(t\), to the probability of surviving longer than
\(t\). If you know the distribution of durations, or
&#8220;lifetimes&#8221;, finding the survival curve is easy; it’s just the
complement of the CDF:</p>
</div>
<div class="stemblock">
<div class="content">
\[S(t) = 1 - \mathrm{CDF}(t)\]
</div>
</div>
<div class="paragraph">
<p>where \(CDF(t)\) is the probability of a lifetime less than or
equal to \(t\).</p>
</div>
<div class="paragraph">
<p>For example, in the NSFG dataset, we know the duration of 11189 complete
pregnancies. We can read this data and compute the CDF:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    preg = nsfg.ReadFemPreg()
    complete = preg.query('outcome in [1, 3, 4]').prglngth
    cdf = thinkstats2.Cdf(complete, label='cdf')</code></pre>
</div>
</div>
<div class="paragraph">
<p>The outcome codes <code>1, 3, 4</code> indicate live birth, stillbirth, and
miscarriage. For this analysis I am excluding induced abortions, ectopic
pregnancies, and pregnancies that were in progress when the respondent
was interviewed.</p>
</div>
<div class="paragraph">
<p>The DataFrame method <code>query</code> takes a boolean expression and evaluates
it for each row, selecting the rows that yield True.</p>
</div>
<div id="survival1" class="imageblock">
<div class="content">
<img src="figs/survival1.png" alt="survival1" height="288">
</div>
<div class="title">Figure 46. Cdf and survival curve for pregnancy length (top), hazard curve (bottom).</div>
</div>
<div class="paragraph">
<p><a href="#survival1">Figure 46</a> (top) shows the CDF of pregnancy length and
its complement, the survival curve. To represent the survival curve, I
define an object that wraps a Cdf and adapts the interface:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">class SurvivalFunction(object):
    def __init__(self, cdf, label=''):
        self.cdf = cdf
        self.label = label or cdf.label

    @property
    def ts(self):
        return self.cdf.xs

    @property
    def ss(self):
        return 1 - self.cdf.ps</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>SurvivalFunction</code> provides two properties: <code>ts</code>, which is the
sequence of lifetimes, and <code>ss</code>, which is the survival curve. In
Python, a &#8220;property&#8221; is a method that can be invoked as if it were a
variable.</p>
</div>
<div class="paragraph">
<p>We can instantiate a <code>SurvivalFunction</code> by passing the CDF of
lifetimes:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    sf = SurvivalFunction(cdf)</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>SurvivalFunction</code> also provides <code>__getitem__</code> and <code>Prob</code>, which
evaluates the survival curve:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python"># class SurvivalFunction

    def __getitem__(self, t):
        return self.Prob(t)

    def Prob(self, t):
        return 1 - self.cdf.Prob(t)</code></pre>
</div>
</div>
<div class="paragraph">
<p>For example, <code>sf[13]</code> is the fraction of pregnancies that proceed past
the first trimester:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; sf[13]
0.86022
&gt;&gt;&gt; cdf[13]
0.13978</pre>
</div>
</div>
<div class="paragraph">
<p>About 86% of pregnancies proceed past the first trimester; about 14% do
not.</p>
</div>
<div class="paragraph">
<p><code>SurvivalFunction</code> provides <code>Render</code>, so we can plot <code>sf</code> using
the functions in <code>thinkplot</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    thinkplot.Plot(sf)</code></pre>
</div>
</div>
<div class="paragraph">
<p><a href="#survival1">Figure 46</a> (top) shows the result. The curve is nearly
flat between 13 and 26 weeks, which shows that few pregnancies end in
the second trimester. And the curve is steepest around 39 weeks, which
is the most common pregnancy length.</p>
</div>
</div>
<div class="sect2">
<h3 id="hazard"><a class="anchor" href="#hazard"></a><a class="link" href="#hazard">13.2. Hazard function</a></h3>
<div class="paragraph">
<p>From the survival curve we can derive the <strong>hazard function</strong>; for
pregnancy lengths, the hazard function maps from a time, \(t\),
to the fraction of pregnancies that continue until \(t\) and
then end at \(t\). To be more precise:</p>
</div>
<div class="stemblock">
<div class="content">
\[\lambda(t) = \frac{S(t) - S(t+1)}{S(t)}\]
</div>
</div>
<div class="paragraph">
<p>The numerator is the fraction of lifetimes that end at \(t\),
which is also \(\mathrm{PMF}(t)\).</p>
</div>
<div class="paragraph">
<p><code>SurvivalFunction</code> provides <code>MakeHazard</code>, which calculates the
hazard function:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python"># class SurvivalFunction

    def MakeHazard(self, label=''):
        ss = self.ss
        lams = {}
        for i, t in enumerate(self.ts[:-1]):
            hazard = (ss[i] - ss[i+1]) / ss[i]
            lams[t] = hazard

        return HazardFunction(lams, label=label)</code></pre>
</div>
</div>
<div class="paragraph">
<p>The <code>HazardFunction</code> object is a wrapper for a pandas Series:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">class HazardFunction(object):

    def __init__(self, d, label=''):
        self.series = pandas.Series(d)
        self.label = label</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>d</code> can be a dictionary or any other type that can initialize a
Series, including another Series. <code>label</code> is a string used to identify
the HazardFunction when plotted.</p>
</div>
<div class="paragraph">
<p><code>HazardFunction</code> provides <code>__getitem__</code>, so we can evaluate it like
this:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; hf = sf.MakeHazard()
&gt;&gt;&gt; hf[39]
0.49689</pre>
</div>
</div>
<div class="paragraph">
<p>So of all pregnancies that proceed until week 39, about 50% end in week
39.</p>
</div>
<div class="paragraph">
<p><a href="#survival1">Figure 46</a> (bottom) shows the hazard function for
pregnancy lengths. For times after week 42, the hazard function is
erratic because it is based on a small number of cases. Other than that
the shape of the curve is as expected: it is highest around 39 weeks,
and a little higher in the first trimester than in the second.</p>
</div>
<div class="paragraph">
<p>The hazard function is useful in its own right, but it is also an
important tool for estimating survival curves, as we’ll see in the next
section.</p>
</div>
</div>
<div class="sect2">
<h3 id="_inferring_survival_curves"><a class="anchor" href="#_inferring_survival_curves"></a><a class="link" href="#_inferring_survival_curves">13.3. Inferring survival curves</a></h3>
<div class="paragraph">
<p>If someone gives you the CDF of lifetimes, it is easy to compute the
survival and hazard functions. But in many real-world scenarios, we
can’t measure the distribution of lifetimes directly. We have to infer
it.</p>
</div>
<div class="paragraph">
<p>For example, suppose you are following a group of patients to see how
long they survive after diagnosis. Not all patients are diagnosed on the
same day, so at any point in time, some patients have survived longer
than others. If some patients have died, we know their survival times.
For patients who are still alive, we don’t know survival times, but we
have a lower bound.</p>
</div>
<div class="paragraph">
<p>If we wait until all patients are dead, we can compute the survival
curve, but if we are evaluating the effectiveness of a new treatment, we
can’t wait that long! We need a way to estimate survival curves using
incomplete information.</p>
</div>
<div class="paragraph">
<p>As a more cheerful example, I will use NSFG data to quantify how long
respondents &#8220;survive&#8221; until they get married for the first time. The
range of respondents’ ages is 14 to 44 years, so the dataset provides a
snapshot of women at different stages in their lives.</p>
</div>
<div class="paragraph">
<p>For women who have been married, the dataset includes the date of their
first marriage and their age at the time. For women who have not been
married, we know their age when interviewed, but have no way of knowing
when or if they will get married.</p>
</div>
<div class="paragraph">
<p>Since we know the age at first marriage for <em>some</em> women, it might be
tempting to exclude the rest and compute the CDF of the known data. That
is a bad idea. The result would be doubly misleading: (1) older women
would be overrepresented, because they are more likely to be married
when interviewed, and (2) married women would be overrepresented! In
fact, this analysis would lead to the conclusion that all women get
married, which is obviously incorrect.</p>
</div>
</div>
<div class="sect2">
<h3 id="_kaplan_meier_estimation"><a class="anchor" href="#_kaplan_meier_estimation"></a><a class="link" href="#_kaplan_meier_estimation">13.4. Kaplan-Meier estimation</a></h3>
<div class="paragraph">
<p>In this example it is not only desirable but necessary to include
observations of unmarried women, which brings us to one of the central
algorithms in survival analysis, <strong>Kaplan-Meier estimation</strong>.</p>
</div>
<div class="paragraph">
<p>The general idea is that we can use the data to estimate the hazard
function, then convert the hazard function to a survival curve. To
estimate the hazard function, we consider, for each age, (1) the number
of women who got married at that age and (2) the number of women &#8220;at
risk&#8221; of getting married, which includes all women who were not married
at an earlier age.</p>
</div>
<div class="paragraph">
<p>Here’s the code:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def EstimateHazardFunction(complete, ongoing, label=''):

    hist_complete = Counter(complete)
    hist_ongoing = Counter(ongoing)

    ts = list(hist_complete | hist_ongoing)
    ts.sort()

    at_risk = len(complete) + len(ongoing)

    lams = pandas.Series(index=ts)
    for t in ts:
        ended = hist_complete[t]
        censored = hist_ongoing[t]

        lams[t] = ended / at_risk
        at_risk -= ended + censored

    return HazardFunction(lams, label=label)</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>complete</code> is the set of complete observations; in this case, the ages
when respondents got married. <code>ongoing</code> is the set of incomplete
observations; that is, the ages of unmarried women when they were
interviewed.</p>
</div>
<div class="paragraph">
<p>First, we precompute <code>hist_complete</code>, which is a Counter that maps
from each age to the number of women married at that age, and
<code>hist_ongoing</code> which maps from each age to the number of unmarried
women interviewed at that age.</p>
</div>
<div class="paragraph">
<p><code>ts</code> is the union of ages when respondents got married and ages when
unmarried women were interviewed, sorted in increasing order.</p>
</div>
<div class="paragraph">
<p><code>at_risk</code> keeps track of the number of respondents considered &#8220;at
risk&#8221; at each age; initially, it is the total number of respondents.</p>
</div>
<div class="paragraph">
<p>The result is stored in a Pandas <code>Series</code> that maps from each age to
the estimated hazard function at that age.</p>
</div>
<div class="paragraph">
<p>Each time through the loop, we consider one age, <code>t</code>, and compute the
number of events that end at <code>t</code> (that is, the number of respondents
married at that age) and the number of events censored at <code>t</code> (that
is, the number of women interviewed at <code>t</code> whose future marriage dates
are censored). In this context, &#8220;censored&#8221; means that the data are
unavailable because of the data collection process.</p>
</div>
<div class="paragraph">
<p>The estimated hazard function is the fraction of the cases at risk that
end at <code>t</code>.</p>
</div>
<div class="paragraph">
<p>At the end of the loop, we subtract from <code>at_risk</code> the number of cases
that ended or were censored at <code>t</code>.</p>
</div>
<div class="paragraph">
<p>Finally, we pass <code>lams</code> to the <code>HazardFunction</code> constructor and
return the result.</p>
</div>
</div>
<div class="sect2">
<h3 id="_the_marriage_curve"><a class="anchor" href="#_the_marriage_curve"></a><a class="link" href="#_the_marriage_curve">13.5. The marriage curve</a></h3>
<div class="paragraph">
<p>To test this function, we have to do some data cleaning and
transformation. The NSFG variables we need are:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>cmbirth</code>: The respondent’s date of birth, known for all
respondents.</p>
</li>
<li>
<p><code>cmintvw</code>: The date the respondent was interviewed, known for all
respondents.</p>
</li>
<li>
<p><code>cmmarrhx</code>: The date the respondent was first married, if applicable
and known.</p>
</li>
<li>
<p><code>evrmarry</code>: 1 if the respondent had been married prior to the date
of interview, 0 otherwise.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The first three variables are encoded in &#8220;century-months&#8221;; that is,
the integer number of months since December 1899. So century-month 1 is
January 1900.</p>
</div>
<div class="paragraph">
<p>First, we read the respondent file and replace invalid values of
<code>cmmarrhx</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    resp = chap01soln.ReadFemResp()
    resp.cmmarrhx.replace([9997, 9998, 9999], np.nan, inplace=True)</code></pre>
</div>
</div>
<div class="paragraph">
<p>Then we compute each respondent’s age when married and age when
interviewed:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    resp['agemarry'] = (resp.cmmarrhx - resp.cmbirth) / 12.0
    resp['age'] = (resp.cmintvw - resp.cmbirth) / 12.0</code></pre>
</div>
</div>
<div class="paragraph">
<p>Next we extract <code>complete</code>, which is the age at marriage for women who
have been married, and <code>ongoing</code>, which is the age at interview for
women who have not:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    complete = resp[resp.evrmarry==1].agemarry
    ongoing = resp[resp.evrmarry==0].age</code></pre>
</div>
</div>
<div class="paragraph">
<p>Finally we compute the hazard function.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    hf = EstimateHazardFunction(complete, ongoing)</code></pre>
</div>
</div>
<div class="paragraph">
<p><a href="#survival2">Figure 47</a> (top) shows the estimated hazard function;
it is low in the teens, higher in the 20s, and declining in the 30s. It
increases again in the 40s, but that is an artifact of the estimation
process; as the number of respondents &#8220;at risk&#8221; decreases, a small
number of women getting married yields a large estimated hazard. The
survival curve will smooth out this noise.</p>
</div>
</div>
<div class="sect2">
<h3 id="_estimating_the_survival_curve"><a class="anchor" href="#_estimating_the_survival_curve"></a><a class="link" href="#_estimating_the_survival_curve">13.6. Estimating the survival curve</a></h3>
<div class="paragraph">
<p>Once we have the hazard function, we can estimate the survival curve.
The chance of surviving past time <code>t</code> is the chance of surviving all
times up through <code>t</code>, which is the cumulative product of the
complementary hazard function:</p>
</div>
<div class="stemblock">
<div class="content">
\[[1-\lambda(0)] [1-\lambda(1)] ... [1-\lambda(t)]\]
</div>
</div>
<div class="paragraph">
<p>The <code>HazardFunction</code> class provides <code>MakeSurvival</code>, which computes
this product:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python"># class HazardFunction:

    def MakeSurvival(self):
        ts = self.series.index
        ss = (1 - self.series).cumprod()
        cdf = thinkstats2.Cdf(ts, 1-ss)
        sf = SurvivalFunction(cdf)
        return sf</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>ts</code> is the sequence of times where the hazard function is estimated.
<code>ss</code> is the cumulative product of the complementary hazard function,
so it is the survival curve.</p>
</div>
<div class="paragraph">
<p>Because of the way <code>SurvivalFunction</code> is implemented, we have to
compute the complement of <code>ss</code>, make a Cdf, and then instantiate a
SurvivalFunction object.</p>
</div>
<div id="survival2" class="imageblock">
<div class="content">
<img src="figs/survival2.png" alt="survival2" height="240">
</div>
<div class="title">Figure 47. Hazard function for age at first marriage (top) and survival curve (bottom).</div>
</div>
<div class="paragraph">
<p><a href="#survival2">Figure 47</a> (bottom) shows the result. The survival
curve is steepest between 25 and 35, when most women get married.
Between 35 and 45, the curve is nearly flat, indicating that women who
do not marry before age 35 are unlikely to get married.</p>
</div>
<div class="paragraph">
<p>A curve like this was the basis of a famous magazine article in 1986;
<em>Newsweek</em> reported that a 40-year old unmarried woman was &#8220;more likely
to be killed by a terrorist&#8221; than get married. These statistics were
widely reported and became part of popular culture, but they were wrong
then (because they were based on faulty analysis) and turned out to be
even more wrong (because of cultural changes that were already in
progress and continued). In 2006, <em>Newsweek</em> ran an another article
admitting that they were wrong.</p>
</div>
<div class="paragraph">
<p>I encourage you to read more about this article, the statistics it was
based on, and the reaction. It should remind you of the ethical
obligation to perform statistical analysis with care, interpret the
results with appropriate skepticism, and present them to the public
accurately and honestly.</p>
</div>
</div>
<div class="sect2">
<h3 id="_confidence_intervals"><a class="anchor" href="#_confidence_intervals"></a><a class="link" href="#_confidence_intervals">13.7. Confidence intervals</a></h3>
<div class="paragraph">
<p>Kaplan-Meier analysis yields a single estimate of the survival curve,
but it is also important to quantify the uncertainty of the estimate. As
usual, there are three possible sources of error: measurement error,
sampling error, and modeling error.</p>
</div>
<div class="paragraph">
<p>In this example, measurement error is probably small. People generally
know when they were born, whether they’ve been married, and when. And
they can be expected to report this information accurately.</p>
</div>
<div class="paragraph">
<p>We can quantify sampling error by resampling. Here’s the code:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def ResampleSurvival(resp, iters=101):
    low, high = resp.agemarry.min(), resp.agemarry.max()
    ts = np.arange(low, high, 1/12.0)

    ss_seq = []
    for i in range(iters):
        sample = thinkstats2.ResampleRowsWeighted(resp)
        hf, sf = EstimateSurvival(sample)
        ss_seq.append(sf.Probs(ts))

    low, high = thinkstats2.PercentileRows(ss_seq, [5, 95])
    thinkplot.FillBetween(ts, low, high)</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>ResampleSurvival</code> takes <code>resp</code>, a DataFrame of respondents, and
<code>iters</code>, the number of times to resample. It computes <code>ts</code>, which is
the sequence of ages where we will evaluate the survival curves.</p>
</div>
<div class="paragraph">
<p>Inside the loop, <code>ResampleSurvival</code>:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Resamples the respondents using <code>ResampleRowsWeighted</code>, which we saw
in <a href="#weighted">Section 10.7</a>.</p>
</li>
<li>
<p>Calls <code>EstimateSurvival</code>, which uses the process in the previous
sections to estimate the hazard and survival curves, and</p>
</li>
<li>
<p>Evaluates the survival curve at each age in <code>ts</code>.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><code>ss_seq</code> is a sequence of evaluated survival curves.
<code>PercentileRows</code> takes this sequence and computes the 5th and 95th
percentiles, returning a 90% confidence interval for the survival curve.</p>
</div>
<div id="survival3" class="imageblock">
<div class="content">
<img src="figs/survival3.png" alt="survival3" height="240">
</div>
<div class="title">Figure 48. Survival curve for age at first marriage (dark line) and a 90% confidence interval based on weighted resampling (gray line).</div>
</div>
<div class="paragraph">
<p><a href="#survival3">Figure 48</a> shows the result along with the survival
curve we estimated in the previous section. The confidence interval
takes into account the sampling weights, unlike the estimated curve. The
discrepancy between them indicates that the sampling weights have a
substantial effect on the estimate—we will have to keep that in mind.</p>
</div>
</div>
<div class="sect2">
<h3 id="_cohort_effects"><a class="anchor" href="#_cohort_effects"></a><a class="link" href="#_cohort_effects">13.8. Cohort effects</a></h3>
<div class="paragraph">
<p>One of the challenges of survival analysis is that different parts of
the estimated curve are based on different groups of respondents. The
part of the curve at time <code>t</code> is based on respondents whose age was at
least <code>t</code> when they were interviewed. So the leftmost part of the
curve includes data from all respondents, but the rightmost part
includes only the oldest respondents.</p>
</div>
<div class="paragraph">
<p>If the relevant characteristics of the respondents are not changing over
time, that’s fine, but in this case it seems likely that marriage
patterns are different for women born in different generations. We can
investigate this effect by grouping respondents according to their
decade of birth. Groups like this, defined by date of birth or similar
events, are called <strong>cohorts</strong>, and differences between the groups are
called <strong>cohort effects</strong>.</p>
</div>
<div class="paragraph">
<p>To investigate cohort effects in the NSFG marriage data, I gathered the
Cycle 6 data from 2002 used throughout this book; the Cycle 7 data from
2006–2010 used in <a href="#replication">Section 9.11</a>; and the Cycle 5 data
from 1995. In total these datasets include 30,769 respondents.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    resp5 = ReadFemResp1995()
    resp6 = ReadFemResp2002()
    resp7 = ReadFemResp2010()
    resps = [resp5, resp6, resp7]</code></pre>
</div>
</div>
<div class="paragraph">
<p>For each DataFrame, <code>resp</code>, I use <code>cmbirth</code> to compute the decade of
birth for each respondent:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    month0 = pandas.to_datetime('1899-12-15')
    dates = [month0 + pandas.DateOffset(months=cm)
             for cm in resp.cmbirth]
    resp['decade'] = (pandas.DatetimeIndex(dates).year - 1900) // 10</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>cmbirth</code> is encoded as the integer number of months since December
1899; <code>month0</code> represents that date as a Timestamp object. For each
birth date, we instantiate a <code>DateOffset</code> that contains the
century-month and add it to <code>month0</code>; the result is a sequence of
Timestamps, which is converted to a <code>DateTimeIndex</code>. Finally, we
extract <code>year</code> and compute decades.</p>
</div>
<div class="paragraph">
<p>To take into account the sampling weights, and also to show variability
due to sampling error, I resample the data, group respondents by decade,
and plot survival curves:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    for i in range(iters):
        samples = [thinkstats2.ResampleRowsWeighted(resp)
                   for resp in resps]
        sample = pandas.concat(samples, ignore_index=True)
        groups = sample.groupby('decade')

        EstimateSurvivalByDecade(groups, alpha=0.2)</code></pre>
</div>
</div>
<div class="paragraph">
<p>Data from the three NSFG cycles use different sampling weights, so I
resample them separately and then use <code>concat</code> to merge them into a
single DataFrame. The parameter <code>ignore_index</code> tells <code>concat</code> not to
match up respondents by index; instead it creates a new index from 0 to
30768.</p>
</div>
<div class="paragraph">
<p><code>EstimateSurvivalByDecade</code> plots survival curves for each cohort:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def EstimateSurvivalByDecade(resp):
    for name, group in groups:
        hf, sf = EstimateSurvival(group)
        thinkplot.Plot(sf)</code></pre>
</div>
</div>
<div id="survival4" class="imageblock">
<div class="content">
<img src="figs/survival4.png" alt="survival4" height="240">
</div>
<div class="title">Figure 49. Survival curves for respondents born during different decades.</div>
</div>
<div class="paragraph">
<p><a href="#survival4">Figure 49</a> shows the results. Several patterns are
visible:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Women born in the 50s married earliest, with successive cohorts
marrying later and later, at least until age 30 or so.</p>
</li>
<li>
<p>Women born in the 60s follow a surprising pattern. Prior to age 25,
they were marrying at slower rates than their predecessors. After age
25, they were marrying faster. By age 32 they had overtaken the 50s
cohort, and at age 44 they are substantially more likely to have
married.</p>
<div class="paragraph">
<p>Women born in the 60s turned 25 between 1985 and 1995. Remembering that
the <em>Newsweek</em> article I mentioned was published in 1986, it is tempting
to imagine that the article triggered a marriage boom. That explanation
would be too pat, but it is possible that the article and the reaction
to it were indicative of a mood that affected the behavior of this
cohort.</p>
</div>
</li>
<li>
<p>The pattern of the 70s cohort is similar. They are less likely than
their predecessors to be married before age 25, but at age 35 they have
caught up with both of the previous cohorts.</p>
</li>
<li>
<p>Women born in the 80s are even less likely to marry before age 25.
What happens after that is not clear; for more data, we have to wait for
the next cycle of the NSFG.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>In the meantime we can make some predictions.</p>
</div>
</div>
<div class="sect2">
<h3 id="_extrapolation"><a class="anchor" href="#_extrapolation"></a><a class="link" href="#_extrapolation">13.9. Extrapolation</a></h3>
<div class="paragraph">
<p>The survival curve for the 70s cohort ends at about age 38; for the 80s
cohort it ends at age 28, and for the 90s cohort we hardly have any data
at all.</p>
</div>
<div class="paragraph">
<p>We can extrapolate these curves by &#8220;borrowing&#8221; data from the previous
cohort. HazardFunction provides a method, <code>Extend</code>, that copies the
tail from another longer HazardFunction:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python"># class HazardFunction

    def Extend(self, other):
        last = self.series.index[-1]
        more = other.series[other.series.index &gt; last]
        self.series = pandas.concat([self.series, more])</code></pre>
</div>
</div>
<div class="paragraph">
<p>As we saw in <a href="#hazard">Section 13.2</a>, the HazardFunction contains a
Series that maps from \(t\) to \(\lambda(t)\).
<code>Extend</code> finds <code>last</code>, which is the last index in <code>self.series</code>,
selects values from <code>other</code> that come later than <code>last</code>, and appends
them onto <code>self.series</code>.</p>
</div>
<div class="paragraph">
<p>Now we can extend the HazardFunction for each cohort, using values from
the predecessor:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def PlotPredictionsByDecade(groups):
    hfs = []
    for name, group in groups:
        hf, sf = EstimateSurvival(group)
        hfs.append(hf)

    thinkplot.PrePlot(len(hfs))
    for i, hf in enumerate(hfs):
        if i &gt; 0:
            hf.Extend(hfs[i-1])
        sf = hf.MakeSurvival()
        thinkplot.Plot(sf)</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>groups</code> is a GroupBy object with respondents grouped by decade of
birth. The first loop computes the HazardFunction for each group.</p>
</div>
<div class="paragraph">
<p>The second loop extends each HazardFunction with values from its
predecessor, which might contain values from the previous group, and so
on. Then it converts each HazardFunction to a SurvivalFunction and plots
it.</p>
</div>
<div id="survival5" class="imageblock">
<div class="content">
<img src="figs/survival5.png" alt="survival5" height="240">
</div>
<div class="title">Figure 50. Survival curves for respondents born during different decades, with predictions for the later cohorts.</div>
</div>
<div class="paragraph">
<p><a href="#survival5">Figure 50</a> shows the results; I’ve removed the 50s
cohort to make the predictions more visible. These results suggest that
by age 40, the most recent cohorts will converge with the 60s cohort,
with fewer than 20% never married.</p>
</div>
</div>
<div class="sect2">
<h3 id="_expected_remaining_lifetime"><a class="anchor" href="#_expected_remaining_lifetime"></a><a class="link" href="#_expected_remaining_lifetime">13.10. Expected remaining lifetime</a></h3>
<div class="paragraph">
<p>Given a survival curve, we can compute the expected remaining lifetime
as a function of current age. For example, given the survival curve of
pregnancy length from <a href="#survival">Section 13.1</a>, we can compute the
expected time until delivery.</p>
</div>
<div class="paragraph">
<p>The first step is to extract the PMF of lifetimes. <code>SurvivalFunction</code>
provides a method that does that:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python"># class SurvivalFunction

    def MakePmf(self, filler=None):
        pmf = thinkstats2.Pmf()
        for val, prob in self.cdf.Items():
            pmf.Set(val, prob)

        cutoff = self.cdf.ps[-1]
        if filler is not None:
            pmf[filler] = 1-cutoff

        return pmf</code></pre>
</div>
</div>
<div class="paragraph">
<p>Remember that the SurvivalFunction contains the Cdf of lifetimes. The
loop copies the values and probabilities from the Cdf into a Pmf.</p>
</div>
<div class="paragraph">
<p><code>cutoff</code> is the highest probability in the Cdf, which is 1 if the Cdf
is complete, and otherwise less than 1. If the Cdf is incomplete, we
plug in the provided value, <code>filler</code>, to cap it off.</p>
</div>
<div class="paragraph">
<p>The Cdf of pregnancy lengths is complete, so we don’t have to worry
about this detail yet.</p>
</div>
<div class="paragraph">
<p>The next step is to compute the expected remaining lifetime, where
&#8220;expected&#8221; means average. <code>SurvivalFunction</code> provides a method that
does that, too:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python"># class SurvivalFunction

    def RemainingLifetime(self, filler=None, func=thinkstats2.Pmf.Mean):
        pmf = self.MakePmf(filler=filler)
        d = {}
        for t in sorted(pmf.Values())[:-1]:
            pmf[t] = 0
            pmf.Normalize()
            d[t] = func(pmf) - t

        return pandas.Series(d)</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>RemainingLifetime</code> takes <code>filler</code>, which is passed along to
<code>MakePmf</code>, and <code>func</code> which is the function used to summarize the
distribution of remaining lifetimes.</p>
</div>
<div class="paragraph">
<p><code>pmf</code> is the Pmf of lifetimes extracted from the SurvivalFunction.
<code>d</code> is a dictionary that contains the results, a map from current age,
<code>t</code>, to expected remaining lifetime.</p>
</div>
<div class="paragraph">
<p>The loop iterates through the values in the Pmf. For each value of <code>t</code>
it computes the conditional distribution of lifetimes, given that the
lifetime exceeds <code>t</code>. It does that by removing values from the Pmf one
at a time and renormalizing the remaining values.</p>
</div>
<div class="paragraph">
<p>Then it uses <code>func</code> to summarize the conditional distribution. In this
example the result is the mean pregnancy length, given that the length
exceeds <code>t</code>. By subtracting <code>t</code> we get the mean remaining pregnancy
length.</p>
</div>
<div id="survival6" class="imageblock">
<div class="content">
<img src="figs/survival6.png" alt="survival6" height="240">
</div>
<div class="title">Figure 51. Expected remaining pregnancy length (left) and years until first marriage (right).</div>
</div>
<div class="paragraph">
<p><a href="#survival6">Figure 51</a> (left) shows the expected remaining
pregnancy length as a function of the current duration. For example,
during Week 0, the expected remaining duration is about 34 weeks. That’s
less than full term (39 weeks) because terminations of pregnancy in the
first trimester bring the average down.</p>
</div>
<div class="paragraph">
<p>The curve drops slowly during the first trimester. After 13 weeks, the
expected remaining lifetime has dropped by only 9 weeks, to 25. After
that the curve drops faster, by about a week per week.</p>
</div>
<div class="paragraph">
<p>Between Week 37 and 42, the curve levels off between 1 and 2 weeks. At
any time during this period, the expected remaining lifetime is the
same; with each week that passes, the destination gets no closer.
Processes with this property are called <strong>memoryless</strong> because the past
has no effect on the predictions. This behavior is the mathematical
basis of the infuriating mantra of obstetrics nurses: &#8220;any day now.&#8221;</p>
</div>
<div class="paragraph">
<p><a href="#survival6">Figure 51</a> (right) shows the median remaining time
until first marriage, as a function of age. For an 11 year-old girl, the
median time until first marriage is about 14 years. The curve decreases
until age 22 when the median remaining time is about 7 years. After that
it increases again: by age 30 it is back where it started, at 14 years.</p>
</div>
<div class="paragraph">
<p>Based on this data, young women have decreasing remaining &#8220;lifetimes&#8221;.
Mechanical components with this property are called <strong>NBUE</strong> for &#8220;new
better than used in expectation,&#8221; meaning that a new part is expected
to last longer.</p>
</div>
<div class="paragraph">
<p>Women older than 22 have increasing remaining time until first marriage.
Components with this property are called <strong>UBNE</strong> for &#8220;used better than
new in expectation.&#8221; That is, the older the part, the longer it is
expected to last. Newborns and cancer patients are also UBNE; their life
expectancy increases the longer they live.</p>
</div>
<div class="paragraph">
<p>For this example I computed median, rather than mean, because the Cdf is
incomplete; the survival curve projects that about 20% of respondents
will not marry before age 44. The age of first marriage for these women
is unknown, and might be non-existent, so we can’t compute a mean.</p>
</div>
<div class="paragraph">
<p>I deal with these unknown values by replacing them with <code>np.inf</code>, a
special value that represents infinity. That makes the mean infinity for
all ages, but the median is well-defined as long as more than 50% of the
remaining lifetimes are finite, which is true until age 30. After that
it is hard to define a meaningful expected remaining lifetime.</p>
</div>
<div class="paragraph">
<p>Here’s the code that computes and plots these functions:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    rem_life1 = sf1.RemainingLifetime()
    thinkplot.Plot(rem_life1)

    func = lambda pmf: pmf.Percentile(50)
    rem_life2 = sf2.RemainingLifetime(filler=np.inf, func=func)
    thinkplot.Plot(rem_life2)</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>sf1</code> is the survival curve for pregnancy length; in this case we can
use the default values for <code>RemainingLifetime</code>.</p>
</div>
<div class="paragraph">
<p><code>sf2</code> is the survival curve for age at first marriage; <code>func</code> is a
function that takes a Pmf and computes its median (50th percentile).</p>
</div>
</div>
<div class="sect2">
<h3 id="_exercises_13"><a class="anchor" href="#_exercises_13"></a><a class="link" href="#_exercises_13">13.11. Exercises</a></h3>
<div class="paragraph">
<p></p>
</div>
<div class="paragraph">
<p>My solution to this exercise is in <code>chap13soln.py</code>.</p>
</div>
<div class="paragraph">
<p><strong>Exercise 13.1</strong></p>
</div>
<div class="paragraph">
<p>In NSFG Cycles 6 and 7, the variable <code>cmdivorcx</code> contains the date of
divorce for the respondent’s first marriage, if applicable, encoded in
century-months.</p>
</div>
<div class="paragraph">
<p>Compute the duration of marriages that have ended in divorce, and the
duration, so far, of marriages that are ongoing. Estimate the hazard and
survival curve for the duration of marriage.</p>
</div>
<div class="paragraph">
<p>Use resampling to take into account sampling weights, and plot data from
several resamples to visualize sampling error.</p>
</div>
<div class="paragraph">
<p>Consider dividing the respondents into groups by decade of birth, and
possibly by age at first marriage.</p>
</div>
</div>
<div class="sect2">
<h3 id="_glossary_13"><a class="anchor" href="#_glossary_13"></a><a class="link" href="#_glossary_13">13.12. Glossary</a></h3>
<div class="ulist">
<ul>
<li>
<p><strong>survival analysis</strong>: A set of methods for describing and predicting
lifetimes, or more generally time until an event occurs.</p>
</li>
<li>
<p><strong>survival curve</strong>: A function that maps from a time, \(t\), to
the probability of surviving past \(t\).</p>
</li>
<li>
<p><strong>hazard function</strong>: A function that maps from \(t\) to the
fraction of people alive until \(t\) who die at \(t\).</p>
</li>
<li>
<p><strong>Kaplan-Meier estimation</strong>: An algorithm for estimating hazard and
survival functions.</p>
</li>
<li>
<p><strong>cohort</strong>: a group of subjects defined by an event, like date of birth,
in a particular interval of time.</p>
</li>
<li>
<p><strong>cohort effect</strong>: a difference between cohorts.</p>
</li>
<li>
<p><strong>NBUE</strong>: A property of expected remaining lifetime, &#8220;New better than
used in expectation.&#8221;</p>
</li>
<li>
<p><strong>UBNE</strong>: A property of expected remaining lifetime, &#8220;Used better than
new in expectation.&#8221;</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="analysis"><a class="anchor" href="#analysis"></a><a class="link" href="#analysis">14. Analytic methods</a></h2>
<div class="sectionbody">
<div class="paragraph">
<p>This book has focused on computational methods like simulation and
resampling, but some of the problems we solved have analytic solutions
that can be much faster.</p>
</div>
<div class="paragraph">
<p>I present some of these methods in this chapter, and explain how they
work. At the end of the chapter, I make suggestions for integrating
computational and analytic methods for exploratory data analysis.</p>
</div>
<div class="paragraph">
<p>The code in this chapter is in <code>normal.py</code>. For information about
downloading and working with this code, see <a href="#code">Using the code</a>.</p>
</div>
<div class="sect2">
<h3 id="why_normal"><a class="anchor" href="#why_normal"></a><a class="link" href="#why_normal">14.1. Normal distributions</a></h3>
<div class="paragraph">
<p>As a motivating example, let’s review the problem from
<a href="#gorilla">Section 8.3</a>:</p>
</div>
<div class="quoteblock">
<blockquote>
<div class="paragraph">
<p>Suppose you are a scientist studying gorillas in a wildlife preserve.
Having weighed 9 gorillas, you find sample mean \(\bar{x}=90\)
kg and sample standard deviation, \(S=7.5\) kg. If you use
\(\bar{x}\) to estimate the population mean, what is the
standard error of the estimate?</p>
</div>
</blockquote>
</div>
<div class="paragraph">
<p>To answer that question, we need the sampling distribution of
\(\bar{x}\). In <a href="#gorilla">Section 8.3</a> we approximated this
distribution by simulating the experiment (weighing 9 gorillas),
computing \(\bar{x}\) for each simulated experiment, and
accumulating the distribution of estimates.</p>
</div>
<div class="paragraph">
<p>The result is an approximation of the sampling distribution. Then we use
the sampling distribution to compute standard errors and confidence
intervals:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>The standard deviation of the sampling distribution is the standard
error of the estimate; in the example, it is about 2.5 kg.</p>
</li>
<li>
<p>The interval between the 5th and 95th percentile of the sampling
distribution is a 90% confidence interval. If we run the experiment many
times, we expect the estimate to fall in this interval 90% of the time.
In the example, the 90% CI is \((86, 94)\) kg.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>Now we’ll do the same calculation analytically. We take advantage of the
fact that the weights of adult female gorillas are roughly normally
distributed. Normal distributions have two properties that make them
amenable for analysis: they are &#8220;closed&#8221; under linear transformation
and addition. To explain what that means, I need some notation.</p>
</div>
<div class="paragraph">
<p>If the distribution of a quantity, \(X\), is normal with
parameters \(\mu\) and \(\sigma\), you can write</p>
</div>
<div class="stemblock">
<div class="content">
\[X \sim \mathcal{N}~(\mu, \sigma^{2})\]
</div>
</div>
<div class="paragraph">
<p>where the symbol \(\sim\) means &#8220;is distributed&#8221; and the
script letter \(\mathcal{N}\) stands for &#8220;normal.&#8221;</p>
</div>
<div class="paragraph">
<p>A linear transformation of \(X\) is something like
\(X' = a X + b\), where \(a\) and \(b\) are real
numbers. A family of distributions is closed under linear transformation
if \(X'\) is in the same family as \(X\). The normal
distribution has this property; if \(X \sim \mathcal{N}~(\mu,
\sigma^2)\),</p>
</div>
<div id="eq:1" class="stemblock">
<div class="content">
\[    X' \sim \mathcal{N}~(a \mu + b, a^{2} \sigma^2) \tag*{1}\]
</div>
</div>
<div class="paragraph">
<p>Normal distributions are also closed under addition. If
\(Z = X + Y\) and
\(X \sim \mathcal{N}~(\mu_{X}, \sigma_{X}^{2})\) and
\(Y \sim \mathcal{N}~(\mu_{Y}, \sigma_{Y}^{2})\) then</p>
</div>
<div id="eq:2" class="stemblock">
<div class="content">
\[    Z \sim \mathcal{N}~(\mu_X + \mu_Y, \sigma_X^2 + \sigma_Y^2) \tag*{2}\]
</div>
</div>
<div class="paragraph">
<p>In the special case \(Z = X + X\), we have</p>
</div>
<div class="stemblock">
<div class="content">
\[Z \sim \mathcal{N}~(2 \mu_X, 2 \sigma_X^2)\]
</div>
</div>
<div class="paragraph">
<p>and in general if we draw \(n\) values of \(X\) and add
them up, we have</p>
</div>
<div id="eq:3" class="stemblock">
<div class="content">
\[    Z \sim \mathcal{N}~(n \mu_X, n \sigma_X^2) \tag*{3}\]
</div>
</div>
</div>
<div class="sect2">
<h3 id="sampling-distributions"><a class="anchor" href="#sampling-distributions"></a><a class="link" href="#sampling-distributions">14.2. Sampling distributions</a></h3>
<div class="paragraph">
<p>Now we have everything we need to compute the sampling distribution of
\(\bar{x}\). Remember that we compute \(\bar{x}\) by
weighing \(n\) gorillas, adding up the total weight, and
dividing by \(n\).</p>
</div>
<div class="paragraph">
<p>Assume that the distribution of gorilla weights, \(X\), is
approximately normal:</p>
</div>
<div class="stemblock">
<div class="content">
\[X \sim \mathcal{N}~(\mu, \sigma^2)\]
</div>
</div>
<div class="paragraph">
<p>If we weigh \(n\) gorillas, the total weight, \(Y\), is
distributed</p>
</div>
<div class="stemblock">
<div class="content">
\[Y \sim \mathcal{N}~(n \mu, n \sigma^2)\]
</div>
</div>
<div class="paragraph">
<p>using <a href="#eq:3">Equation 3</a>. And if we divide by \(n\), the
sample mean, \(Z\), is distributed</p>
</div>
<div class="stemblock">
<div class="content">
\[Z \sim \mathcal{N}~(\mu, \sigma^2/n)\]
</div>
</div>
<div class="paragraph">
<p>using <a href="#eq:1">Equation 1</a> with \(a = 1/n\).</p>
</div>
<div class="paragraph">
<p>The distribution of \(Z\) is the sampling distribution of
\(\bar{x}\). The mean of \(Z\) is \(\mu\), which
shows that \(\bar{x}\) is an unbiased estimate of
\(\mu\). The variance of the sampling distribution is
\(\sigma^2 / n\).</p>
</div>
<div class="paragraph">
<p>So the standard deviation of the sampling distribution, which is the
standard error of the estimate, is \(\sigma / \sqrt{n}\). In the
example, \(\sigma\) is 7.5 kg and \(n\) is 9, so the
standard error is 2.5 kg. That result is consistent with what we
estimated by simulation, but much faster to compute!</p>
</div>
<div class="paragraph">
<p>We can also use the sampling distribution to compute confidence
intervals. A 90% confidence interval for \(\bar{x}\) is the
interval between the 5th and 95th percentiles of \(Z\). Since
\(Z\) is normally distributed, we can compute percentiles by
evaluating the inverse CDF.</p>
</div>
<div class="paragraph">
<p>There is no closed form for the CDF of the normal distribution or its
inverse, but there are fast numerical methods and they are implemented
in SciPy, as we saw in <a href="#normal">Section 5.2</a>. <code>thinkstats2</code>
provides a wrapper function that makes the SciPy function a little
easier to use:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def EvalNormalCdfInverse(p, mu=0, sigma=1):
    return scipy.stats.norm.ppf(p, loc=mu, scale=sigma)</code></pre>
</div>
</div>
<div class="paragraph">
<p>Given a probability, <code>p</code>, it returns the corresponding percentile from
a normal distribution with parameters <code>mu</code> and <code>sigma</code>. For the 90%
confidence interval of \(\bar{x}\), we compute the 5th and 95th
percentiles like this:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; thinkstats2.EvalNormalCdfInverse(0.05, mu=90, sigma=2.5)
85.888

&gt;&gt;&gt; thinkstats2.EvalNormalCdfInverse(0.95, mu=90, sigma=2.5)
94.112</pre>
</div>
</div>
<div class="paragraph">
<p>So if we run the experiment many times, we expect the estimate,
\(\bar{x}\), to fall in the range \((85.9, 94.1)\) about
90% of the time. Again, this is consistent with the result we got by
simulation.</p>
</div>
</div>
<div class="sect2">
<h3 id="_representing_normal_distributions"><a class="anchor" href="#_representing_normal_distributions"></a><a class="link" href="#_representing_normal_distributions">14.3. Representing normal distributions</a></h3>
<div class="paragraph">
<p>To make these calculations easier, I have defined a class called
<code>Normal</code> that represents a normal distribution and encodes the
equations in the previous sections. Here’s what it looks like:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">class Normal(object):

    def __init__(self, mu, sigma2):
        self.mu = mu
        self.sigma2 = sigma2

    def __str__(self):
        return 'N(%g, %g)' % (self.mu, self.sigma2)</code></pre>
</div>
</div>
<div class="paragraph">
<p>So we can instantiate a Normal that represents the distribution of
gorilla weights:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; dist = Normal(90, 7.5**2)
&gt;&gt;&gt; dist
N(90, 56.25)</pre>
</div>
</div>
<div class="paragraph">
<p><code>Normal</code> provides <code>Sum</code>, which takes a sample size, <code>n</code>, and
returns the distribution of the sum of <code>n</code> values, using
<a href="#eq:3">Equation 3</a>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    def Sum(self, n):
        return Normal(n * self.mu, n * self.sigma2)</code></pre>
</div>
</div>
<div class="paragraph">
<p>Normal also knows how to multiply and divide using
<a href="#eq:1">Equation 1</a>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    def __mul__(self, factor):
        return Normal(factor * self.mu, factor**2 * self.sigma2)

    def __div__(self, divisor):
        return 1 / divisor * self</code></pre>
</div>
</div>
<div class="paragraph">
<p>So we can compute the sampling distribution of the mean with sample size
9:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; dist_xbar = dist.Sum(9) / 9
&gt;&gt;&gt; dist_xbar.sigma
2.5</pre>
</div>
</div>
<div class="paragraph">
<p>The standard deviation of the sampling distribution is 2.5 kg, as we saw
in the previous section. Finally, Normal provides <code>Percentile</code>, which
we can use to compute a confidence interval:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; dist_xbar.Percentile(5), dist_xbar.Percentile(95)
85.888 94.113</pre>
</div>
</div>
<div class="paragraph">
<p>And that’s the same answer we got before. We’ll use the Normal class
again later, but before we go on, we need one more bit of analysis.</p>
</div>
</div>
<div class="sect2">
<h3 id="CLT"><a class="anchor" href="#CLT"></a><a class="link" href="#CLT">14.4. Central limit theorem</a></h3>
<div class="paragraph">
<p>As we saw in the previous sections, if we add values drawn from normal
distributions, the distribution of the sum is normal. Most other
distributions don’t have this property; if we add values drawn from
other distributions, the sum does not generally have an analytic
distribution.</p>
</div>
<div class="paragraph">
<p>But if we add up <code>n</code> values from almost any distribution, the
distribution of the sum converges to normal as <code>n</code> increases.</p>
</div>
<div class="paragraph">
<p>More specifically, if the distribution of the values has mean and
standard deviation \(\mu\) and \(\sigma\), the
distribution of the sum is approximately
\(\mathcal{N}(n \mu, n \sigma^2)\).</p>
</div>
<div class="paragraph">
<p>This result is the Central Limit Theorem (CLT). It is one of the most
useful tools for statistical analysis, but it comes with caveats:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The values have to be drawn independently. If they are correlated, the
CLT doesn’t apply (although this is seldom a problem in practice).</p>
</li>
<li>
<p>The values have to come from the same distribution (although this
requirement can be relaxed).</p>
</li>
<li>
<p>The values have to be drawn from a distribution with finite mean and
variance. So most Pareto distributions are out.</p>
</li>
<li>
<p>The rate of convergence depends on the skewness of the distribution.
Sums from an exponential distribution converge for small <code>n</code>. Sums
from a lognormal distribution require larger sizes.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The Central Limit Theorem explains the prevalence of normal
distributions in the natural world. Many characteristics of living
things are affected by genetic and environmental factors whose effect is
additive. The characteristics we measure are the sum of a large number
of small effects, so their distribution tends to be normal.</p>
</div>
</div>
<div class="sect2">
<h3 id="_testing_the_clt"><a class="anchor" href="#_testing_the_clt"></a><a class="link" href="#_testing_the_clt">14.5. Testing the CLT</a></h3>
<div class="paragraph">
<p>To see how the Central Limit Theorem works, and when it doesn’t, let’s
try some experiments. First, we’ll try an exponential distribution:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def MakeExpoSamples(beta=2.0, iters=1000):
    samples = []
    for n in [1, 10, 100]:
        sample = [np.sum(np.random.exponential(beta, n))
                  for _ in range(iters)]
        samples.append((n, sample))
    return samples</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>MakeExpoSamples</code> generates samples of sums of exponential values (I
use &#8220;exponential values&#8221; as shorthand for &#8220;values from an exponential
distribution&#8221;). <code>beta</code> is the parameter of the distribution;
<code>iters</code> is the number of sums to generate.</p>
</div>
<div class="paragraph">
<p>To explain this function, I’ll start from the inside and work my way
out. Each time we call <code>np.random.exponential</code>, we get a sequence of
<code>n</code> exponential values and compute its sum. <code>sample</code> is a list of
these sums, with length <code>iters</code>.</p>
</div>
<div class="paragraph">
<p>It is easy to get <code>n</code> and <code>iters</code> confused: <code>n</code> is the number of
terms in each sum; <code>iters</code> is the number of sums we compute in order
to characterize the distribution of sums.</p>
</div>
<div class="paragraph">
<p>The return value is a list of <code>(n, sample)</code> pairs. For each pair, we
make a normal probability plot:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def NormalPlotSamples(samples, plot=1, ylabel=''):
    for n, sample in samples:
        thinkplot.SubPlot(plot)
        thinkstats2.NormalProbabilityPlot(sample)

        thinkplot.Config(title='n=%d' % n, ylabel=ylabel)
        plot += 1</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>NormalPlotSamples</code> takes the list of pairs from <code>MakeExpoSamples</code>
and generates a row of normal probability plots.</p>
</div>
<div id="normal1" class="imageblock">
<div class="content">
<img src="figs/normal1.png" alt="normal1" height="336">
</div>
<div class="title">Figure 52. Distributions of sums of exponential values (top row) and lognormal values (bottom row).</div>
</div>
<div class="paragraph">
<p><a href="#normal1">Figure 52</a> (top row) shows the results. With <code>n=1</code>,
the distribution of the sum is still exponential, so the normal
probability plot is not a straight line. But with <code>n=10</code> the
distribution of the sum is approximately normal, and with <code>n=100</code> it
is all but indistinguishable from normal.</p>
</div>
<div class="paragraph">
<p><a href="#normal1">Figure 52</a> (bottom row) shows similar results for a
lognormal distribution. Lognormal distributions are generally more
skewed than exponential distributions, so the distribution of sums takes
longer to converge. With <code>n=10</code> the normal probability plot is nowhere
near straight, but with <code>n=100</code> it is approximately normal.</p>
</div>
<div id="normal2" class="imageblock">
<div class="content">
<img src="figs/normal2.png" alt="normal2" height="336">
</div>
<div class="title">Figure 53. Distributions of sums of Pareto values (top row) and correlated exponential values (bottom row).</div>
</div>
<div class="paragraph">
<p>Pareto distributions are even more skewed than lognormal. Depending on
the parameters, many Pareto distributions do not have finite mean and
variance. As a result, the Central Limit Theorem does not apply.
<a href="#normal2">Figure 53</a> (top row) shows distributions of sums of
Pareto values. Even with <code>n=100</code> the normal probability plot is far
from straight.</p>
</div>
<div class="paragraph">
<p>I also mentioned that CLT does not apply if the values are correlated.
To test that, I generate correlated values from an exponential
distribution. The algorithm for generating correlated values is (1)
generate correlated normal values, (2) use the normal CDF to transform
the values to uniform, and (3) use the inverse exponential CDF to
transform the uniform values to exponential.</p>
</div>
<div class="paragraph">
<p><code>GenerateCorrelated</code> returns an iterator of <code>n</code> normal values with
serial correlation <code>rho</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def GenerateCorrelated(rho, n):
    x = random.gauss(0, 1)
    yield x

    sigma = math.sqrt(1 - rho**2)
    for _ in range(n-1):
        x = random.gauss(x*rho, sigma)
        yield x</code></pre>
</div>
</div>
<div class="paragraph">
<p>The first value is a standard normal value. Each subsequent value
depends on its predecessor: if the previous value is <code>x</code>, the mean of
the next value is <code>x*rho</code>, with variance <code>1-rho**2</code>. Note that
<code>random.gauss</code> takes the standard deviation as the second argument,
not variance.</p>
</div>
<div class="paragraph">
<p><code>GenerateExpoCorrelated</code> takes the resulting sequence and transforms
it to exponential:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def GenerateExpoCorrelated(rho, n):
    normal = list(GenerateCorrelated(rho, n))
    uniform = scipy.stats.norm.cdf(normal)
    expo = scipy.stats.expon.ppf(uniform)
    return expo</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>normal</code> is a list of correlated normal values. <code>uniform</code> is a
sequence of uniform values between 0 and 1. <code>expo</code> is a correlated
sequence of exponential values. <code>ppf</code> stands for &#8220;percent point
function,&#8221; which is another name for the inverse CDF.</p>
</div>
<div class="paragraph">
<p><a href="#normal2">Figure 53</a> (bottom row) shows distributions of sums of
correlated exponential values with <code>rho=0.9</code>. The correlation slows
the rate of convergence; nevertheless, with <code>n=100</code> the normal
probability plot is nearly straight. So even though CLT does not
strictly apply when the values are correlated, moderate correlations are
seldom a problem in practice.</p>
</div>
<div class="paragraph">
<p>These experiments are meant to show how the Central Limit Theorem works,
and what happens when it doesn’t. Now let’s see how we can use it.</p>
</div>
</div>
<div class="sect2">
<h3 id="usingCLT"><a class="anchor" href="#usingCLT"></a><a class="link" href="#usingCLT">14.6. Applying the CLT</a></h3>
<div class="paragraph">
<p>To see why the Central Limit Theorem is useful, let’s get back to the
example in <a href="#testdiff">Section 9.3</a>: testing the apparent difference
in mean pregnancy length for first babies and others. As we’ve seen, the
apparent difference is about 0.078 weeks:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; live, firsts, others = first.MakeFrames()
&gt;&gt;&gt; delta = firsts.prglngth.mean() - others.prglngth.mean()
0.078</pre>
</div>
</div>
<div class="paragraph">
<p>Remember the logic of hypothesis testing: we compute a p-value, which is
the probability of the observed difference under the null hypothesis; if
it is small, we conclude that the observed difference is unlikely to be
due to chance.</p>
</div>
<div class="paragraph">
<p>In this example, the null hypothesis is that the distribution of
pregnancy lengths is the same for first babies and others. So we can
compute the sampling distribution of the mean like this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    dist1 = SamplingDistMean(live.prglngth, len(firsts))
    dist2 = SamplingDistMean(live.prglngth, len(others))</code></pre>
</div>
</div>
<div class="paragraph">
<p>Both sampling distributions are based on the same population, which is
the pool of all live births. <code>SamplingDistMean</code> takes this sequence of
values and the sample size, and returns a Normal object representing the
sampling distribution:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def SamplingDistMean(data, n):
    mean, var = data.mean(), data.var()
    dist = Normal(mean, var)
    return dist.Sum(n) / n</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>mean</code> and <code>var</code> are the mean and variance of <code>data</code>. We
approximate the distribution of the data with a normal distribution,
<code>dist</code>.</p>
</div>
<div class="paragraph">
<p>In this example, the data are not normally distributed, so this
approximation is not very good. But then we compute <code>dist.Sum(n) / n</code>,
which is the sampling distribution of the mean of <code>n</code> values. Even if
the data are not normally distributed, the sampling distribution of the
mean is, by the Central Limit Theorem.</p>
</div>
<div class="paragraph">
<p>Next, we compute the sampling distribution of the difference in the
means. The <code>Normal</code> class knows how to perform subtraction using
<a href="#eq:2">Equation 2</a>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    def __sub__(self, other):
        return Normal(self.mu - other.mu,
                      self.sigma2 + other.sigma2)</code></pre>
</div>
</div>
<div class="paragraph">
<p>So we can compute the sampling distribution of the difference like this:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; dist = dist1 - dist2
N(0, 0.0032)</pre>
</div>
</div>
<div class="paragraph">
<p>The mean is 0, which makes sense because we expect two samples from the
same distribution to have the same mean, on average. The variance of the
sampling distribution is 0.0032.</p>
</div>
<div class="paragraph">
<p><code>Normal</code> provides <code>Prob</code>, which evaluates the normal CDF. We can use
<code>Prob</code> to compute the probability of a difference as large as
<code>delta</code> under the null hypothesis:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; 1 - dist.Prob(delta)
0.084</pre>
</div>
</div>
<div class="paragraph">
<p>Which means that the p-value for a one-sided test is 0.84. For a
two-sided test we would also compute</p>
</div>
<div class="literalblock">
<div class="content">
<pre>&gt;&gt;&gt; dist.Prob(-delta)
0.084</pre>
</div>
</div>
<div class="paragraph">
<p>Which is the same because the normal distribution is symmetric. The sum
of the tails is 0.168, which is consistent with the estimate in
<a href="#testdiff">Section 9.3</a>, which was 0.17.</p>
</div>
</div>
<div class="sect2">
<h3 id="_correlation_test"><a class="anchor" href="#_correlation_test"></a><a class="link" href="#_correlation_test">14.7. Correlation test</a></h3>
<div class="paragraph">
<p>In <a href="#corrtest">Section 9.5</a> we used a permutation test for the
correlation between birth weight and mother’s age, and found that it is
statistically significant, with p-value less than 0.001.</p>
</div>
<div class="paragraph">
<p>Now we can do the same thing analytically. The method is based on this
mathematical result: given two variables that are normally distributed
and uncorrelated, if we generate a sample with size \(n\),
compute Pearson’s correlation, \(r\), and then compute the
transformed correlation</p>
</div>
<div class="stemblock">
<div class="content">
\[t = r \sqrt{\frac{n-2}{1-r^2}}\]
</div>
</div>
<div class="paragraph">
<p>the distribution of \(t\) is Student’s t-distribution with
parameter \(n-2\). The t-distribution is an analytic
distribution; the CDF can be computed efficiently using gamma functions.</p>
</div>
<div class="paragraph">
<p>We can use this result to compute the sampling distribution of
correlation under the null hypothesis; that is, if we generate
uncorrelated sequences of normal values, what is the distribution of
their correlation? <code>StudentCdf</code> takes the sample size, <code>n</code>, and
returns the sampling distribution of correlation:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def StudentCdf(n):
    ts = np.linspace(-3, 3, 101)
    ps = scipy.stats.t.cdf(ts, df=n-2)
    rs = ts / np.sqrt(n - 2 + ts**2)
    return thinkstats2.Cdf(rs, ps)</code></pre>
</div>
</div>
<div class="paragraph">
<p><code>ts</code> is a NumPy array of values for \(t\), the transformed
correlation. <code>ps</code> contains the corresponding probabilities, computed
using the CDF of the Student’s t-distribution implemented in SciPy. The
parameter of the t-distribution, <code>df</code>, stands for &#8220;degrees of
freedom.&#8221; I won’t explain that term, but you can read about it at
<a href="http://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics" class="bare">http://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics</a>).</p>
</div>
<div id="normal4" class="imageblock">
<div class="content">
<img src="figs/normal4.png" alt="normal4" height="240">
</div>
<div class="title">Figure 54. Sampling distribution of correlations for uncorrelated normal variables.</div>
</div>
<div class="paragraph">
<p>To get from <code>ts</code> to the correlation coefficients, <code>rs</code>, we apply the
inverse transform,</p>
</div>
<div class="stemblock">
<div class="content">
\[r = t / \sqrt{n - 2 + t^2}\]
</div>
</div>
<div class="paragraph">
<p>The result is the sampling distribution of \(r\) under the null
hypothesis. <a href="#normal4">Figure 54</a> shows this distribution along
with the distribution we generated in <a href="#corrtest">Section 9.5</a> by
resampling. They are nearly identical. Although the actual distributions
are not normal, Pearson’s coefficient of correlation is based on sample
means and variances. By the Central Limit Theorem, these moment-based
statistics are normally distributed even if the data are not.</p>
</div>
<div class="paragraph">
<p>From <a href="#normal4">Figure 54</a>, we can see that the observed
correlation, 0.07, is unlikely to occur if the variables are actually
uncorrelated. Using the analytic distribution, we can compute just how
unlikely:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    t = r * math.sqrt((n-2) / (1-r**2))
    p_value = 1 - scipy.stats.t.cdf(t, df=n-2)</code></pre>
</div>
</div>
<div class="paragraph">
<p>We compute the value of <code>t</code> that corresponds to <code>r=0.07</code>, and then
evaluate the t-distribution at <code>t</code>. The result is <code>2.9e-11</code>. This
example demonstrates an advantage of the analytic method: we can compute
very small p-values. But in practice it usually doesn’t matter.</p>
</div>
</div>
<div class="sect2">
<h3 id="_chi_squared_test"><a class="anchor" href="#_chi_squared_test"></a><a class="link" href="#_chi_squared_test">14.8. Chi-squared test</a></h3>
<div class="paragraph">
<p>In <a href="#casino2">Section 9.7</a> we used the chi-squared statistic to test
whether a die is crooked. The chi-squared statistic measures the total
normalized deviation from the expected values in a table:</p>
</div>
<div class="stemblock">
<div class="content">
\[\chi^2 = \sum_i \frac{{(O_i - E_i)}^2}{E_i}\]
</div>
</div>
<div class="paragraph">
<p>One reason the chi-squared statistic is widely used is that its sampling
distribution under the null hypothesis is analytic; by a remarkable
coincidence<sup class="footnote">[<a id="_footnoteref_6" class="footnote" href="#_footnote_6" title="View footnote.">6</a>]</sup>, it is called the chi-squared
distribution. Like the t-distribution, the chi-squared CDF can be
computed efficiently using gamma functions.</p>
</div>
<div id="normal5" class="imageblock">
<div class="content">
<img src="figs/normal5.png" alt="normal5" height="240">
</div>
<div class="title">Figure 55. Sampling distribution of chi-squared statistics for a fair six-sided die.</div>
</div>
<div class="paragraph">
<p>SciPy provides an implementation of the chi-squared distribution, which
we use to compute the sampling distribution of the chi-squared
statistic:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def ChiSquaredCdf(n):
    xs = np.linspace(0, 25, 101)
    ps = scipy.stats.chi2.cdf(xs, df=n-1)
    return thinkstats2.Cdf(xs, ps)</code></pre>
</div>
</div>
<div class="paragraph">
<p><a href="#normal5">Figure 55</a> shows the analytic result along with the
distribution we got by resampling. They are very similar, especially in
the tail, which is the part we usually care most about.</p>
</div>
<div class="paragraph">
<p>We can use this distribution to compute the p-value of the observed test
statistic, <code>chi2</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    p_value = 1 - scipy.stats.chi2.cdf(chi2, df=n-1)</code></pre>
</div>
</div>
<div class="paragraph">
<p>The result is 0.041, which is consistent with the result from
<a href="#casino2">Section 9.7</a>.</p>
</div>
<div class="paragraph">
<p>The parameter of the chi-squared distribution is &#8220;degrees of freedom&#8221;
again. In this case the correct parameter is <code>n-1</code>, where <code>n</code> is the
size of the table, 6. Choosing this parameter can be tricky; to be
honest, I am never confident that I have it right until I generate
something like <a href="#normal5">Figure 55</a> to compare the analytic
results to the resampling results.</p>
</div>
</div>
<div class="sect2">
<h3 id="_discussion"><a class="anchor" href="#_discussion"></a><a class="link" href="#_discussion">14.9. Discussion</a></h3>
<div class="paragraph">
<p>This book focuses on computational methods like resampling and
permutation. These methods have several advantages over analysis:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>They are easier to explain and understand. For example, one of the
most difficult topics in an introductory statistics class is hypothesis
testing. Many students don’t really understand what p-values are. I
think the approach I presented in <a href="#testing">Chapter 9</a>—simulating
the null hypothesis and computing test statistics—makes the fundamental
idea clearer.</p>
</li>
<li>
<p>They are robust and versatile. Analytic methods are often based on
assumptions that might not hold in practice. Computational methods
require fewer assumptions, and can be adapted and extended more easily.</p>
</li>
<li>
<p>They are debuggable. Analytic methods are often like a black box: you
plug in numbers and they spit out results. But it’s easy to make subtle
errors, hard to be confident that the results are right, and hard to
find the problem if they are not. Computational methods lend themselves
to incremental development and testing, which fosters confidence in the
results.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>But there is one drawback: computational methods can be slow. Taking
into account these pros and cons, I recommend the following process:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Use computational methods during exploration. If you find a
satisfactory answer and the run time is acceptable, you can stop.</p>
</li>
<li>
<p>If run time is not acceptable, look for opportunities to optimize.
Using analytic methods is one of several methods of optimization.</p>
</li>
<li>
<p>If replacing a computational method with an analytic method is
appropriate, use the computational method as a basis of comparison,
providing mutual validation between the computational and analytic
results.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>For the vast majority of problems I have worked on, I didn’t have to go
past Step 1.</p>
</div>
</div>
<div class="sect2">
<h3 id="_exercises_14"><a class="anchor" href="#_exercises_14"></a><a class="link" href="#_exercises_14">14.10. Exercises</a></h3>
<div class="paragraph">
<p></p>
</div>
<div class="paragraph">
<p>A solution to these exercises is in <code>chap14soln.py</code></p>
</div>
<div class="paragraph">
<p><strong>Exercise 14.1</strong></p>
</div>
<div id="log_clt" class="paragraph">
<p>In <a href="#lognormal">Section 5.4</a>, we saw
that the distribution of adult weights is approximately lognormal. One
possible explanation is that the weight a person gains each year is
proportional to their current weight. In that case, adult weight is the
product of a large number of multiplicative factors:</p>
</div>
<div class="stemblock">
<div class="content">
\[w = w_0 f_1 f_2 ... f_n\]
</div>
</div>
<div class="paragraph">
<p>where \(w\) is adult weight, \(w_0\) is birth weight,
and \(f_i\) is the weight gain factor for year \(i\).</p>
</div>
<div class="paragraph">
<p>The log of a product is the sum of the logs of the factors:</p>
</div>
<div class="stemblock">
<div class="content">
\[\log w = \log w_0 + \log f_1 + \log f_2 + ... + \log f_n\]
</div>
</div>
<div class="paragraph">
<p>So by the Central Limit Theorem, the distribution of \(\log w\)
is approximately normal for large \(n\), which implies that the
distribution of \(w\) is lognormal.</p>
</div>
<div class="paragraph">
<p>To model this phenomenon, choose a distribution for \(f\) that
seems reasonable, then generate a sample of adult weights by choosing a
random value from the distribution of birth weights, choosing a sequence
of factors from the distribution of \(f\), and computing the
product. What value of \(n\) is needed to converge to a
lognormal distribution?</p>
</div>
<div class="paragraph">
<p><strong>Exercise 14.2</strong></p>
</div>
<div class="paragraph">
<p>In <a href="#usingCLT">Section 14.6</a> we used the Central Limit Theorem to
find the sampling distribution of the difference in means,
\(\delta\), under the null hypothesis that both samples are
drawn from the same population.</p>
</div>
<div class="paragraph">
<p>We can also use this distribution to find the standard error of the
estimate and confidence intervals, but that would only be approximately
correct. To be more precise, we should compute the sampling distribution
of \(\delta\) under the alternate hypothesis that the samples
are drawn from different populations.</p>
</div>
<div class="paragraph">
<p>Compute this distribution and use it to calculate the standard error and
a 90% confidence interval for the difference in means.</p>
</div>
<div class="paragraph">
<p><strong>Exercise 14.3</strong></p>
</div>
<div class="paragraph">
<p>In a recent paper<sup class="footnote">[<a id="_footnoteref_7" class="footnote" href="#_footnote_7" title="View footnote.">7</a>]</sup>, Stein et al. investigate the effects of an
intervention intended to mitigate gender-stereotypical task allocation
within student engineering teams.</p>
</div>
<div class="paragraph">
<p>Before and after the intervention, students responded to a survey that
asked them to rate their contribution to each aspect of class projects
on a 7-point scale.</p>
</div>
<div class="paragraph">
<p>Before the intervention, male students reported higher scores for the
programming aspect of the project than female students; on average men
reported a score of 3.57 with standard error 0.28. Women reported 1.91,
on average, with standard error 0.32.</p>
</div>
<div class="paragraph">
<p>Compute the sampling distribution of the gender gap (the difference in
means), and test whether it is statistically significant. Because you
are given standard errors for the estimated means, you don’t need to
know the sample size to figure out the sampling distributions.</p>
</div>
<div class="paragraph">
<p>After the intervention, the gender gap was smaller: the average score
for men was 3.44 (SE 0.16); the average score for women was 3.18 (SE
0.16). Again, compute the sampling distribution of the gender gap and
test it.</p>
</div>
<div class="paragraph">
<p>Finally, estimate the change in gender gap; what is the sampling
distribution of this change, and is it statistically significant?</p>
</div>
</div>
</div>
</div>
</div>
<div id="footnotes">
<hr>
<div class="footnote" id="_footnote_1">
<a href="#_footnoteref_1">1</a>. This example is based on information and data from Dunn, &#8220;A Simple Dataset for Demonstrating Common Distributions,&#8221; Journal of Statistics Education v.7, n.3 (1999).
</div>
<div class="footnote" id="_footnote_2">
<a href="#_footnoteref_2">2</a>. I was tipped off to this possibility by a comment (without citation) at <a href="http://mathworld.wolfram.com/LogNormalDistribution.html" class="bare">http://mathworld.wolfram.com/LogNormalDistribution.html</a>. Subsequently I found a paper that proposes the log transform and suggests a cause: Penman and Johnson, &#8220;The Changing Shape of the Body Mass Index Distribution Curve in the Population,&#8221; Preventing Chronic Disease, 2006 July; 3(3): A74. Online at <a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1636707" class="bare">http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1636707</a>.
</div>
<div class="footnote" id="_footnote_3">
<a href="#_footnoteref_3">3</a>. Centers for Disease Control and Prevention (CDC). Behavioral Risk Factor Surveillance System Survey Data. Atlanta, Georgia: U.S. Department of Health and Human Services, Centers for Disease Control and Prevention, 2008.
</div>
<div class="footnote" id="_footnote_4">
<a href="#_footnoteref_4">4</a>. For more about Bayesian inference, see the sequel to this book, <em>Think Bayes</em>.
</div>
<div class="footnote" id="_footnote_5">
<a href="#_footnoteref_5">5</a>. Adapted from MacKay, <em>Information Theory, Inference, and Learning Algorithms</em>, 2003.
</div>
<div class="footnote" id="_footnote_6">
<a href="#_footnoteref_6">6</a>. Not really.
</div>
<div class="footnote" id="_footnote_7">
<a href="#_footnoteref_7">7</a>. &#8220;Evidence for the persistent effects of an intervention to mitigate gender-sterotypical task allocation within student engineering teams,&#8221; Proceedings of the IEEE Frontiers in Education Conference, 2014.
</div>
</div>
<div id="footer">
<div id="footer-text">
Version 2.0.38<br>
Last updated 2019-09-05 16:01:20 AEST
</div>
</div>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlighting()</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  messageStyle: "none",
  tex2jax: {
    inlineMath: [["\\(", "\\)"]],
    displayMath: [["\\[", "\\]"]],
    ignoreClass: "nostem|nolatexmath"
  },
  asciimath2jax: {
    delimiters: [["\\$", "\\$"]],
    ignoreClass: "nostem|noasciimath"
  },
  TeX: { equationNumbers: { autoNumber: "none" } }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.6.0/MathJax.js?config=TeX-MML-AM_HTMLorMML"></script>
</body>
</html>